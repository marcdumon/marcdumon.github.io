

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Backpropagation &#8212; Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MSZJYDCNQ4"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MSZJYDCNQ4');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/build_language_model/4_backprop';</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Wavenet" href="5_wavenet.html" />
    <link rel="prev" title="Activations, Gradients and Batch Normalisation - Part 2" href="3.2_activations_gradients_batchnorm.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">BUILD A LANGUAGE MODEL</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_intro.html">Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_bigrams.html">Bigram Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_mlp.html">Multilayer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="3.1_activations_gradients_batchnorm.html">Activations, Gradients and Batch Normalisation - Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="3.2_activations_gradients_batchnorm.html">Activations, Gradients and Batch Normalisation - Part 2</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_wavenet.html">Wavenet</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">MY FILES</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../my_files/jupyter_book_notes.html">Jupyter Book Notes</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/marcdumon/marcdumon.github.io/blob/master/docs/docs/build_language_model/4_backprop.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/marcdumon/marcdumon.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/marcdumon/marcdumon.github.io/issues/new?title=Issue%20on%20page%20%2Fdocs/build_language_model/4_backprop.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/docs/build_language_model/4_backprop.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Backpropagation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">Forward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-pass">Backward pass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-backward-pass">Pytorch backward pass</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-backward-pass">Manual backward pass</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy-loss">Cross entropy loss</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-2">Layer 2</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-1">Layer 1</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linearity">Non-linearity</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-norm">Batch norm</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-batch-norm">Pre-batch norm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-layer">Embedding layer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-through-cross-entropy-loss">Backpropagation through cross entropy loss</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Backward pass</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-of-cross-entropy-loss">Intuition of cross-entropy loss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-through-batchnorm">Backpropagation through batchnorm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Backward pass</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-mlp-model-with-manual-backpropagation">Train the MLP model with manual backpropagation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>

<span class="n">torch</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</details>
</div>
<hr>
<section class="tex2jax_ignore mathjax_ignore" id="backpropagation">
<h1>Backpropagation<a class="headerlink" href="#backpropagation" title="Permalink to this heading">#</a></h1>
<hr><p>In this notebook, we will write the backpropagation manually. It is valuable exercise for gaining a deeper understanding of the algorithm. Although in practice, we typically do not implement backpropagation manually, it is crucial to comprehend the process that occurs beneath the surface.</p>
<p>One challenge with backpropagation is that it represents a <a class="reference external" href="https://www.wikiwand.com/en/Leaky_abstraction">leaky abstraction</a>. Understanding of the algorithm can avoid numerous errors and improve the ability to debug issues that may arise.</p>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this heading">#</a></h2>
<p><strong>Load the data</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;./data/names.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">names</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total # names: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">)</span><span class="si">:</span><span class="s2">.&gt;25,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total # characters in all names: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">names</span><span class="p">))</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">names</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total # names: ...................32,033
Total # characters in all names: 196,113
[&#39;emma&#39;, &#39;olivia&#39;, &#39;ava&#39;, &#39;isabella&#39;, &#39;sophia&#39;]
</pre></div>
</div>
</div>
</div>
<p><strong>Encoding and decoding characters</strong><br />
We define an encoder <code class="docutils literal notranslate"><span class="pre">ch2ix</span></code> and decoder <code class="docutils literal notranslate"><span class="pre">ix2ch</span></code> function that maps a character with a numerical representation (i.e. a unique integer) and vice-versa.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chars</span> <span class="o">=</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_lowercase</span>
<span class="n">ch2ix</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="n">ix2ch</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ch2ix</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoder ch2ix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ch2ix</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decoder ix2ch:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ix2ch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;E.g. emma:&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">ch2ix</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="s2">&quot;.emma.&quot;</span><span class="p">])</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocab size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encoder ch2ix:
 {&#39;.&#39;: 0, &#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 4, &#39;e&#39;: 5, &#39;f&#39;: 6, &#39;g&#39;: 7, &#39;h&#39;: 8, &#39;i&#39;: 9, &#39;j&#39;: 10, &#39;k&#39;: 11, &#39;l&#39;: 12, &#39;m&#39;: 13, &#39;n&#39;: 14, &#39;o&#39;: 15, &#39;p&#39;: 16, &#39;q&#39;: 17, &#39;r&#39;: 18, &#39;s&#39;: 19, &#39;t&#39;: 20, &#39;u&#39;: 21, &#39;v&#39;: 22, &#39;w&#39;: 23, &#39;x&#39;: 24, &#39;y&#39;: 25, &#39;z&#39;: 26}
Decoder ix2ch:
 {0: &#39;.&#39;, 1: &#39;a&#39;, 2: &#39;b&#39;, 3: &#39;c&#39;, 4: &#39;d&#39;, 5: &#39;e&#39;, 6: &#39;f&#39;, 7: &#39;g&#39;, 8: &#39;h&#39;, 9: &#39;i&#39;, 10: &#39;j&#39;, 11: &#39;k&#39;, 12: &#39;l&#39;, 13: &#39;m&#39;, 14: &#39;n&#39;, 15: &#39;o&#39;, 16: &#39;p&#39;, 17: &#39;q&#39;, 18: &#39;r&#39;, 19: &#39;s&#39;, 20: &#39;t&#39;, 21: &#39;u&#39;, 22: &#39;v&#39;, 23: &#39;w&#39;, 24: &#39;x&#39;, 25: &#39;y&#39;, 26: &#39;z&#39;}
E.g. emma: [0, 5, 13, 13, 1, 0]
Vocab size: 27
</pre></div>
</div>
</div>
</div>
<p><strong>Dataset</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Dataset</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attributes:</span>
<span class="sd">    names: list of names.</span>
<span class="sd">    block_size: context length: how many characters do we take to predict the next one?</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">Y</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_make_dataset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_make_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">names</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>
            <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">w</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">:</span>
                <span class="n">ix</span> <span class="o">=</span> <span class="n">ch2ix</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span>
                <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
                <span class="n">Y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ix2ch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">context</span><span class="p">),</span> <span class="s2">&quot;---&gt;&quot;</span><span class="p">,</span> <span class="n">ix2ch</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
                <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>  <span class="c1"># crop and append</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;X.shape: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, Y.shape: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a training, validation and test set</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>

<span class="n">n1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">))</span>
<span class="n">n2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">))</span>

<span class="n">ds_tr</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">names</span><span class="p">[:</span><span class="n">n1</span><span class="p">],</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">)</span>
<span class="n">Xtr</span><span class="p">,</span> <span class="n">Ytr</span> <span class="o">=</span> <span class="n">ds_tr</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">ds_tr</span><span class="o">.</span><span class="n">Y</span>  <span class="c1"># 80%</span>
<span class="n">ds_val</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">],</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">)</span>
<span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span> <span class="o">=</span> <span class="n">ds_val</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">ds_val</span><span class="o">.</span><span class="n">Y</span>  <span class="c1"># 10%</span>
<span class="n">ds_te</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="n">n2</span><span class="p">:],</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">)</span>
<span class="n">Xte</span><span class="p">,</span> <span class="n">Yte</span> <span class="o">=</span> <span class="n">ds_te</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">ds_te</span><span class="o">.</span><span class="n">Y</span>  <span class="c1"># 10%</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>X.shape: torch.Size([182671, 3]), Y.shape: torch.Size([182671])
X.shape: torch.Size([22784, 3]), Y.shape: torch.Size([22784])
X.shape: torch.Size([22691, 3]), Y.shape: torch.Size([22691])
</pre></div>
</div>
</div>
</div>
<p><strong>Utility functions</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compare_gradients</span><span class="p">(</span><span class="n">description</span><span class="p">,</span> <span class="n">manual_grads</span><span class="p">,</span> <span class="n">autograd_grads</span><span class="p">):</span>
    <span class="n">exact_match</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">manual_grads</span> <span class="o">==</span> <span class="n">autograd_grads</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">approx_match</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">manual_grads</span><span class="p">,</span> <span class="n">autograd_grads</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="n">max_difference</span> <span class="o">=</span> <span class="p">(</span><span class="n">manual_grads</span> <span class="o">-</span> <span class="n">autograd_grads</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">description</span><span class="si">:</span><span class="s2">15s</span><span class="si">}</span><span class="s2"> | exact: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">exact_match</span><span class="p">)</span><span class="si">:</span><span class="s2">5s</span><span class="si">}</span><span class="s2"> | approximate: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">approx_match</span><span class="p">)</span><span class="si">:</span><span class="s2">5s</span><span class="si">}</span><span class="s2"> | maxdiff: </span><span class="si">{</span><span class="n">max_difference</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this heading">#</a></h2>
<p>We initialise a MLP model with 64 hidden units and an character embedding dimension of 10.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Because we have a batch norm layer, we do not need to add a bias term to the linear layers. We define them anyway because itâ€™s still interesting to calculate the gradients.</p>
</div>
<p>Normally we should initialise the weights and biases as follows:</p>
<ul class="simple">
<li><p>gain = 5/3 for tanh</p></li>
<li><p><span class="math notranslate nohighlight">\(W_1: \large\frac{5/3}{\sqrt{\text{fan}_{\text{in}}}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(b_1: 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(W_2: \large\frac{5/3}{\sqrt{\text{fan}_{\text{in}}}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(b_2: 0\)</span></p></li>
</ul>
<p>However, we will initialise the weights and biases in non-standard ways because sometimes initializating with e.g. all zeros could mask an incorrect implementation of the backward pass.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_emb</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># the dimensionality of the character embedding vectors</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># the number of neurons in the hidden layer of the MLP</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># for reproducibility</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_emb</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>

<span class="c1"># Layer 1</span>
<span class="n">W1</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">n_emb</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="o">*</span> <span class="p">(</span><span class="mi">5</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span>
    <span class="o">/</span> <span class="p">((</span><span class="n">n_emb</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>

<span class="c1"># Layer 2</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>

<span class="c1"># BatchNorm parameters</span>
<span class="n">bn_gain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">+</span> <span class="mf">1.0</span>
<span class="n">bn_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bn_gain</span><span class="p">,</span> <span class="n">bn_bias</span><span class="p">]</span>
<span class="n">n_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total # parameters: </span><span class="si">{</span><span class="n">n_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total # parameters: 4,137
</pre></div>
</div>
</div>
</div>
</section>
<section id="forward-pass">
<h2>Forward pass<a class="headerlink" href="#forward-pass" title="Permalink to this heading">#</a></h2>
<p><strong>Calculate a single batch</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="c1"># construct a minibatch</span>
<span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtr</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>  <span class="c1"># batch</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Calculate a forward pass on the batch</strong></p>
<p>We will make the forward pass with the smallest steps possible. This will make it easier to understand the backward pass.</p>
<p>Batch norm calculation:</p>
<ul class="simple">
<li><p>emb_cat is a 32x30 tensor (batch size x sequence length)</p></li>
<li><p><span class="math notranslate nohighlight">\(\textrm{h_pre_bn} = \textrm{emb_cat} \cdot W_1 + b_1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\textrm{bn_mean_i} = \sum_{i=1}^{32}\textrm{h_pre_bn}_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\textrm{bn_diff} = \textrm{h_pre_bn} - \textrm{bn_mean_i}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\textrm{bn_diff2}= \textrm{bn_diff}^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\textrm{bn_var} = \sum_{i=1}^{32}\textrm{bn_diff2}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\textrm{bn_var_inv} = \frac{1}{\sqrt{(\textrm{bn_var} + \epsilon)}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\textrm{bn_raw} = \textrm{bn_diff} * \textrm{bn_var_inv}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\textrm{h_pre_act} = \textrm{bn_gain} * \textrm{bn_raw} + \textrm{bn_bias}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Embed the characters into vectors and concatenate them</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xb</span><span class="p">]</span>  <span class="c1"># 32 x 3 x 10</span>
<span class="n">emb_cat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 32 x 30</span>

<span class="c1"># Linear layer 1</span>
<span class="c1"># pre-batch norm</span>
<span class="n">h_pre_bn</span> <span class="o">=</span> <span class="n">emb_cat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c1"># 32 x 64</span>
<span class="c1"># batch norm layer</span>
<span class="n">bn_mean_i</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">h_pre_bn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="n">bs</span>
<span class="p">)</span>  <span class="c1"># 1 x 64 (keepdim=True to keep the 1 x 64 shape otherwise 64)</span>
<span class="n">bn_diff</span> <span class="o">=</span> <span class="n">h_pre_bn</span> <span class="o">-</span> <span class="n">bn_mean_i</span>  <span class="c1"># 32 x 64</span>
<span class="n">bn_diff2</span> <span class="o">=</span> <span class="n">bn_diff</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># 32 x 64</span>
<span class="n">bn_var</span> <span class="o">=</span> <span class="n">bn_diff2</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
    <span class="n">bs</span> <span class="o">-</span> <span class="mi">1</span>
<span class="p">)</span>  <span class="c1"># 1 x 64  Bessel&#39;s correction (dividing by n-1, not n)</span>
<span class="n">bn_var_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">bn_var</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>  <span class="c1"># 1 x 64</span>
<span class="n">bn_raw</span> <span class="o">=</span> <span class="n">bn_diff</span> <span class="o">*</span> <span class="n">bn_var_inv</span>  <span class="c1"># 32 x 64 batchnorm raw output</span>
<span class="n">h_pre_act</span> <span class="o">=</span> <span class="n">bn_gain</span> <span class="o">*</span> <span class="n">bn_raw</span> <span class="o">+</span> <span class="n">bn_bias</span>  <span class="c1"># 32 x 64 hiodden layer pre-activation</span>
<span class="c1"># non-linearity</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">h_pre_act</span><span class="p">)</span>  <span class="c1"># 32 x 64</span>

<span class="c1"># Linear layer 2</span>
<span class="c1"># output layer</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">W2</span> <span class="o">+</span> <span class="n">b2</span>  <span class="c1"># 32 x 28</span>

<span class="c1"># cross entropy loss (same as F.cross_entropy(logits, Yb))</span>
<span class="n">logit_maxes</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># 32 x 1</span>
<span class="n">norm_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">logit_maxes</span>  <span class="c1"># 32 x 27 subtract max for numerical stability</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">norm_logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>  <span class="c1"># 32 x 27</span>
<span class="n">counts_sum</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 32 x 1</span>
<span class="n">counts_sum_inv</span> <span class="o">=</span> <span class="n">counts_sum</span><span class="o">**-</span><span class="mi">1</span>  <span class="c1"># 32 x 1</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>  <span class="c1"># 32 x 27</span>
<span class="n">logprobs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>  <span class="c1"># 32 x 27</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">logprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="backward-pass">
<h2>Backward pass<a class="headerlink" href="#backward-pass" title="Permalink to this heading">#</a></h2>
<section id="pytorch-backward-pass">
<h3>Pytorch backward pass<a class="headerlink" href="#pytorch-backward-pass" title="Permalink to this heading">#</a></h3>
<p>We will first calculate the backward pass with Pytorchâ€™s autograd. This will allow us to compare the results of our manual implementation with the Pytorch implementation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch backward pass</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">[</span>
    <span class="n">logprobs</span><span class="p">,</span>
    <span class="n">probs</span><span class="p">,</span>
    <span class="n">counts</span><span class="p">,</span>
    <span class="n">counts_sum</span><span class="p">,</span>
    <span class="n">counts_sum_inv</span><span class="p">,</span>
    <span class="n">norm_logits</span><span class="p">,</span>
    <span class="n">logit_maxes</span><span class="p">,</span>
    <span class="n">logits</span><span class="p">,</span>
    <span class="n">h</span><span class="p">,</span>
    <span class="n">h_pre_act</span><span class="p">,</span>
    <span class="n">bn_raw</span><span class="p">,</span>
    <span class="n">bn_var_inv</span><span class="p">,</span>
    <span class="n">bn_var</span><span class="p">,</span>
    <span class="n">bn_diff2</span><span class="p">,</span>
    <span class="n">bn_diff</span><span class="p">,</span>
    <span class="n">h_pre_bn</span><span class="p">,</span>
    <span class="n">bn_mean_i</span><span class="p">,</span>
    <span class="n">emb_cat</span><span class="p">,</span>
    <span class="n">emb</span><span class="p">,</span>
<span class="p">]:</span>
    <span class="n">t</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(3.4002, grad_fn=&lt;NegBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="manual-backward-pass">
<h3>Manual backward pass<a class="headerlink" href="#manual-backward-pass" title="Permalink to this heading">#</a></h3>
<section id="cross-entropy-loss">
<h4>Cross entropy loss<a class="headerlink" href="#cross-entropy-loss" title="Permalink to this heading">#</a></h4>
<p>We now calculate the backward pass of each step manually. We will compare the results with the Pytorch implementation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The shape of the gradient should be the same as the shape of its parameter</p>
</div>
<p><strong>logprobs</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">logprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span>
</pre></div>
</div>
<p>plucks the <code class="docutils literal notranslate"><span class="pre">logprob</span></code> corresponding to the value of the element of <code class="docutils literal notranslate"><span class="pre">Yb</span></code> for each sample in the batch.</p>
<p>The derivative of <code class="docutils literal notranslate"><span class="pre">loss</span></code> with respect to the <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> is 0 everywhere except for the <code class="docutils literal notranslate"><span class="pre">logprob</span></code> corresponding to the value of the elements of <code class="docutils literal notranslate"><span class="pre">Yb</span></code> where it is <span class="math notranslate nohighlight">\(-\large\frac{1}{bs}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_logprobs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logprobs</span><span class="p">)</span>  <span class="c1"># 32 x 27</span>
<span class="n">d_logprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">bs</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;logprobs&quot;</span><span class="p">,</span> <span class="n">d_logprobs</span><span class="p">,</span> <span class="n">logprobs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>logprobs        | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p><strong>probs</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logprobs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
</pre></div>
</div>
<p>All the elements of <code class="docutils literal notranslate"><span class="pre">probs</span></code> are loged element-wise. The derivative of a log is <span class="math notranslate nohighlight">\({\large\frac{d}{dx}} log(x) = \large\frac{1}{x}\)</span>.</p>
<p>The derivative of <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> with respect to <code class="docutils literal notranslate"><span class="pre">probs</span></code> is thus (chain rule) <span class="math notranslate nohighlight">\({\large\frac{1}{probs}} * \textrm{d\_logprobs}\)</span>.</p>
<p>Intuitively, if an element of <code class="docutils literal notranslate"><span class="pre">probs</span></code> is close to 1, the network correctly predicts the next character.  The <code class="docutils literal notranslate"><span class="pre">d_logprobs</span></code> is just passed through.<br />
However, when an element of <code class="docutils literal notranslate"><span class="pre">probs</span></code> is small, 1/probs is big and amplifies <code class="docutils literal notranslate"><span class="pre">d_logprobs</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">probs</span></code> and <code class="docutils literal notranslate"><span class="pre">d_logprobs</span></code> are both 32x27 tensors and therfore can be multiplied without broadcasting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_probs</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">probs</span><span class="p">)</span> <span class="o">*</span> <span class="n">d_logprobs</span>  <span class="c1"># 32 x 27</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;probs&quot;</span><span class="p">,</span> <span class="n">d_probs</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>probs           | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p><strong>d_counts_sum_inv</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>
</pre></div>
</div>
<hr class="docutils" />
<p>Toy example:<br />
Broadcasting a 3x1 tensor to a 3x3 tensor</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
  c &amp;= a * b\\
  c[3\text{x}3] &amp;= a[3\text{x}3] * b[3\text{x}1]\\
  \begin{bmatrix}
    c_{11} &amp; c_{12} &amp; c_{13}\\
    c_{21} &amp; c_{22} &amp; c_{23}\\
    c_{31} &amp; c_{32} &amp; c_{33}
  \end{bmatrix} &amp;=
  \begin{bmatrix}
    a_{11}*b_1 &amp; a_{12}*b_1 &amp; a_{13}*b_1\\
    a_{21}*b_2 &amp; a_{22}*b_2 &amp; a_{23}*b_2\\
    a_{31}*b_3 &amp; a_{32}*b_3 &amp; a_{33}*b_3
  \end{bmatrix}
\end{align*}
\end{split}\]</div>
<p>This is actually 2 operations: first we replicate the 3x1 tensor to a 3x3 tensor and then we multiply the 2 tensors element-wise.</p>
<p>To calculate the derivative of c with respect to b, we first calculate the derivative of a * b with respect to b. This is a.</p>
<p>Then we have to calculate the derivative of the replication of b with respect to b. On the computational graph, b is branched into 3 branches.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
  \begin{bmatrix}
    a_{11}*b_1\\
    a_{21}*b_2\\
    a_{31}*b_3    
  \end{bmatrix} ,
  \begin{bmatrix}
    a_{12}*b_1\\
    a_{22}*b_2\\
    a_{32}*b_3    
  \end{bmatrix} \textrm{and}
  \begin{bmatrix}
    a_{13}*b_1\\
    a_{23}*b_2\\
    a_{33}*b_3    
  \end{bmatrix} 
\end{align*}
\end{split}\]</div>
<p>The derivative is the sum of the derivatives of the 3 branches:
$<span class="math notranslate nohighlight">\(
\begin{align*}
  \begin{bmatrix}
    a_{11} + a_{21} + a_{31}\\
    a_{21} + a_{22} + a_{32}\\
    a_{31} + a_{23} + a_{33}    
  \end{bmatrix}
\end{align*}
\)</span>$</p>
<hr class="docutils" />
<p><code class="docutils literal notranslate"><span class="pre">counts</span></code> is a 32x27 tensor and <code class="docutils literal notranslate"><span class="pre">counts_sum_inv</span></code> is a 32x1 tensor. <code class="docutils literal notranslate"><span class="pre">counts_sum_inv</span></code> gets broadcasted (replicated 27x) to a 32x27 tensor and then multiplied by <code class="docutils literal notranslate"><span class="pre">counts</span></code>. In the backward pass we have to sum the derivatives of the 27 branches.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the forward pass, if we sum the tensor or compute its mean, max, etc., we need to replicate it (for broadcasting) in the backward pass. Similarly, if we replicate the tensor in the forward pass, we need to sum it in the backward pass.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_counts_sum_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">counts</span> <span class="o">*</span> <span class="n">d_probs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 32 x 1</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;counts_sum_inv&quot;</span><span class="p">,</span> <span class="n">d_counts_sum_inv</span><span class="p">,</span> <span class="n">counts_sum_inv</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p><strong>d_counts</strong> - branch 1</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">counts_sum_inv</span></code> depends on <code class="docutils literal notranslate"><span class="pre">counts</span></code>. <code class="docutils literal notranslate"><span class="pre">counts_sum_inv</span></code> backpropagates through <code class="docutils literal notranslate"><span class="pre">counts_sum</span></code> and <code class="docutils literal notranslate"><span class="pre">counts_sum</span></code> backpropagates through <code class="docutils literal notranslate"><span class="pre">counts</span></code>. <code class="docutils literal notranslate"><span class="pre">counts</span></code> is a node that is used twice. The two branches are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>
</pre></div>
</div>
<p>and</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">counts_sum</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  
<span class="n">counts_sum_inv</span> <span class="o">=</span> <span class="n">counts_sum</span><span class="o">**-</span><span class="mi">1</span>
</pre></div>
</div>
<p>We have to sum the gradients of the two branches later to get the correct gradient of <code class="docutils literal notranslate"><span class="pre">counts</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">counts</span></code> is a 32x27 tensor <code class="docutils literal notranslate"><span class="pre">counts_sum_inv</span></code> is a 32x1 tensor. <code class="docutils literal notranslate"><span class="pre">counts_sum_inv</span></code> gets broadcasted, not<code class="docutils literal notranslate"><span class="pre">counts</span></code>, so we donâ€™t sum.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_counts</span> <span class="o">=</span> <span class="n">counts_sum_inv</span> <span class="o">*</span> <span class="n">d_probs</span>  <span class="c1"># 32 x 27</span>
</pre></div>
</div>
</div>
</div>
<p><strong>d_counts_sum</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">counts_sum_inv</span> <span class="o">=</span> <span class="n">counts_sum</span><span class="o">**-</span><span class="mi">1</span>
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\(\large\frac{d}{dx}\frac{1}{x} = -\frac{1}{x^2}\)</span></p>
<p><code class="docutils literal notranslate"><span class="pre">counts_sum</span></code> and <code class="docutils literal notranslate"><span class="pre">d_counts_sum_inv</span></code> are both 32x1 tensors and can be multiplied without broadcasting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_counts_sum</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">counts_sum</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">d_counts_sum_inv</span>  <span class="c1"># 32 x 1</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;counts_sum&quot;</span><span class="p">,</span> <span class="n">d_counts_sum</span><span class="p">,</span> <span class="n">counts_sum</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p><strong>d_counts</strong> - branch 2</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">counts_sum</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">counts_sum</span></code> is the sum of the rows of <code class="docutils literal notranslate"><span class="pre">counts</span></code>. <code class="docutils literal notranslate"><span class="pre">counts</span></code> has shape 32x27 and <code class="docutils literal notranslate"><span class="pre">counts_sum</span></code> has shape 32x1.</p>
<hr class="docutils" />
<p>Toy example:
$<span class="math notranslate nohighlight">\(
\begin{align*}
  \begin{bmatrix}
    a_{11} &amp; a_{12} &amp; a_{13}\\
    a_{21} &amp; a_{22} &amp; a_{23}\\
    a_{31} &amp; a_{32} &amp; a_{33}
  \end{bmatrix} =&gt;
  \begin{bmatrix}
    b_1(=a_{11} + a_{12} + a_{13})\\
    b_2(=a_{21} + a_{22} + a_{23})\\
    b_3(=a_{31} + a_{32} + a_{33})
  \end{bmatrix}
\end{align*}
\)</span>$</p>
<p>We have a 3x3 tensor a and sum the rows to get a 3x1 tensor b. We have the derivative of the loss with respect to b. We want to calculate the derivative of the loss with respect to the elements of a.<br />
b1 only depends on a11, a12 and a13. The derivative of b1 with respect to a11, a12, and a13 is 1. The derivative of b1 with respect to all the other elements of a is 0.<br />
In the chain rule, we multiply the derivative of the loss with respect to b1 with the derivative of b1 with respect to a11, a12 and a13.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
  \begin{bmatrix}
    1 &amp; 1 &amp; 1\\
    1 &amp; 1 &amp; 1\\
    1 &amp; 1 &amp; 1
  \end{bmatrix} *
    \begin{bmatrix}
    b_1\\
    b_2\\
    b_3
  \end{bmatrix} =
  \begin{bmatrix}
    b_1 &amp; b_1 &amp; b_1\\
    b_2 &amp; b_2 &amp; b_2\\
    b_3 &amp; b_3 &amp; b_3
  \end{bmatrix}
\end{align*}
\end{split}\]</div>
<hr class="docutils" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>An addition can be seen as a router of gradients. The gradient comming from above gets routed equally to all the elements that participate in the addition. In this case the derivative of the loss with respect to b1 is routed equally to a11, a12 and a13.</p>
</div>
<p>In the backpropagation we need to take the 32x1 tensor <code class="docutils literal notranslate"><span class="pre">d_counts_sum</span></code> and replicate it to form a 32x27 tensor.  We also need to add this <code class="docutils literal notranslate"><span class="pre">d_counts</span></code> gradients to the <code class="docutils literal notranslate"><span class="pre">d_counts</span></code> gradients that from the other branch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_counts</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span> <span class="o">*</span> <span class="n">d_counts_sum</span>  <span class="c1"># 32 x 27</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;counts&quot;</span><span class="p">,</span> <span class="n">d_counts</span><span class="p">,</span> <span class="n">counts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>counts          | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p><strong>d_norm_logits</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">counts</span> <span class="o">=</span> <span class="n">norm_logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\(\large\frac{d}{dx}e^x = e^x\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_norm_logits</span> <span class="o">=</span> <span class="n">norm_logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">*</span> <span class="n">d_counts</span>  <span class="c1"># 32 x 27 == counts * d_counts</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;norm_logits&quot;</span><span class="p">,</span> <span class="n">d_norm_logits</span><span class="p">,</span> <span class="n">norm_logits</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p><strong>d_logits</strong> - branch 1, <strong>d_logit_maxes</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">norm_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">logit_maxes</span> 
</pre></div>
</div>
<hr class="docutils" />
<p>Toy example:
$<span class="math notranslate nohighlight">\(
\begin{align*}
  \begin{bmatrix}
    c_{11} &amp; c_{12} &amp; c_{13}\\
    c_{21} &amp; c_{22} &amp; _{23}\\
    c_{31} &amp; c_{32} &amp; _{33}
  \end{bmatrix} =
  \begin{bmatrix}
    a_{11} &amp; a_{12} &amp; a_{13}\\
    a_{21} &amp; a_{22} &amp; a_{23}\\
    a_{31} &amp; a_{32} &amp; a_{33}
  \end{bmatrix} -
  \begin{bmatrix}
    b_1\\
    b_2\\
    b_3
  \end{bmatrix} = 
  \begin{bmatrix}
    a_{11}-b1 &amp; a_{12}-b_1 &amp; a_{13}-b_1\\
    a_{21}-b2 &amp; a_{22}-b_2 &amp; a_{23}-b_2\\
    a_{31}-b3 &amp; a_{32}-b_3 &amp; a_{33}-b_3
  \end{bmatrix}
\end{align*}
\)</span>$</p>
<p>The broadcasting of b to a is done by replicating b to a 3x3 tensor. This effectively creates 3 branches. To calculate the derivative of c with respect to b, we need to sum the derivatives of the 3 branches.</p>
<hr class="docutils" />
<p><code class="docutils literal notranslate"><span class="pre">norm_logits</span></code> is a 32x27 tensor, <code class="docutils literal notranslate"><span class="pre">logits</span></code> is a 32x27 tensor and <code class="docutils literal notranslate"><span class="pre">logit_maxes</span></code> is a 32x1 tensor.
<code class="docutils literal notranslate"><span class="pre">logit_maxes</span></code> gets broadcasted so we sum the gradients. <code class="docutils literal notranslate"><span class="pre">logits</span></code> doesnâ€™t gets broadcasted so we donâ€™t sum the gradients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_logits</span> <span class="o">=</span> <span class="n">d_norm_logits</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># 32 x 27</span>
<span class="n">d_logits_maxes</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">d_norm_logits</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 32 x 1</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;logits_maxes&quot;</span><span class="p">,</span> <span class="n">d_logits_maxes</span><span class="p">,</span> <span class="n">logit_maxes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>logits_maxes    | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>We substracted the maximum of the logits from the logits for numerical stability, otherwise the exponentiation of the logits could overflow when a logit is large. This shifting operation has no effect on <code class="docutils literal notranslate"><span class="pre">probs</span></code>.</p>
<p><span class="math notranslate nohighlight">\(\Large\frac{e^x}{\sum_i{e^x_i}} =  \frac{e^{x+a}}{\sum_i{e^{x_i+a}}}\)</span></p>
<p>Because subtracting the maximum of the logits from the logits has no effect on <code class="docutils literal notranslate"><span class="pre">probs</span></code> it will also not change <code class="docutils literal notranslate"><span class="pre">loss</span></code>. The gradients of <code class="docutils literal notranslate"><span class="pre">logits_maxes</span></code> should all be zero.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_logits_maxes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-1.8626e-09],
        [ 1.8626e-09],
        [ 9.3132e-10],
        [ 4.6566e-09],
        [ 3.2596e-09],
        [ 2.5611e-09],
        [ 1.6298e-09],
        [ 1.3970e-09],
        [-2.0955e-09],
        [ 0.0000e+00],
        [-1.8626e-09],
        [-3.2596e-09],
        [ 1.8626e-09],
        [ 1.6298e-09],
        [ 0.0000e+00],
        [-1.1642e-09],
        [ 6.9849e-10],
        [ 2.3283e-09],
        [ 1.3970e-09],
        [-2.0955e-09],
        [-2.0955e-09],
        [-3.2596e-09],
        [-9.3132e-10],
        [ 1.1642e-09],
        [ 1.3970e-09],
        [-2.3283e-10],
        [-5.8208e-09],
        [ 6.9849e-10],
        [ 0.0000e+00],
        [ 4.6566e-10],
        [-4.6566e-10],
        [ 0.0000e+00]], grad_fn=&lt;SumBackward1&gt;)
</pre></div>
</div>
</div>
</div>
<p><strong>d_logits</strong> - branch 2</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logit_maxes</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">logits.max(1)</span></code> returns a tuple of 2 tensors. The first tensor is the maximum values of each row of <code class="docutils literal notranslate"><span class="pre">logits</span></code> and the second tensor is the indices of the maximum values.</p>
<p>We have a 32 x 27 tensor. The max of the tensor plucks out the maximum value of each row and returns a 32 x 1 tensor. The derivative should be 1 * d_logit_maxes for the maximum value and 0 for all the other values in the row.<br />
We can calculate the derivative like we did for <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> where we created a 32 x 27 zero-tensor and then populate it with 1 at the indices of the maximum values. Another way is to create a one-hot tensor with a 1 at the indices of the maximum values.</p>
<p>Because we are on the second branch, we also need to add the d_logits from the first branch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/279d0ddbaf3e45122bf77dfb0d96225c1b839b3c00d9ca817db82f9d56156254.png" src="../../_images/279d0ddbaf3e45122bf77dfb0d96225c1b839b3c00d9ca817db82f9d56156254.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_logits</span> <span class="o">+=</span> <span class="p">(</span>
    <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">d_logits_maxes</span>
<span class="p">)</span>  <span class="c1"># 32 x 27</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">d_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>logits          | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
</section>
<section id="layer-2">
<h4>Layer 2<a class="headerlink" href="#layer-2" title="Permalink to this heading">#</a></h4>
<p><strong>d_h</strong>, <strong>d_W2</strong>, <strong>d_b2</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">W2</span> <span class="o">+</span> <span class="n">b2</span> 
</pre></div>
</div>
<hr class="docutils" />
<p>Toy example:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
  d &amp;= a &#64; b + c\\\\
  \begin{bmatrix}
    d_{11} &amp; d_{12}\\
    d_{21} &amp; d_{22}
  \end{bmatrix} &amp;=
  \begin{bmatrix}
    a_{11} &amp; a_{12}\\
    a_{21} &amp; a_{22}
  \end{bmatrix} \cdot
  \begin{bmatrix}
      b_{11} &amp; b_{12}\\
      b_{21} &amp; b_{22}
  \end{bmatrix} +
  \begin{bmatrix}
      c_{11} &amp; c_{12}\\
      c_{21} &amp; c_{22}
  \end{bmatrix}\\\\
  &amp;\Rightarrow
  \begin{cases}
    d_{11} &amp;= a_{11}.b_{11} +a_{12}.b_{21} + c_1\\
    d_{12} &amp;= a_{11}.b_{12} +a_{12}.b_{22} + c_2\\
    d_{21} &amp;= a_{21}.b_{11} +a_{22}.b_{21} + c_1\\
    d_{22} &amp;= a_{21}.b_{12} +a_{22}.b_{22} + c_2
  \end{cases}\\\\
\end{align*}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
  \textsf{We can now calculate the derivative of d with respect to a.}&amp;\\
  \begin{cases}
    \large\frac{\delta L}{\delta a_{11}} &amp;= {\large\frac{\delta L}{\delta d_{11}}}.b_{11} + {\large\frac{\delta L}{\delta d_{12}}}.b_{12}\\\\
    \large\frac{\delta L}{\delta a_{12}} &amp;= {\large\frac{\delta L}{\delta d_{11}}}.b_{21} + {\large\frac{\delta L}{\delta d_{12}}}.b_{22}\\\\
    \large\frac{\delta L}{\delta a_{21}} &amp;= {\large\frac{\delta L}{\delta d_{21}}}.b_{11} + {\large\frac{\delta L}{\delta d_{22}}}.b_{12}\\\\
    \large\frac{\delta L}{\delta a_{22}} &amp;= {\large\frac{\delta L}{\delta d_{21}}}.b_{21} + {\large\frac{\delta L}{\delta d_{22}}}.b_{22} 
  \end{cases}\\\\
  \frac{\delta L}{\delta a} =
  \begin{bmatrix}
    \large\frac{\delta L}{\delta a_{11}} &amp; \large\frac{\delta L}{\delta a_{12}}\\\\
    \large\frac{\delta L}{\delta a_{21}} &amp; \large\frac{\delta L}{\delta a_{22}}
  \end{bmatrix} =
  \begin{bmatrix}
    \large\frac{\delta L}{\delta d_{11}} &amp; \large\frac{\delta L}{\delta d_{12}}\\\\
    \large\frac{\delta L}{\delta d_{21}} &amp; \large\frac{\delta L}{\delta d_{22}}
  \end{bmatrix} \cdot
  \begin{bmatrix}
    b_{11} &amp; b_{21}\\
    b_{12} &amp; b_{22}
  \end{bmatrix} &amp;=
  \begin{bmatrix}
    \large\frac{\delta L}{\delta d_{11}} &amp; \large\frac{\delta L}{\delta d_{12}}\\\\
    \large\frac{\delta L}{\delta d_{21}} &amp; \large\frac{\delta L}{\delta d_{22}}
  \end{bmatrix} \cdot
  \begin{bmatrix}
    b_{11} &amp; b_{12}\\
    b_{21} &amp; b_{22}
  \end{bmatrix}^T =
  \frac{\delta L}{\delta d} \cdot b^T\\\\
  \textsf{Similarly, we can calculate the derivative of d with respect to b.}\\
  \frac{\delta L}{\delta b} = a^T \cdot \frac{\delta L}{\delta d}\\\\
  \textsf{And the derivative of d with respect to c.}\\
  \begin{cases}
    \large\frac{\delta L}{\delta c_1} &amp;= {\large\frac{\delta L}{\delta d_{11}}}.1 + {\large\frac{\delta L}{\delta d_{21}}}.1\\\\
    \large\frac{\delta L}{\delta c_2} &amp;= {\large\frac{\delta L}{\delta d_{12}}}.1 + {\large\frac{\delta L}{\delta d_{22}}}.1
  \end{cases}\\\\
  \frac{\delta L}{\delta c} = \frac{\delta L}{\delta d}\textsf{.sum(axis=0)}\\\\
\end{align*}
\end{split}\]</div>
<hr class="docutils" />
<p><code class="docutils literal notranslate"><span class="pre">h</span></code> is a 32 x 64 tensor, <code class="docutils literal notranslate"><span class="pre">W2</span></code> is a 64 x 27 tensor and <code class="docutils literal notranslate"><span class="pre">b2</span></code> is 27d vector. The bias <code class="docutils literal notranslate"><span class="pre">b2</span></code> is broadcasted to a 32 x 27 tensor (<code class="docutils literal notranslate"><span class="pre">h</span> <span class="pre">&#64;</span> <span class="pre">W2</span></code>). Internally, the broadcasting is done by casting the 27d vector to a 1 x 27 tensor and then replicating it vertically to a 32 x 27 tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Itâ€™s not realy necessary to remember the formulaâ€™s for the derivatives of the linear transformation. The dimensions of the tensors will tell how to calculate the derivatives.</p>
<p>Example:
To calculate <code class="docutils literal notranslate"><span class="pre">d_h</span></code>. The shape of <code class="docutils literal notranslate"><span class="pre">d_h</span></code> is equal to the shape of <code class="docutils literal notranslate"><span class="pre">h</span></code>. Itâ€™s 32 x 64. We also know that <code class="docutils literal notranslate"><span class="pre">d_h</span></code> should be some matrix multiplication of <code class="docutils literal notranslate"><span class="pre">d_logits</span></code> and <code class="docutils literal notranslate"><span class="pre">W2</span></code>. The shape of <code class="docutils literal notranslate"><span class="pre">d_logits</span></code> is 32 x 27 and the shape of <code class="docutils literal notranslate"><span class="pre">W2</span></code> is 64 x 27. So we can conclude that <code class="docutils literal notranslate"><span class="pre">d_h</span> <span class="pre">=</span> <span class="pre">d_logits</span> <span class="pre">&#64;</span> <span class="pre">W2.T</span></code>.</p>
<p>The shape of <code class="docutils literal notranslate"><span class="pre">d_W2</span></code> is equal to the shape of <code class="docutils literal notranslate"><span class="pre">W2</span></code>. Itâ€™s 64 x 27. We also know that <code class="docutils literal notranslate"><span class="pre">d_W2</span></code> should be some matrix multiplication of <code class="docutils literal notranslate"><span class="pre">h.T</span></code> and <code class="docutils literal notranslate"><span class="pre">d_logits</span></code>. The shape of <code class="docutils literal notranslate"><span class="pre">h.T</span></code> is 64 x 32 and the shape of <code class="docutils literal notranslate"><span class="pre">d_logits</span></code> is 32 x 27. So we can conclude that <code class="docutils literal notranslate"><span class="pre">d_W2</span> <span class="pre">=</span> <span class="pre">h.T</span> <span class="pre">&#64;</span> <span class="pre">d_logits</span></code>.</p>
<p>The shape of <code class="docutils literal notranslate"><span class="pre">d_b2</span></code> is equal to the shape of <code class="docutils literal notranslate"><span class="pre">b2</span></code>. Itâ€™s 27. We also know that <code class="docutils literal notranslate"><span class="pre">d_b2</span></code> should be the sum of <code class="docutils literal notranslate"><span class="pre">d_logits</span></code> along axis 0. The shape of <code class="docutils literal notranslate"><span class="pre">d_logits</span></code> is 32 x 27. So we can conclude that <code class="docutils literal notranslate"><span class="pre">d_b2</span> <span class="pre">=</span> <span class="pre">d_logits.sum(axis=0)</span></code>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_h</span> <span class="o">=</span> <span class="n">d_logits</span> <span class="o">@</span> <span class="n">W2</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># 32 x 64</span>
<span class="n">d_W2</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">d_logits</span>  <span class="c1"># 64 x 27</span>
<span class="n">d_b2</span> <span class="o">=</span> <span class="n">d_logits</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 1 x 27</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="n">d_h</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;W2&quot;</span><span class="p">,</span> <span class="n">d_W2</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span>
<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;b2&quot;</span><span class="p">,</span> <span class="n">d_b2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>h               | exact: True  | approximate: True  | maxdiff: 0.0
W2              | exact: True  | approximate: True  | maxdiff: 0.0
b2              | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
</section>
<section id="layer-1">
<h4>Layer 1<a class="headerlink" href="#layer-1" title="Permalink to this heading">#</a></h4>
<section id="non-linearity">
<h5>Non-linearity<a class="headerlink" href="#non-linearity" title="Permalink to this heading">#</a></h5>
<p><strong>d_h_pre_act</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">h_pre_act</span><span class="p">)</span>
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\({\large\frac{d}{dx}}\text{tanh}(x) = 1 - \text{tanh}^2(x)\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_h_pre_act</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">h</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">d_h</span>  <span class="c1"># 32 x 64</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;h_pre_act&quot;</span><span class="p">,</span> <span class="n">d_h_pre_act</span><span class="p">,</span> <span class="n">h_pre_act</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>h_pre_act       | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10
</pre></div>
</div>
</div>
</div>
</section>
<section id="batch-norm">
<h5>Batch norm<a class="headerlink" href="#batch-norm" title="Permalink to this heading">#</a></h5>
<p><strong>d_bn_gain</strong>, <strong>d_bn_raw</strong>, <strong>d_bn_bias</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">h_pre_act</span> <span class="o">=</span> <span class="n">bn_gain</span> <span class="o">*</span> <span class="n">bn_raw</span> <span class="o">+</span> <span class="n">bn_bias</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">bn_gain</span></code> and <code class="docutils literal notranslate"><span class="pre">bn_bias</span></code> are 1 x 64 tensors. <code class="docutils literal notranslate"><span class="pre">bn_raw</span></code> and <code class="docutils literal notranslate"><span class="pre">h_pre_act</span></code> are 32 x 64 tensors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_bn_gain</span> <span class="o">=</span> <span class="p">(</span><span class="n">bn_raw</span> <span class="o">*</span> <span class="n">d_h_pre_act</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 1 x 64</span>
<span class="n">d_bn_raw</span> <span class="o">=</span> <span class="n">bn_gain</span> <span class="o">*</span> <span class="n">d_h_pre_act</span>  <span class="c1"># 32 x 64</span>
<span class="n">d_bn_bias</span> <span class="o">=</span> <span class="n">d_h_pre_act</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 1 x 64</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;bn_gain&quot;</span><span class="p">,</span> <span class="n">d_bn_gain</span><span class="p">,</span> <span class="n">bn_gain</span><span class="p">)</span>
<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;bn_raw&quot;</span><span class="p">,</span> <span class="n">d_bn_raw</span><span class="p">,</span> <span class="n">bn_raw</span><span class="p">)</span>
<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;bn_bias&quot;</span><span class="p">,</span> <span class="n">d_bn_bias</span><span class="p">,</span> <span class="n">bn_bias</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bn_gain         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09
bn_raw          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10
bn_bias         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09
</pre></div>
</div>
</div>
</div>
<p><strong>d_bn_diff</strong> - branch 1, <strong>d_bn_var_inv</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bn_raw</span> <span class="o">=</span> <span class="n">bn_diff</span> <span class="o">*</span> <span class="n">bn_var_inv</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">bn_diff</span></code> and <code class="docutils literal notranslate"><span class="pre">bn_raw</span></code> are 32 x 64 tensors, <code class="docutils literal notranslate"><span class="pre">bn_var_inv</span></code> is a 1 x 64 tensor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_bn_diff</span> <span class="o">=</span> <span class="n">bn_var_inv</span> <span class="o">*</span> <span class="n">d_bn_raw</span>  <span class="c1"># 32 x 64</span>
<span class="n">d_bn_var_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">bn_diff</span> <span class="o">*</span> <span class="n">d_bn_raw</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 1 x 64</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;bn_var_inv&quot;</span><span class="p">,</span> <span class="n">d_bn_var_inv</span><span class="p">,</span> <span class="n">bn_var_inv</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bn_var_inv      | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09
</pre></div>
</div>
</div>
</div>
<p><strong>d_bn_var</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bn_var_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">bn_var</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span> 
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\(\large\frac{d}{dx}\frac{1}{\sqrt(x)} = -\frac{1}{2x^{3/2}}\)</span></p>
<p><code class="docutils literal notranslate"><span class="pre">bn_var_inv</span></code> and <code class="docutils literal notranslate"><span class="pre">bn_var</span></code> are 1 x 64 tensors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_bn_var</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">bn_var</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">3</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">d_bn_var_inv</span>  <span class="c1"># 1 x 64</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;bn_var&quot;</span><span class="p">,</span> <span class="n">d_bn_var</span><span class="p">,</span> <span class="n">bn_var</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bn_var          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10
</pre></div>
</div>
</div>
</div>
<p><strong>d_bn_diff2</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bn_var</span> <span class="o">=</span> <span class="n">bn_diff2</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">bs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>Toy example:
$<span class="math notranslate nohighlight">\(
\begin{align*}
  \begin{bmatrix}
    a_{11} &amp; a_{12}\\
    a_{21} &amp; a_{22}
  \end{bmatrix} -&gt;
  \begin{bmatrix}
    b_1 &amp; b_2
  \end{bmatrix} =
  \begin{bmatrix}
    (a_{11} + a_{12})/(bs-1) &amp;  (a_{21} + a_{22})/(bs-1)
  \end{bmatrix}
\end{align*}
\)</span>$</p>
<hr class="docutils" />
<p><code class="docutils literal notranslate"><span class="pre">bn_diff2</span></code> is a 32 x 64 tensor, <code class="docutils literal notranslate"><span class="pre">bn_var</span></code> is a 1 x 64 tensor.
The tensor <code class="docutils literal notranslate"><span class="pre">bn_diff2</span></code> is summed in the forward pass, therefore we have to replicate the gradients in the backward pass.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_bn_diff2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">bn_diff2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">bs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">d_bn_var</span>  <span class="c1"># 1 x 64</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;bn_diff2&quot;</span><span class="p">,</span> <span class="n">d_bn_diff2</span><span class="p">,</span> <span class="n">bn_diff2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bn_diff2        | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11
</pre></div>
</div>
</div>
</div>
<p><strong>d_bn_diff</strong> - branch 2</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bn_diff2</span> <span class="o">=</span> <span class="n">bn_diff</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\({\large\frac{d}{dx}}x^2 = 2x\)</span></p>
<p><code class="docutils literal notranslate"><span class="pre">bn_diff2</span></code> and <code class="docutils literal notranslate"><span class="pre">bn_diff</span></code> are 32 x 64 tensors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_bn_diff</span> <span class="o">+=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">bn_diff</span> <span class="o">*</span> <span class="n">d_bn_diff2</span>  <span class="c1"># 32 x 64</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;bn_diff&quot;</span><span class="p">,</span> <span class="n">d_bn_diff</span><span class="p">,</span> <span class="n">bn_diff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bn_diff         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bn_diff</span> <span class="o">=</span> <span class="n">h_pre_bn</span> <span class="o">-</span> <span class="n">bn_mean_i</span>
</pre></div>
</div>
<p><strong>d_h_pre_bn</strong> - branch 1, <strong>d_bn_mean_i</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bn_diff</span> <span class="o">=</span> <span class="n">h_pre_bn</span> <span class="o">-</span> <span class="n">bn_mean_i</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">bn_diff</span></code> and <code class="docutils literal notranslate"><span class="pre">h_pre_bn</span></code> are 32 x 64 tensors. <code class="docutils literal notranslate"><span class="pre">bn_mean_i</span></code> is a 1 x 64 tensor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_h_pre_bn</span> <span class="o">=</span> <span class="n">d_bn_diff</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># 32 x 64</span>
<span class="n">d_bn_mean_i</span> <span class="o">=</span> <span class="o">-</span><span class="n">d_bn_diff</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 1 x 64</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;bn_mean_i&quot;</span><span class="p">,</span> <span class="n">d_bn_mean_i</span><span class="p">,</span> <span class="n">bn_mean_i</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bn_mean_i       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09
</pre></div>
</div>
</div>
</div>
<p><strong>d_h_pre_bn</strong> - branch 2</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bn_mean_i</span> <span class="o">=</span> <span class="n">h_pre_bn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="n">bs</span> 
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">bn_mean_i</span></code> is a 1 x 64 tensor, <code class="docutils literal notranslate"><span class="pre">h_pre_bn</span></code> is a 32 x 64 tensor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_h_pre_bn</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">h_pre_bn</span><span class="p">)</span> <span class="o">/</span> <span class="n">bs</span> <span class="o">*</span> <span class="n">d_bn_mean_i</span>  <span class="c1"># 32 x 64</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;h_pre_bn&quot;</span><span class="p">,</span> <span class="n">d_h_pre_bn</span><span class="p">,</span> <span class="n">h_pre_bn</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>h_pre_bn        | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10
</pre></div>
</div>
</div>
</div>
</section>
<section id="pre-batch-norm">
<h5>Pre-batch norm<a class="headerlink" href="#pre-batch-norm" title="Permalink to this heading">#</a></h5>
<p><strong>d_emb_cat</strong>, <strong>d_W1</strong>, <strong>d_b1</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">h_pre_bn</span> <span class="o">=</span> <span class="n">emb_cat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">h_pre_bn</span></code> and <code class="docutils literal notranslate"><span class="pre">emb_cat</span></code> are 32 x 64 tensors, <code class="docutils literal notranslate"><span class="pre">W1</span></code> is a 30 x 64 tensor and <code class="docutils literal notranslate"><span class="pre">b1</span></code> is a 64d vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_emb_cat</span> <span class="o">=</span> <span class="n">d_h_pre_bn</span> <span class="o">@</span> <span class="n">W1</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># 32 x 30</span>
<span class="n">d_W1</span> <span class="o">=</span> <span class="n">emb_cat</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">d_h_pre_bn</span>  <span class="c1"># 30 x 64</span>
<span class="n">d_b1</span> <span class="o">=</span> <span class="n">d_h_pre_bn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 64</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;emb_cat&quot;</span><span class="p">,</span> <span class="n">d_emb_cat</span><span class="p">,</span> <span class="n">emb_cat</span><span class="p">)</span>
<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;W1&quot;</span><span class="p">,</span> <span class="n">d_W1</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span>
<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;b1&quot;</span><span class="p">,</span> <span class="n">d_b1</span><span class="p">,</span> <span class="n">b1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>emb_cat         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09
W1              | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09
b1              | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="embedding-layer">
<h3>Embedding layer<a class="headerlink" href="#embedding-layer" title="Permalink to this heading">#</a></h3>
<p><strong>d_emb</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">emb_cat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">emb_cat</span></code> is a 32 x 30 tensor, <code class="docutils literal notranslate"><span class="pre">emb</span></code> is a 32 x 3 x 10 tensor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_emb</span> <span class="o">=</span> <span class="n">d_emb_cat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 32 x 3 x 10</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;emb&quot;</span><span class="p">,</span> <span class="n">d_emb</span><span class="p">,</span> <span class="n">emb</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>emb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09
</pre></div>
</div>
</div>
</div>
<p><strong>d_C</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xb</span><span class="p">]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">emb</span></code> is a 32 x 3 x 10 tensor, <code class="docutils literal notranslate"><span class="pre">C</span></code> is a 27 x 10 tensor and <code class="docutils literal notranslate"><span class="pre">Xb</span></code> is a 32 x 3 tensor.</p>
<p>Each element of <code class="docutils literal notranslate"><span class="pre">Xb</span></code> a character index. Each character index is used to select a row (10d vector) from <code class="docutils literal notranslate"><span class="pre">C</span></code>.<br />
We can calculate the backward pass in different ways.</p>
<ul class="simple">
<li><p>Naive: we first create a zero tensor <code class="docutils literal notranslate"><span class="pre">d_C</span></code> with the same shape as <code class="docutils literal notranslate"><span class="pre">C</span></code> (27 x 10). Then we loop over the 32 samples from <code class="docutils literal notranslate"><span class="pre">Xb</span></code>. For each sample, we loop over the 3 character indices. For each character index, we add the corresponding 10d vector from <code class="docutils literal notranslate"><span class="pre">d_emb</span></code> to the corresponding row in <code class="docutils literal notranslate"><span class="pre">d_C</span></code>.</p></li>
<li><p>Vectorised 1: we first transform each element of <code class="docutils literal notranslate"><span class="pre">Xb</span></code> into a 27d one-hot vector. <code class="docutils literal notranslate"><span class="pre">Xb_1_hot</span></code> is then a 32 x 3 x 27 tensor with 32 samples, each having 3 one-hot vectors. We then reshape <code class="docutils literal notranslate"><span class="pre">Xb_1_hot</span></code> into a (32x3) x 27 tensor and <code class="docutils literal notranslate"><span class="pre">d_emb</span></code> into a (32x3) x 10 tensor. Then <code class="docutils literal notranslate"><span class="pre">d_C</span> <span class="pre">=</span> <span class="pre">Xb_1_hot</span> <span class="pre">&#64;</span> <span class="pre">d_emb</span></code>, which is a 27 x 10 tensor.</p></li>
<li><p>Vectorised 2: We can take the dot product of â€˜Xb_1_hotâ€™ and â€˜d_embâ€™ using <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.tensordot.html">torch.tensordot</a>.</p></li>
<li><p>Vectorised 3: We can use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.einsum.html">torch.einsum</a>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Naive implementation 1</span>
<span class="n">d_C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>  <span class="c1"># 27 x 10</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">Xb</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="n">d_C</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">+=</span> <span class="n">d_emb</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C               | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Vectorized implementation 1</span>
<span class="n">one_hot_Xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">27</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>  <span class="c1"># 32 x 3 x 27</span>
<span class="n">d_C</span> <span class="o">=</span> <span class="n">one_hot_Xb</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">d_emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># 27 x 10</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C               | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Vectorized implementation 2</span>
<span class="n">Xb_1hot</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>  <span class="c1"># 32 x 3 x 27</span>
<span class="n">d_C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">Xb_1hot</span><span class="p">,</span> <span class="n">d_emb</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>  <span class="c1"># type: ignore 27 x 10</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C               | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Vectorized implementation 3</span>
<span class="n">Xb_1hot</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>  <span class="c1"># 32 x 3 x 27</span>
<span class="n">d_C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;ijk,ijl-&gt;kl&quot;</span><span class="p">,</span> <span class="n">Xb_1hot</span><span class="p">,</span> <span class="n">d_emb</span><span class="p">)</span>  <span class="c1">#  27 x 10</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C               | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="backpropagation-through-cross-entropy-loss">
<h2>Backpropagation through cross entropy loss<a class="headerlink" href="#backpropagation-through-cross-entropy-loss" title="Permalink to this heading">#</a></h2>
<p>In the previous sections we calculated the cross entropy loss in very small atomic steps and we backpropagated through each step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Cross entropy loss</span>
<span class="n">logit_maxes</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>  
<span class="n">norm_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">logit_maxes</span> 
<span class="n">counts</span> <span class="o">=</span> <span class="n">norm_logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>  
<span class="n">counts_sum</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">counts_sum_inv</span> <span class="o">=</span> <span class="n">counts_sum</span><span class="o">**-</span><span class="mi">1</span>  
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>  
<span class="n">logprobs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>  
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">logprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<p>In this section we will calculate the cross entropy loss in one step by using <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html">F.cross_entropy</a> and manually backpropagate through it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fast</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;loss_fast: </span><span class="si">{</span><span class="n">loss_fast</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, diff: </span><span class="si">{</span><span class="p">(</span><span class="n">loss_fast</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss_fast: 3.4002, loss: 3.4002, diff: 0.0000
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">loss_fast</span></code> is much faster thant the previous <code class="docutils literal notranslate"><span class="pre">loss</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it
<span class="n">logit_maxes</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>  
<span class="n">norm_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">logit_maxes</span> 
<span class="n">counts</span> <span class="o">=</span> <span class="n">norm_logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>  
<span class="n">counts_sum</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">counts_sum_inv</span> <span class="o">=</span> <span class="n">counts_sum</span><span class="o">**-</span><span class="mi">1</span>  
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>  
<span class="n">logprobs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>  
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">logprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>115 Âµs Â± 627 ns per loop (mean Â± std. dev. of 7 runs, 10,000 loops each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>it
<span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>17.1 Âµs Â± 687 ns per loop (mean Â± std. dev. of 7 runs, 100,000 loops each)
</pre></div>
</div>
</div>
</div>
<section id="id1">
<h3>Backward pass<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>We will derive the mathematical expressions for calculatind <code class="docutils literal notranslate"><span class="pre">d_logits</span></code> in one step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fast</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="cross_entropy_diag" src="../../_images/cross_entropy_diag.png" /></p>
<p>For a single sample, the cross entropy loss is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
loss &amp;= -log (p_y)\\
p_i &amp;= \frac{e^{l_i}}{\sum_j e^{l_j}}\\
\Rightarrow loss &amp;= -log\frac{e^{l_y}}{\sum_j e^{l_j}}
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is a vector of all probablilties, <span class="math notranslate nohighlight">\(p_y\)</span> are the probabilities of the label (character) <span class="math notranslate nohighlight">\(y\)</span>, <span class="math notranslate nohighlight">\(p_i\)</span> are the probabilities of the <span class="math notranslate nohighlight">\(i\)</span>-th sample and <span class="math notranslate nohighlight">\(l_i\)</span> the logit of the <span class="math notranslate nohighlight">\(i\)</span>-th sample.</p>
<p>The derivative of the cross entropy loss with respect to the <span class="math notranslate nohighlight">\(i\)</span>-th logit is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
  \frac{d}{dl_i}loss = \frac{d}{dl_i}(-log\frac{e^{l_y}}{\sum_j e^{l_j}})\\
\end{align*}
\end{split}\]</div>
<p>using <span class="math notranslate nohighlight">\({\large\frac{d}{dx}}log(x) =\large\frac{1}{x}\)</span><br />
$<span class="math notranslate nohighlight">\(
\begin{align*}
  &amp; = -\frac{\sum_j e^{l_j}}{e^{l_y}}\frac{d}{dl_i}(\frac{e^{l_y}}{\sum_j e^{l_j}}) \\
  &amp; = -\frac{\sum_j{e^{l_j}}}{e^{l_y}} {\left[\frac{e^{l_y}}{\sum_j e^{l_j}} -\frac{e^{l_y}e^{l_i}}{(\sum_j e^{l_j})^2} \right]}\\
\end{align*}
\)</span>$</p>
<p>if <span class="math notranslate nohighlight">\(i \neq y\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
&amp; =\frac{\sum_j{e^{l_j}}}{e^{l_y}} {\left[0 + \frac{e^{l_y}e^{l_i}}{(\sum_j{e^{l_j}})^2}  \right]}\\
&amp; =\frac{e^{l_i}}{\sum_j{e^{l_j}}}\\
&amp; =p_i
\end{align*}
\end{split}\]</div>
<p>if <span class="math notranslate nohighlight">\(i = y\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
&amp; = -{\left[ 1-\frac{e^{l_i}}{\sum_j{e^{l_j}}} \right]}\\
&amp; = \frac{e^{l_i}}{\sum_j{e^{l_j}}} - 1\\
&amp; = p_i - 1
\end{align*}
\end{split}\]</div>
<p>The gradient is either the softmax (<span class="math notranslate nohighlight">\(p_i\)</span>) or the softmax minus 1 (<span class="math notranslate nohighlight">\(p_i-1\)</span>) depending on whether the logit .<br />
This is the gradient of a single sample <span class="math notranslate nohighlight">\(i\)</span>. The loss of a batch is the average of the losses of all samples in the batch. We have to backpropagate through the average as well. The gradient of the average is <span class="math notranslate nohighlight">\(\large\frac{1}{bs}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_logits</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 32 x 27</span>
<span class="c1"># substract 1 from the correct class</span>
<span class="n">d_logits</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span> <span class="o">-=</span> <span class="mf">1.0</span>
<span class="c1"># backprop through averaging</span>
<span class="n">d_logits</span> <span class="o">/=</span> <span class="n">bs</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">d_logits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>logits          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-09
</pre></div>
</div>
</div>
</div>
</section>
<section id="intuition-of-cross-entropy-loss">
<h3>Intuition of cross-entropy loss<a class="headerlink" href="#intuition-of-cross-entropy-loss" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">d_logits</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;d_logits&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/8e994bbeeaf68dacbfb7545cfb94b7361912a1387894ec75d41a298cd347cc5f.png" src="../../_images/8e994bbeeaf68dacbfb7545cfb94b7361912a1387894ec75d41a298cd347cc5f.png" />
</div>
</div>
<p>The black <code class="docutils literal notranslate"><span class="pre">d_logits</span></code> squares are the position of the correct indices where we substracted 1 from the gradient. The other squares are the positions where <code class="docutils literal notranslate"><span class="pre">d_logits</span> <span class="pre">*</span> <span class="pre">bs</span></code> is equal to the probability.</p>
<p>Letâ€™s look at 1 sample:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y:        </span><span class="si">{</span><span class="n">Yb</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;probs:    </span><span class="si">{</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="n">s</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d_logits: </span><span class="si">{</span><span class="n">d_logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">bs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;diff:     </span><span class="si">{</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="n">s</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">d_logits</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">*</span><span class="n">bs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y:        3
probs:    tensor([0.0169, 0.0170, 0.0293, 0.0593, 0.0290, 0.0248, 0.0547, 0.0782, 0.0586, 0.0179, 0.0154, 0.0233, 0.0286, 0.1021,
        0.0118, 0.0096, 0.0166, 0.0185, 0.0146, 0.1269, 0.0520, 0.0301, 0.0391, 0.0276, 0.0204, 0.0548, 0.0231],
       grad_fn=&lt;SelectBackward0&gt;)
d_logits: tensor([ 0.0169,  0.0170,  0.0293, -0.9407,  0.0290,  0.0248,  0.0547,  0.0782,  0.0586,  0.0179,  0.0154,  0.0233,
         0.0286,  0.1021,  0.0118,  0.0096,  0.0166,  0.0185,  0.0146,  0.1269,  0.0520,  0.0301,  0.0391,  0.0276,
         0.0204,  0.0548,  0.0231], grad_fn=&lt;MulBackward0&gt;)
diff:     tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       grad_fn=&lt;SubBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">probs</span></code> sums to 1, <code class="docutils literal notranslate"><span class="pre">d_logits</span></code> sums to 0. This is logic because <code class="docutils literal notranslate"><span class="pre">d_logits</span></code> is equal to <code class="docutils literal notranslate"><span class="pre">probs</span></code> except in exactly one place where we substracted 1 .</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sum of probs: </span><span class="si">{</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sum of d_logits: </span><span class="si">{</span><span class="n">d_logits</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>sum of probs: 1.0
sum of d_logits: -9.313225746154785e-10
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The intuition is to think about the <code class="docutils literal notranslate"><span class="pre">d_logits</span></code> gradients of each sample as a force that pushes the logits up for position corresponding to the correct label and pushes the logits down for all other positions. The total amount of force is 0 because the sum of the gradients is 0.<br />
The amount of force that we are applying to each position is proportional to the probability of that position. If the probabilities are exactly correct then the force that we apply to each position is 0.  The <code class="docutils literal notranslate"><span class="pre">probs</span></code> is 0 everywhere except for the correct position where it is 1. <code class="docutils literal notranslate"><span class="pre">d_logits</span></code> is 0 everywhere. If we have a confidently mispredicted element then the <code class="docutils literal notranslate"><span class="pre">probs</span></code> is ~0 everywhere except for the mispredicted position where it is ~1. <code class="docutils literal notranslate"><span class="pre">d_logits</span></code> is ~0 everywhere except for the mispredicted position where it is ~-1 and ~1 for the correct position. The force applied to the mispredicted position pulls that logit down and the equal force is applied to the correct position and pushes that logit up.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exacly correct</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y:             </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">probs_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">probs_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;probs:         </span><span class="si">{</span><span class="n">probs_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">d_logits_</span> <span class="o">=</span> <span class="n">probs_</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">d_logits_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">-=</span> <span class="mf">1.0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d_logits:      </span><span class="si">{</span><span class="n">d_logits_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;probs_updated: </span><span class="si">{</span><span class="n">probs_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">d_logits_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y:             3
probs:         tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
d_logits:      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
probs_updated: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Confidently wrong</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y:             </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">probs_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">probs_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># confident wrong predicting 0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;probs:         </span><span class="si">{</span><span class="n">probs_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">d_logits_</span> <span class="o">=</span> <span class="n">probs_</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">d_logits_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">-=</span> <span class="mf">1.0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d_logits:      </span><span class="si">{</span><span class="n">d_logits_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;probs_updated: </span><span class="si">{</span><span class="n">probs_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">d_logits_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y:             3
probs:         tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
d_logits:      tensor([ 1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.])
probs_updated: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="backpropagation-through-batchnorm">
<h2>Backpropagation through batchnorm<a class="headerlink" href="#backpropagation-through-batchnorm" title="Permalink to this heading">#</a></h2>
<p>In the previous sections we calculated the batchnorm in very small atomic steps and we backpropagated through each step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># batchnorm</span>
<span class="n">bn_mean_i</span> <span class="o">=</span> <span class="n">h_pre_bn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="n">bs</span>  
<span class="n">bn_diff</span> <span class="o">=</span> <span class="n">h_pre_bn</span> <span class="o">-</span> <span class="n">bn_mean_i</span>  
<span class="n">bn_diff2</span> <span class="o">=</span> <span class="n">bn_diff</span><span class="o">**</span><span class="mi">2</span>  
<span class="n">bn_var</span> <span class="o">=</span> <span class="n">bn_diff2</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">bs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">bn_var_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">bn_var</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
<span class="n">bn_raw</span> <span class="o">=</span> <span class="n">bn_diff</span> <span class="o">*</span> <span class="n">bn_var_inv</span>  
<span class="n">h_pre_act</span> <span class="o">=</span> <span class="n">bn_gain</span> <span class="o">*</span> <span class="n">bn_raw</span> <span class="o">+</span> <span class="n">bn_bias</span>
</pre></div>
</div>
<p>In this section we will calculate the batchnorm in one step by using the formula <span class="math notranslate nohighlight">\(y_i=\gamma{\large\frac{x_i-\mu}{\sqrt{\sigma^2+\epsilon}}} + \beta\)</span> and manually backpropagate through it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h_pre_act_fast</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">bn_gain</span>
    <span class="o">*</span> <span class="p">(</span><span class="n">h_pre_bn</span> <span class="o">-</span> <span class="n">h_pre_bn</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">h_pre_bn</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">bn_bias</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;max diff:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">h_pre_act_fast</span> <span class="o">-</span> <span class="n">h_pre_act</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>max diff: tensor(4.7684e-07, grad_fn=&lt;MaxBackward1&gt;)
</pre></div>
</div>
</div>
</div>
<section id="id2">
<h3>Backward pass<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>We will derive the mathematical expressions for calculating <code class="docutils literal notranslate"><span class="pre">d_h_pre_bn</span></code> in one step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">h_pre_act_fast</span> <span class="o">=</span> <span class="n">bn_gain</span><span class="o">*</span><span class="p">(</span><span class="n">h_pre_bn</span> <span class="o">-</span> <span class="n">h_pre_bn</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">&amp;=</span><span class="kc">True</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">h_pre_bn</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">&amp;=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">+</span> <span class="n">bn_bias</span>
</pre></div>
</div>
<p><img alt="batchnorm_diag" src="../../_images/batchnorm_diag.png" /></p>
<p><span class="math notranslate nohighlight">\(\mu = \frac{1}{m}\sum_{i}^{m}x_i\\\)</span>
<span class="math notranslate nohighlight">\(\sigma^2 = \frac{1}{m-1}\sum_{i}^{m}(x_i-\mu)^2\\\)</span>
<span class="math notranslate nohighlight">\(\hat{x_i} = \frac{x_i-\mu}{\sqrt{\sigma^2+\epsilon}}\\\)</span>
<span class="math notranslate nohighlight">\(y_i = \gamma\hat{x_i} + \beta\\\)</span></p>
<p>We have <span class="math notranslate nohighlight">\(\large\frac{\delta L}{\delta y_1}\\\)</span><br />
We want <span class="math notranslate nohighlight">\(\large\frac{\delta L}{\delta x_1}\\\)</span></p>
<p>(1)</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
  \frac{\delta L}{\delta \hat{x_i}} &amp;= \frac{\delta L}{\delta y_i}*\gamma
\end{align*}
\]</div>
<p>(2)<br />
The <span class="math notranslate nohighlight">\(i\)</span> components of <span class="math notranslate nohighlight">\(\hat x\)</span> are dependent on <span class="math notranslate nohighlight">\(\sigma\)</span> in the forward pass. Therefore we have to sum over all <span class="math notranslate nohighlight">\(i\)</span>â€™s in the backward pass.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
  \frac{\delta L}{\delta \sigma^2} &amp;= \sum_{i}\frac{\delta \hat{x_i}}{\delta \sigma^2}\frac{\delta L}{\delta \hat{x_i}}\\ 
  &amp;= \gamma*\sum_{i}\frac{\delta}{\delta \sigma^2}\left[\frac{x_i-\mu}{\sqrt{\sigma^2+\epsilon}}\right]\frac{\delta L}{\delta y_i}\\
  &amp;= -\frac{1}{2}*\gamma*\sum_{i}\frac{\delta}{\delta y_i}{\left[\frac{x_i-\mu}{(\sigma^2+\epsilon)^{\frac{3}{2}}}\right]}\\
  \end{align*}
\end{split}\]</div>
<p>(3)<br />
The <span class="math notranslate nohighlight">\(i\)</span> components of <span class="math notranslate nohighlight">\(\hat x\)</span> are dependent on <span class="math notranslate nohighlight">\(\mu\)</span> in the forward pass. Therefore we have to sum over all <span class="math notranslate nohighlight">\(i\)</span>â€™s in the backward pass. Additionally we <span class="math notranslate nohighlight">\(\sigma\)</span> is dependent on <span class="math notranslate nohighlight">\(\mu\)</span> and therefore we also have to add the gradient of <span class="math notranslate nohighlight">\(\sigma\)</span> with respect to <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
  \frac{\delta L}{\delta \mu} &amp;= \sum_{i}\frac{\delta \hat{x_i}}{\delta \mu}\frac{\delta L}{\delta \hat{x_i}} + \frac{\delta \sigma^2}{\delta \mu}\frac{\delta L}{\delta \sigma^2}\\\\
  \frac{\delta\hat{x_i}}{\delta \mu} &amp;= \frac{\delta}{\delta \mu}\left[\frac{x_i-\mu}{\sqrt{\sigma^2+\epsilon}}\right]\\
  &amp;= -\frac{1}{\sqrt{\sigma^2+\epsilon}}\\\\
  \frac{\delta\sigma^2}{\delta \mu} &amp;= \frac{\delta}{\delta \mu}\left[\frac{1}{m-1}\sum_{i}(x_i-\mu)^2\right]\\
  &amp;= \frac{-2}{m-1}\sum_{i}(x_i-\mu)\\\\
  &amp;= \frac{-2}{m-1}\sum_{i}x_i -\sum_{i}-\mu\\
  &amp;= \frac{-2}{m-1}\left[m\mu -m\mu\right] \quad(\textsf{because }\small\mu=\frac{1}{m}\sum_{i}x_i)\\
  &amp;= 0\\\\
  \Rightarrow  \frac{\delta L}{\delta \mu} &amp;= -\frac{\gamma}{\sqrt{\sigma^2+\epsilon}}\sum_{i}\frac{\delta L}{\delta y_i} + 0\\
\end{align*}
\end{split}\]</div>
<p>(4)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
  \frac{\delta L}{\delta x_i} &amp;= \frac{\delta \hat{x_i}}{\delta x_i}\frac{\delta L}{\delta \hat{x_i}} + \frac{\delta \mu}{\delta x_i}\frac{\delta L}{\delta \mu} + \frac{\delta \sigma^2}{\delta x_i}\frac{\delta L}{\delta \sigma^2}\\\\
  \frac{\delta \hat{x_i}}{\delta x_i} &amp;= \frac{\delta}{\delta x_i}\left[\frac{x_i-\mu}{\sqrt{\sigma^2+\epsilon}}\right]\\
  &amp;= \frac{1}{\sqrt{\sigma^2+\epsilon}}\\\\
  \frac{\delta \mu}{\delta x_i} &amp;= \frac{\delta}{\delta x_i}\left[\frac{1}{m}\sum_{i}x_i\right]\\
  &amp;= \frac{1}{m}\\\\
  \frac{\delta \sigma^2}{\delta x_i} &amp;= \frac{\delta}{\delta x_i}\left[\frac{1}{m-1}\sum_{i}(x_i-\mu)^2\right]\\
  &amp;= \frac{2}{m-1}(x_i-\mu)\\\\
  \Rightarrow\frac{\delta L}{\delta x_i} &amp;= \frac{1}{\sqrt{\sigma^2+\epsilon}}\frac{\delta L}{\delta \hat{y_i}}*\gamma + \left[-\frac{1}{m}\frac{\gamma}{\sqrt{\sigma^2+\epsilon}}\sum_{j}\frac{\delta L}{\delta y_j}\right] + \left[\frac{2}{m-1}(x_i-\mu)\right]\left[-\frac{1}{2}\gamma\sum_{j}\frac{\delta L}{\delta y_j}\frac{x_j-\mu}{(\sigma^2+\epsilon)^{\frac{3}{2}}}\right]\\
  &amp;= \frac{1}{\sqrt{\sigma^2+\epsilon}}\frac{\delta L}{\delta \hat{y_i}}*\gamma - \left[\frac{1}{m}\frac{\gamma}{\sqrt{\sigma^2+\epsilon}}\sum_{j}\frac{\delta L}{\delta y_j}\right] - \left[\frac{1}{m-1}\frac{x_i-\mu}{\sqrt{\sigma^2+\epsilon}}\right]\left[\frac{\gamma}{\sqrt{\sigma^2+\epsilon}}\sum_{j}\frac{\delta L}{\delta y_j}\frac{x_j-\mu}{\sqrt{\sigma^2+\epsilon}}\right]\\
  \left(\textsf{because }\small\hat{x_i}=\frac{x_i-\mu}{\sqrt{\sigma^2+\epsilon}}\right)\\
  &amp;= \frac{\gamma}{m\sqrt{\sigma^2+\epsilon}}\left[m\frac{\delta L}{\delta y_i}-\sum_{j}\frac{\delta L}{\delta y_j}-\frac{m}{m-1}\hat{x_i}\sum_{j}\frac{\delta L}{\delta y_j}\hat{x_j}\right]\\
\end{align*}
\end{split}\]</div>
<p>The formula for <span class="math notranslate nohighlight">\(\large\frac{\delta L}{\delta x_i}\)</span> is for a single neuron and a batch of 32 samples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_h_pre_bn</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">bn_gain</span>
    <span class="o">*</span> <span class="n">bn_var_inv</span>
    <span class="o">/</span> <span class="n">bs</span>
    <span class="o">*</span> <span class="p">(</span>
        <span class="n">bs</span> <span class="o">*</span> <span class="n">d_h_pre_act</span>
        <span class="o">-</span> <span class="n">d_h_pre_act</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">bs</span> <span class="o">/</span> <span class="p">(</span><span class="n">bs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">bn_raw</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_h_pre_act</span> <span class="o">*</span> <span class="n">bn_raw</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">compare_gradients</span><span class="p">(</span><span class="s2">&quot;h_pre_bn&quot;</span><span class="p">,</span> <span class="n">d_h_pre_bn</span><span class="p">,</span> <span class="n">h_pre_bn</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>h_pre_bn        | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="train-the-mlp-model-with-manual-backpropagation">
<h2>Train the MLP model with manual backpropagation<a class="headerlink" href="#train-the-mlp-model-with-manual-backpropagation" title="Permalink to this heading">#</a></h2>
<p>We will now train our MLP model with the manual backpropagation algorithm.</p>
<p><strong>Model</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_emb</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># the dimensionality of the character embedding vectors</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># the number of neurons in the hidden layer of the MLP</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># for reproducibility</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_emb</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>

<span class="c1"># Layer 1</span>
<span class="n">W1</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">n_emb</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="o">*</span> <span class="p">(</span><span class="mi">5</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span>
    <span class="o">/</span> <span class="p">((</span><span class="n">n_emb</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>

<span class="c1"># Layer 2</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>

<span class="c1"># BatchNorm parameters</span>
<span class="n">bn_gain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">+</span> <span class="mf">1.0</span>
<span class="n">bn_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bn_gain</span><span class="p">,</span> <span class="n">bn_bias</span><span class="p">]</span>
<span class="n">n_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total # parameters: </span><span class="si">{</span><span class="n">n_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total # parameters: 4,137
</pre></div>
</div>
</div>
</div>
<p><strong>Training</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># same optimization as last time</span>
<span class="n">max_steps</span> <span class="o">=</span> <span class="mi">200000</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">loss_i</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># use this context manager for efficiency once your backward pass is written (TODO)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># kick off optimization</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
        <span class="c1"># minibatch construct</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtr</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>  <span class="c1"># batch X,Y</span>

        <span class="c1"># forward pass</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xb</span><span class="p">]</span>  <span class="c1"># embed the characters into vectors</span>
        <span class="n">emb_cat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concatenate the vectors</span>
        <span class="c1"># Linear layer</span>
        <span class="n">h_pre_bn</span> <span class="o">=</span> <span class="n">emb_cat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c1"># hidden layer pre-activation</span>
        <span class="c1"># BatchNorm layer</span>
        <span class="c1"># -------------------------------------------------------------</span>
        <span class="n">bn_mean</span> <span class="o">=</span> <span class="n">h_pre_bn</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">bn_var</span> <span class="o">=</span> <span class="n">h_pre_bn</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">bn_var_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">bn_var</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
        <span class="n">bn_raw</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_pre_bn</span> <span class="o">-</span> <span class="n">bn_mean</span><span class="p">)</span> <span class="o">*</span> <span class="n">bn_var_inv</span>
        <span class="n">h_pre_act</span> <span class="o">=</span> <span class="n">bn_gain</span> <span class="o">*</span> <span class="n">bn_raw</span> <span class="o">+</span> <span class="n">bn_bias</span>
        <span class="c1"># -------------------------------------------------------------</span>
        <span class="c1"># Non-linearity</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">h_pre_act</span><span class="p">)</span>  <span class="c1"># hidden layer</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">W2</span> <span class="o">+</span> <span class="n">b2</span>  <span class="c1"># output layer</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>  <span class="c1"># loss function</span>

        <span class="c1"># backward pass</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># loss.backward() # use this for correctness comparisons, delete it later!</span>

        <span class="c1"># manual backprop!</span>
        <span class="c1"># -----------------</span>
        <span class="n">d_logits</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">d_logits</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">),</span> <span class="n">Yb</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="n">d_logits</span> <span class="o">/=</span> <span class="n">bs</span>
        <span class="c1"># 2nd layer backprop</span>
        <span class="n">d_h</span> <span class="o">=</span> <span class="n">d_logits</span> <span class="o">@</span> <span class="n">W2</span><span class="o">.</span><span class="n">T</span>
        <span class="n">d_W2</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">d_logits</span>
        <span class="n">d_b2</span> <span class="o">=</span> <span class="n">d_logits</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># tanh</span>
        <span class="n">d_h_pre_act</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">h</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">d_h</span>
        <span class="c1"># batchnorm backprop</span>
        <span class="n">d_bn_gain</span> <span class="o">=</span> <span class="p">(</span><span class="n">bn_raw</span> <span class="o">*</span> <span class="n">d_h_pre_act</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">d_bn_bias</span> <span class="o">=</span> <span class="n">d_h_pre_act</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">d_h_pre_bn</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">bn_gain</span>
            <span class="o">*</span> <span class="n">bn_var_inv</span>
            <span class="o">/</span> <span class="n">bs</span>
            <span class="o">*</span> <span class="p">(</span>
                <span class="n">bs</span> <span class="o">*</span> <span class="n">d_h_pre_act</span>
                <span class="o">-</span> <span class="n">d_h_pre_act</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="o">-</span> <span class="n">bs</span> <span class="o">/</span> <span class="p">(</span><span class="n">bs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">bn_raw</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_h_pre_act</span> <span class="o">*</span> <span class="n">bn_raw</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># 1st layer</span>
        <span class="n">d_emb_cat</span> <span class="o">=</span> <span class="n">d_h_pre_bn</span> <span class="o">@</span> <span class="n">W1</span><span class="o">.</span><span class="n">T</span>
        <span class="n">d_W1</span> <span class="o">=</span> <span class="n">emb_cat</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">d_h_pre_bn</span>
        <span class="n">d_b1</span> <span class="o">=</span> <span class="n">d_h_pre_bn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># embedding</span>
        <span class="n">d_emb</span> <span class="o">=</span> <span class="n">d_emb_cat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">d_C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">ix</span> <span class="o">=</span> <span class="n">Xb</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
                <span class="n">d_C</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">+=</span> <span class="n">d_emb</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">d_C</span><span class="p">,</span> <span class="n">d_W1</span><span class="p">,</span> <span class="n">d_b1</span><span class="p">,</span> <span class="n">d_W2</span><span class="p">,</span> <span class="n">d_b2</span><span class="p">,</span> <span class="n">d_bn_gain</span><span class="p">,</span> <span class="n">d_bn_bias</span><span class="p">]</span>
        <span class="c1"># -----------------</span>

        <span class="c1"># update</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100000</span> <span class="k">else</span> <span class="mf">0.01</span>  <span class="c1"># step learning rate decay</span>
        <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
            <span class="c1"># p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>  <span class="c1"># new way of swole doge TODO: enable</span>

        <span class="c1"># track stats</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># print every once in a while</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">max_steps</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">loss_i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/ 200000: 3.3884
  10000/ 200000: 2.4541
  20000/ 200000: 2.4857
  30000/ 200000: 2.1198
  40000/ 200000: 2.3114
  50000/ 200000: 1.9466
  60000/ 200000: 2.5517
  70000/ 200000: 2.2360
  80000/ 200000: 1.9703
  90000/ 200000: 2.2679
 100000/ 200000: 2.3220
 110000/ 200000: 1.6446
 120000/ 200000: 2.2917
 130000/ 200000: 2.1344
 140000/ 200000: 2.2928
 150000/ 200000: 1.8781
 160000/ 200000: 2.2685
 170000/ 200000: 2.2070
 180000/ 200000: 1.9090
 190000/ 200000: 1.8184
</pre></div>
</div>
</div>
</div>
<p><strong>Calibrate the batch norm</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># pass the training set through</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">Xtr</span><span class="p">]</span>
    <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="c1"># measure the mean/std over the entire training set</span>
    <span class="n">bnmean</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bnvar</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Evaluate train and val loss</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># this decorator disables gradient tracking</span>
<span class="k">def</span> <span class="nf">split_loss</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">),</span>
        <span class="s2">&quot;val&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">),</span>
        <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xte</span><span class="p">,</span> <span class="n">Yte</span><span class="p">),</span>
    <span class="p">}[</span><span class="n">split</span><span class="p">]</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>  <span class="c1"># (N, block_size, n_embd)</span>
    <span class="n">emb_cat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concat into (N, block_size * n_embd)</span>
    <span class="n">h_pre_act</span> <span class="o">=</span> <span class="n">emb_cat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">h_pre_act</span> <span class="o">=</span> <span class="n">bn_gain</span> <span class="o">*</span> <span class="p">(</span><span class="n">h_pre_act</span> <span class="o">-</span> <span class="n">bnmean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">bnvar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">+</span> <span class="n">bn_bias</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">h_pre_act</span><span class="p">)</span>  <span class="c1"># (N, n_hidden)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">W2</span> <span class="o">+</span> <span class="n">b2</span>  <span class="c1"># (N, vocab_size)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>


<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;val&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 2.1481568813323975
val 2.157963275909424
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># sample from the model</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">2147483647</span> <span class="o">+</span> <span class="mi">10</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>  <span class="c1"># initialize with all ...</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="c1"># ------------</span>
        <span class="c1"># forward pass:</span>
        <span class="c1"># Embedding</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">context</span><span class="p">])]</span>  <span class="c1"># (1,block_size,d)</span>
        <span class="n">emb_cat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concat into (N, block_size * n_embd)</span>
        <span class="n">h_pre_act</span> <span class="o">=</span> <span class="n">emb_cat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span>
        <span class="n">h_pre_act</span> <span class="o">=</span> <span class="n">bn_gain</span> <span class="o">*</span> <span class="p">(</span><span class="n">h_pre_act</span> <span class="o">-</span> <span class="n">bnmean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">bnvar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">+</span> <span class="n">bn_bias</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">h_pre_act</span><span class="p">)</span>  <span class="c1"># (N, n_hidden)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">W2</span> <span class="o">+</span> <span class="n">b2</span>  <span class="c1"># (N, vocab_size)</span>
        <span class="c1"># ------------</span>
        <span class="c1"># Sample</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>
        <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ix2ch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mora.
kayanniee.
madhayla.
renyastendra.
gradered.
eliah.
miloe.
leigh.
van.
aar.
kayzioh.
kalin.
shabergahiriel.
kindreelynn.
novalayubelynder.
yah.
fael.
yuma.
myston.
azhil.
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/build_language_model"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="3.2_activations_gradients_batchnorm.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Activations, Gradients and Batch Normalisation - Part 2</p>
      </div>
    </a>
    <a class="right-next"
       href="5_wavenet.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Wavenet</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">Forward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-pass">Backward pass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-backward-pass">Pytorch backward pass</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-backward-pass">Manual backward pass</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy-loss">Cross entropy loss</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-2">Layer 2</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-1">Layer 1</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linearity">Non-linearity</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-norm">Batch norm</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-batch-norm">Pre-batch norm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-layer">Embedding layer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-through-cross-entropy-loss">Backpropagation through cross entropy loss</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Backward pass</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-of-cross-entropy-loss">Intuition of cross-entropy loss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-through-batchnorm">Backpropagation through batchnorm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Backward pass</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-mlp-model-with-manual-backpropagation">Train the MLP model with manual backpropagation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Marc Dumon
</p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>