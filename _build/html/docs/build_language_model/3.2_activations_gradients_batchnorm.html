

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Activations, Gradients and Batch Normalisation - Part 2 &#8212; Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MSZJYDCNQ4"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MSZJYDCNQ4');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/build_language_model/3.2_activations_gradients_batchnorm';</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Backpropagation" href="4_backprop.html" />
    <link rel="prev" title="Activations, Gradients and Batch Normalisation - Part 1" href="3.1_activations_gradients_batchnorm.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">BUILD A LANGUAGE MODEL</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_intro.html">Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_bigrams.html">Bigram Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_mlp.html">Multilayer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="3.1_activations_gradients_batchnorm.html">Activations, Gradients and Batch Normalisation - Part 1</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Activations, Gradients and Batch Normalisation - Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_backprop.html">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_wavenet.html">Wavenet</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">MY FILES</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../my_files/jupyter_book_notes.html">Jupyter Book Notes</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/marcdumon/marcdumon.github.io/blob/master/docs/docs/build_language_model/3.2_activations_gradients_batchnorm.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/marcdumon/marcdumon.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/marcdumon/marcdumon.github.io/issues/new?title=Issue%20on%20page%20%2Fdocs/build_language_model/3.2_activations_gradients_batchnorm.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/docs/build_language_model/3.2_activations_gradients_batchnorm.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Activations, Gradients and Batch Normalisation - Part 2</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modules">Modules</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear">Linear</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batchnorm1d">BatchNorm1d</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh">Tanh</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp">MLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostic-tools">Diagnostic tools</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-distribution">Activation distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-gradients">Activation gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-gradients">Weight gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#update-weights-ratio">Update weights ratio</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#initialisation-gain">Initialisation gain</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correct-initialisation-gain-5-3">Correct initialisation gain (5/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#too-low-initialisation-gain-1">Too low initialisation gain (1)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#too-high-initialisation-gain-3">Too high initialisation gain (3)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalisation">Batch normalisation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Correct initialisation gain (5/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Too low initialisation gain (1)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Too high initialisation gain (3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions">Conclusions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight">Weight</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Type</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.figure</span> <span class="kn">import</span> <span class="n">Figure</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>

<span class="n">torch</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<hr>
<section class="tex2jax_ignore mathjax_ignore" id="activations-gradients-and-batch-normalisation-part-2">
<h1>Activations, Gradients and Batch Normalisation - Part 2<a class="headerlink" href="#activations-gradients-and-batch-normalisation-part-2" title="Permalink to this heading">#</a></h1>
<hr><p>This notebook will focus on organizing our code into modules, allowing us to build neural networks in a similar way to pytorch while maintaining a consistent API. Our approach involves creating a Linear layer module and a BatchNorm1d module, which we will use to reconstruct our neural net. Once our network is built, we will train it in the same manner as before.</p>
<p>Next, we will examine activation statistics during both the forward and backward passes. Finally, we will proceed with evaluation and sampling, following the same approach as in previous iterations.</p>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this heading">#</a></h2>
<p><strong>Load the data</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./data/names.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">names</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Total # names: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">)</span><span class="si">:</span><span class="s1">.&gt;25,</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total # characters in all names: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">names</span><span class="p">))</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">names</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total # names: ...................32,033
Total # characters in all names: 196,113
[&#39;emma&#39;, &#39;olivia&#39;, &#39;ava&#39;, &#39;isabella&#39;, &#39;sophia&#39;]
</pre></div>
</div>
</div>
</div>
<p><strong>Encoding and decoding characters</strong><br />
We define an encoder <code class="docutils literal notranslate"><span class="pre">ch2ix</span></code> and decoder <code class="docutils literal notranslate"><span class="pre">ix2ch</span></code> function that maps a character with a numerical representation (i.e. a unique integer) and vice-versa.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chars</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span> <span class="o">+</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_lowercase</span>
<span class="n">ch2ix</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="n">ix2ch</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ch2ix</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ix2ch</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Encoder ch2ix:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ch2ix</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Decoder ix2ch:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ix2ch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;E.g. emma:&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">ch2ix</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="s1">&#39;.emma.&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encoder ch2ix:
 {&#39;.&#39;: 0, &#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 4, &#39;e&#39;: 5, &#39;f&#39;: 6, &#39;g&#39;: 7, &#39;h&#39;: 8, &#39;i&#39;: 9, &#39;j&#39;: 10, &#39;k&#39;: 11, &#39;l&#39;: 12, &#39;m&#39;: 13, &#39;n&#39;: 14, &#39;o&#39;: 15, &#39;p&#39;: 16, &#39;q&#39;: 17, &#39;r&#39;: 18, &#39;s&#39;: 19, &#39;t&#39;: 20, &#39;u&#39;: 21, &#39;v&#39;: 22, &#39;w&#39;: 23, &#39;x&#39;: 24, &#39;y&#39;: 25, &#39;z&#39;: 26}
Decoder ix2ch:
 {0: &#39;.&#39;, 1: &#39;a&#39;, 2: &#39;b&#39;, 3: &#39;c&#39;, 4: &#39;d&#39;, 5: &#39;e&#39;, 6: &#39;f&#39;, 7: &#39;g&#39;, 8: &#39;h&#39;, 9: &#39;i&#39;, 10: &#39;j&#39;, 11: &#39;k&#39;, 12: &#39;l&#39;, 13: &#39;m&#39;, 14: &#39;n&#39;, 15: &#39;o&#39;, 16: &#39;p&#39;, 17: &#39;q&#39;, 18: &#39;r&#39;, 19: &#39;s&#39;, 20: &#39;t&#39;, 21: &#39;u&#39;, 22: &#39;v&#39;, 23: &#39;w&#39;, 24: &#39;x&#39;, 25: &#39;y&#39;, 26: &#39;z&#39;}
E.g. emma: [0, 5, 13, 13, 1, 0]
</pre></div>
</div>
</div>
</div>
<p><strong>Build the datasets</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
        <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">w</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">:</span>
            <span class="n">ix</span> <span class="o">=</span> <span class="n">ch2ix</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span>
            <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
            <span class="n">Y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>  <span class="c1"># crop and append</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Shape (X, Y): </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="n">names</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="n">n1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
<span class="n">n2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
<span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span> <span class="o">=</span> <span class="n">make_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="n">n1</span><span class="p">])</span>
<span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span> <span class="o">=</span> <span class="n">make_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">])</span>
<span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span> <span class="o">=</span> <span class="n">make_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">n2</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape (X, Y): torch.Size([182625, 3]), torch.Size([182625])
Shape (X, Y): torch.Size([22655, 3]), torch.Size([22655])
Shape (X, Y): torch.Size([22866, 3]), torch.Size([22866])
</pre></div>
</div>
</div>
</div>
</section>
<section id="modules">
<h2>Modules<a class="headerlink" href="#modules" title="Permalink to this heading">#</a></h2>
<p>We will generate modules for Linear, BatchNorm1d, and Tanh that we can subsequently combine to form layers in our neural network.</p>
<section id="linear">
<h3>Linear<a class="headerlink" href="#linear" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"><code class="docutils literal notranslate"><span class="pre">torch.nn.Linear(in_features,</span> <span class="pre">out_features,</span> <span class="pre">bias=True,</span> <span class="pre">device=None,</span> <span class="pre">dtype=None)</span></code></a></p>
<p>Applies a linear transformation to the incoming data: <span class="math notranslate nohighlight">\(y=xA^T+b\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Linear</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> <span class="k">if</span> <span class="n">seed</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">out_features</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># To be able to access x for plotting</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">]</span> <span class="o">+</span> <span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="k">else</span> <span class="p">[])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="batchnorm1d">
<h3>BatchNorm1d<a class="headerlink" href="#batchnorm1d" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d"><code class="docutils literal notranslate"><span class="pre">torch.nn.BatchNorm1d(num_features,</span> <span class="pre">eps=1e-05,</span> <span class="pre">momentum=0.1,</span> <span class="pre">affine=True,</span> <span class="pre">track_running_stats=True,</span> <span class="pre">device=None,</span> <span class="pre">dtype=None)</span></code></a></p>
<p>Applies Batch Normalization over a 2D or 3D input as described in the paper Batch Normalization: <a class="reference external" href="https://arxiv.org/abs/1502.03167">Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>.</p>
<p><span class="math notranslate nohighlight">\(y=\Large{\frac{x-\mathrm{E}[x]}{\sqrt{\mathrm{Var}[x]+\epsilon}}}\normalsize{*\gamma+\beta}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BatchNorm1d</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># Trainable parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="c1"># Running stats trained with &#39;momentum update&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>  <span class="c1"># To be able to access x for plotting</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">x_mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">x_std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span>
            <span class="n">x_std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_std</span>

        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">x_std</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
        <span class="c1"># Update running stats</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">x_mean</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">running_std</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">x_std</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># To be able to access x for plotting</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="tanh">
<h3>Tanh<a class="headerlink" href="#tanh" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Tanh</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># To be able to access x for plotting</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="mlp">
<h2>MLP<a class="headerlink" href="#mlp" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">27</span><span class="p">,</span> <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> <span class="k">if</span> <span class="n">seed</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_dim</span> <span class="o">=</span> <span class="n">emb_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hid_dim</span> <span class="o">=</span> <span class="n">hid_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>  <span class="c1"># Embedding</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_parameters</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">[</span><span class="n">xs</span><span class="p">]</span>  <span class="c1"># (bs, 3, 10) Embed characters into vectors</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_dim</span><span class="p">)</span>  <span class="c1"># (bs, 30) Concatenate the vectors</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_hat</span>

    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets the gradients to None&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">optimise_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Updat the parameters&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n_emb</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">use_bn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">seed</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="c1"># 1st linear layer</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_emb</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">use_bn</span><span class="p">:</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)]</span>
    <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">Tanh</span><span class="p">()]</span>
    <span class="c1"># Hidden layers</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)]</span>
        <span class="k">if</span> <span class="n">use_bn</span><span class="p">:</span>
            <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)]</span>
        <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">Tanh</span><span class="p">()]</span>
    <span class="c1"># Output layer</span>
    <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">layers</span>


<span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">gain</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># 5/3 -&gt; Kaiming initialisation for Tanh activation</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="c1"># Make last layer less confident</span>
        <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*=</span> <span class="mf">0.1</span>  <span class="c1"># /sqrt(100)</span>
        <span class="c1"># Apply gain for on other layers</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">Linear</span><span class="p">):</span>
                <span class="n">in_features</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">weight</span> <span class="o">*=</span> <span class="n">gain</span> <span class="o">/</span> <span class="n">in_features</span><span class="o">**</span><span class="mf">0.5</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">MLP</span><span class="p">,</span> <span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200000</span><span class="p">,</span> <span class="n">bs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">list</span><span class="p">]:</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> <span class="k">if</span> <span class="n">seed</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">uwr</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Update to weights ratio</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
        <span class="c1"># Minibatch construct</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtrn</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtrn</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytrn</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>

        <span class="c1"># Forward pass</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>

        <span class="c1"># Backward pass</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Update</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100000</span> <span class="k">else</span> <span class="n">lr</span> <span class="o">/</span> <span class="mi">10</span>  <span class="c1"># Step learning rate decay</span>
        <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

        <span class="c1"># Tack stats</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">7d</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">uwr</span><span class="o">.</span><span class="n">append</span><span class="p">([(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">())</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">losses</span><span class="p">,</span> <span class="n">uwr</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="diagnostic-tools">
<h2>Diagnostic tools<a class="headerlink" href="#diagnostic-tools" title="Permalink to this heading">#</a></h2>
<section id="activation-distribution">
<h3>Activation distribution<a class="headerlink" href="#activation-distribution" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plt_activations_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">layer_type</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">Linear</span> <span class="o">|</span> <span class="n">Tanh</span> <span class="o">|</span> <span class="n">BatchNorm1d</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">legends</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>  <span class="c1"># All layers except last softmax layer</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">layer_type</span><span class="p">):</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">out</span>
            <span class="n">sat</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.97</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>  <span class="c1"># % of saturated neurons</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;layer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1"> | </span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">:</span><span class="s1">&lt;12</span><span class="si">}</span><span class="s1"> | mean: </span><span class="si">{</span><span class="n">t</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s1">+.2f</span><span class="si">}</span><span class="s1"> | std: </span><span class="si">{</span><span class="n">t</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> | saturated: </span><span class="si">{</span><span class="n">sat</span><span class="si">:</span><span class="s1">5.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
            <span class="n">hy</span><span class="p">,</span> <span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hx</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">hy</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            <span class="n">legends</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;layer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">legends</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">layer_type</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1"> Activation Distribution&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="activation-gradients">
<h3>Activation gradients<a class="headerlink" href="#activation-gradients" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plt_gradients_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">layer_type</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">Linear</span> <span class="o">|</span> <span class="n">Tanh</span> <span class="o">|</span> <span class="n">BatchNorm1d</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">legends</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>  <span class="c1"># All layers except last softmax layer</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">layer_type</span><span class="p">):</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">grad</span>
            <span class="k">if</span> <span class="n">t</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;layer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1"> | </span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">:</span><span class="s1">&lt;12</span><span class="si">}</span><span class="s1"> | mean: </span><span class="si">{</span><span class="n">t</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s1">+.2f</span><span class="si">}</span><span class="s1"> | std: </span><span class="si">{</span><span class="n">t</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s1">.4e</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
                <span class="n">hy</span><span class="p">,</span> <span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hx</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">hy</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
                <span class="n">legends</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;layer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">legends</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">layer_type</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1"> Gradient Distribution&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="weight-gradients">
<h3>Weight gradients<a class="headerlink" href="#weight-gradients" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plt_weights_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">MLP</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">legends</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">t</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">&gt;5</span><span class="si">}</span><span class="s1"> | weight: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="si">:</span><span class="s1">&gt;10</span><span class="si">}</span><span class="s1"> | mean: </span><span class="si">{</span><span class="n">t</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s1">+.2f</span><span class="si">}</span><span class="s1"> | std: </span><span class="si">{</span><span class="n">t</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s1">.4e</span><span class="si">}</span><span class="s1"> | grad:data ratio: </span><span class="si">{</span><span class="n">t</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">/</span><span class="n">p</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s1">.4e</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">hy</span><span class="p">,</span> <span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hx</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">hy</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            <span class="n">legends</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="nb">tuple</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">legends</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Weights Gradient Distribution&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="update-weights-ratio">
<h3>Update weights ratio<a class="headerlink" href="#update-weights-ratio" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plt_update_weights_ratio</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">MLP</span><span class="p">,</span> <span class="n">uwr</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">legends</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># Only the weights</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">uwr</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">uwr</span><span class="p">))])</span>
            <span class="n">legends</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Param </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">uwr</span><span class="p">)],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>  <span class="c1"># ~1e-3 is roughly the ideal ratio =&gt; plot -3 (log10)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">legends</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Update to Data Ratio&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="initialisation-gain">
<h2>Initialisation gain<a class="headerlink" href="#initialisation-gain" title="Permalink to this heading">#</a></h2>
<p>To evaluate the impact of initialization gain on a neural network’s performance, we will train the network using various gain values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ---------------------------------------------------------------------</span>
<span class="c1"># Model Parameters</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_emb</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># The dim of the embedding vector</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># The nr of neurons in the hidden layer</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">6</span>
<span class="c1"># ---------------------------------------------------------------------</span>
<span class="c1"># Training parameters</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
</pre></div>
</div>
</div>
</div>
<section id="correct-initialisation-gain-5-3">
<h3>Correct initialisation gain (5/3)<a class="headerlink" href="#correct-initialisation-gain-5-3" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">use_bn</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">gain</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">layers</span> <span class="o">=</span> <span class="n">create_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_emb</span><span class="o">=</span><span class="n">n_emb</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">use_bn</span><span class="o">=</span><span class="n">use_bn</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="n">init_weights</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="n">n_emb</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Nr. parameters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">losses</span><span class="p">,</span> <span class="n">uwr</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nr. parameters: 46497
      0/1: 3.3254
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_activations_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">)</span>
<span class="n">plt_gradients_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 1 | Tanh         | mean: +0.03 | std: 0.75 | saturated: 20.56%
layer 3 | Tanh         | mean: -0.01 | std: 0.68 | saturated:  7.88%
layer 5 | Tanh         | mean: +0.02 | std: 0.65 | saturated:  6.59%
layer 7 | Tanh         | mean: +0.04 | std: 0.64 | saturated:  4.53%
layer 9 | Tanh         | mean: +0.00 | std: 0.63 | saturated:  3.97%
</pre></div>
</div>
<img alt="../../_images/e6c1fc9857333a65ed4cf5eb23fcebd21c4486e397a7a87f1dafc2a0d0b89a8d.png" src="../../_images/e6c1fc9857333a65ed4cf5eb23fcebd21c4486e397a7a87f1dafc2a0d0b89a8d.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 1 | Tanh         | mean: -0.00 | std: 4.6075e-03
layer 3 | Tanh         | mean: -0.00 | std: 4.2899e-03
layer 5 | Tanh         | mean: -0.00 | std: 3.7575e-03
layer 7 | Tanh         | mean: +0.00 | std: 3.4078e-03
layer 9 | Tanh         | mean: +0.00 | std: 3.0632e-03
</pre></div>
</div>
<img alt="../../_images/f1410a78f0e38022e37dec44e9a25b152c39a6dd87b42061829576331e28f7ce.png" src="../../_images/f1410a78f0e38022e37dec44e9a25b152c39a6dd87b42061829576331e28f7ce.png" />
</div>
</div>
</section>
<section id="too-low-initialisation-gain-1">
<h3>Too low initialisation gain (1)<a class="headerlink" href="#too-low-initialisation-gain-1" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">use_bn</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">gain</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 5 / 3</span>
<span class="n">layers</span> <span class="o">=</span> <span class="n">create_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_emb</span><span class="o">=</span><span class="n">n_emb</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">use_bn</span><span class="o">=</span><span class="n">use_bn</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="n">init_weights</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="n">n_emb</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Nr. parameters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">losses</span><span class="p">,</span> <span class="n">uwr</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nr. parameters: 46497
      0/1: 3.1976
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_activations_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">)</span>
<span class="n">plt_gradients_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 1 | Tanh         | mean: +0.03 | std: 0.62 | saturated:  4.00%
layer 3 | Tanh         | mean: -0.01 | std: 0.47 | saturated:  0.00%
layer 5 | Tanh         | mean: +0.01 | std: 0.39 | saturated:  0.00%
layer 7 | Tanh         | mean: +0.02 | std: 0.33 | saturated:  0.00%
layer 9 | Tanh         | mean: +0.00 | std: 0.29 | saturated:  0.00%
</pre></div>
</div>
<img alt="../../_images/a1a97a8eff21c8e9cce0631ca2a9e3432a0edc47d1ebd5c336a7079d8a16e7f0.png" src="../../_images/a1a97a8eff21c8e9cce0631ca2a9e3432a0edc47d1ebd5c336a7079d8a16e7f0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 1 | Tanh         | mean: -0.00 | std: 1.7376e-03
layer 3 | Tanh         | mean: -0.00 | std: 2.1532e-03
layer 5 | Tanh         | mean: -0.00 | std: 2.4328e-03
layer 7 | Tanh         | mean: +0.00 | std: 2.8186e-03
layer 9 | Tanh         | mean: +0.00 | std: 3.0382e-03
</pre></div>
</div>
<img alt="../../_images/973c384e7e81b6cb46324336488ee8893df968ef8cb68208f6d8f82311d51fac.png" src="../../_images/973c384e7e81b6cb46324336488ee8893df968ef8cb68208f6d8f82311d51fac.png" />
</div>
</div>
</section>
<section id="too-high-initialisation-gain-3">
<h3>Too high initialisation gain (3)<a class="headerlink" href="#too-high-initialisation-gain-3" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">use_bn</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">gain</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># 5 / 3</span>

<span class="n">layers</span> <span class="o">=</span> <span class="n">create_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_emb</span><span class="o">=</span><span class="n">n_emb</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">use_bn</span><span class="o">=</span><span class="n">use_bn</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="n">init_weights</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="n">n_emb</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Nr. parameters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">losses</span><span class="p">,</span> <span class="n">uwr</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nr. parameters: 46497
      0/1: 3.5164
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_activations_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">)</span>
<span class="n">plt_gradients_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 1 | Tanh         | mean: +0.04 | std: 0.86 | saturated: 47.03%
layer 3 | Tanh         | mean: -0.01 | std: 0.84 | saturated: 40.16%
layer 5 | Tanh         | mean: +0.02 | std: 0.83 | saturated: 39.16%
layer 7 | Tanh         | mean: +0.04 | std: 0.83 | saturated: 38.16%
layer 9 | Tanh         | mean: +0.01 | std: 0.83 | saturated: 38.44%
</pre></div>
</div>
<img alt="../../_images/90631cc3378636ddcb09972bf763cdcd3e3dbcc84a5005989a89a344a18bd7a6.png" src="../../_images/90631cc3378636ddcb09972bf763cdcd3e3dbcc84a5005989a89a344a18bd7a6.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 1 | Tanh         | mean: +0.00 | std: 1.1001e-02
layer 3 | Tanh         | mean: -0.00 | std: 7.9926e-03
layer 5 | Tanh         | mean: +0.00 | std: 5.6239e-03
layer 7 | Tanh         | mean: -0.00 | std: 4.2310e-03
layer 9 | Tanh         | mean: +0.00 | std: 3.1050e-03
</pre></div>
</div>
<img alt="../../_images/a70c0ad774528453d24f8ff495354e7a0d0e814af14d153c5241a9a4aa8123e1.png" src="../../_images/a70c0ad774528453d24f8ff495354e7a0d0e814af14d153c5241a9a4aa8123e1.png" />
</div>
</div>
<section id="conclusion">
<h4>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h4>
<p><strong>If we set the gain to the correct value of 5/3:</strong><br />
Activations:</p>
<ul class="simple">
<li><p>Layer 1 is fairly saturated at ~21%. This is visible in the plot at the values around -1 and 1.</p></li>
<li><p>Other layers are not very saturated (~5%). The reason is because we set the gain to 5/3.</p></li>
</ul>
<p>Gradients:</p>
<ul class="simple">
<li><p>The gradients of each layer are roughly the same.</p></li>
</ul>
<p><strong>If we set the gain too low (1):</strong><br />
Activations:</p>
<ul class="simple">
<li><p>Layer 1 is decent.</p></li>
<li><p>As more layers are traversed, the activation distributions are increasingly squeezed towards 0. Increasing the gain will help preventing this squeezing.</p></li>
</ul>
<p>Gradients:</p>
<ul class="simple">
<li><p>As more layers are traversed, the variance of the gradient distributions increases.</p></li>
</ul>
<p><strong>If we set the gain too high (3):</strong><br />
Activations:</p>
<ul class="simple">
<li><p>The saturations of all layers are way too high.</p></li>
</ul>
<p>Gradients:</p>
<ul class="simple">
<li><p>The variance of the gradient distributions decrease as more layers are traversed.</p></li>
</ul>
</section>
</section>
</section>
<section id="batch-normalisation">
<h2>Batch normalisation<a class="headerlink" href="#batch-normalisation" title="Permalink to this heading">#</a></h2>
<p>To evaluate the impact of initialization gain on a neural network’s performance, we will train the network using various gain values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ---------------------------------------------------------------------</span>
<span class="c1"># Model Parameters</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_emb</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># The dim of the embedding vector</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># The nr of neurons in the hidden layer</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">6</span>
<span class="c1"># ---------------------------------------------------------------------</span>
<span class="c1"># Training parameters</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
</pre></div>
</div>
</div>
</div>
<section id="id1">
<h3>Correct initialisation gain (5/3)<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">use_bn</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">gain</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">/</span> <span class="mi">3</span>

<span class="n">layers</span> <span class="o">=</span> <span class="n">create_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_emb</span><span class="o">=</span><span class="n">n_emb</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">use_bn</span><span class="o">=</span><span class="n">use_bn</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="n">init_weights</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="n">n_emb</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Nr. parameters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">losses</span><span class="p">,</span> <span class="n">uwr</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nr. parameters: 47497
      0/1000: 3.3349
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_activations_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">)</span>
<span class="n">plt_gradients_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 2 | Tanh         | mean: +0.00 | std: 0.64 | saturated:  2.84%
layer 5 | Tanh         | mean: +0.01 | std: 0.64 | saturated:  2.22%
layer 8 | Tanh         | mean: +0.01 | std: 0.65 | saturated:  2.19%
layer 11 | Tanh         | mean: +0.01 | std: 0.65 | saturated:  1.97%
layer 14 | Tanh         | mean: +0.01 | std: 0.65 | saturated:  1.66%
</pre></div>
</div>
<img alt="../../_images/7bb8673990950c2d380485708de61e0060c93898e294d6d4c02ddc2c579859e6.png" src="../../_images/7bb8673990950c2d380485708de61e0060c93898e294d6d4c02ddc2c579859e6.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 2 | Tanh         | mean: +0.00 | std: 4.2741e-03
layer 5 | Tanh         | mean: -0.00 | std: 3.6387e-03
layer 8 | Tanh         | mean: +0.00 | std: 3.3256e-03
layer 11 | Tanh         | mean: +0.00 | std: 3.1141e-03
layer 14 | Tanh         | mean: +0.00 | std: 3.2100e-03
</pre></div>
</div>
<img alt="../../_images/8a12917019fb7381e496f388b4ac851695ef3d7fa89da55ca660f0d9a8ec2487.png" src="../../_images/8a12917019fb7381e496f388b4ac851695ef3d7fa89da55ca660f0d9a8ec2487.png" />
</div>
</div>
</section>
<section id="id2">
<h3>Too low initialisation gain (1)<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">use_bn</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">gain</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 5 / 3</span>
<span class="n">layers</span> <span class="o">=</span> <span class="n">create_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_emb</span><span class="o">=</span><span class="n">n_emb</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">use_bn</span><span class="o">=</span><span class="n">use_bn</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="n">init_weights</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="n">n_emb</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Nr. parameters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">losses</span><span class="p">,</span> <span class="n">uwr</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nr. parameters: 47497
      0/1000: 3.3349
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_activations_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">)</span>
<span class="n">plt_gradients_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 2 | Tanh         | mean: +0.00 | std: 0.63 | saturated:  2.91%
layer 5 | Tanh         | mean: +0.00 | std: 0.64 | saturated:  2.66%
layer 8 | Tanh         | mean: +0.01 | std: 0.65 | saturated:  2.09%
layer 11 | Tanh         | mean: -0.00 | std: 0.65 | saturated:  2.09%
layer 14 | Tanh         | mean: +0.00 | std: 0.66 | saturated:  1.16%
</pre></div>
</div>
<img alt="../../_images/73c272c2bfdabe4ac09f927b2b5333f4abf63044de4bbca8f8df5174c7c7a155.png" src="../../_images/73c272c2bfdabe4ac09f927b2b5333f4abf63044de4bbca8f8df5174c7c7a155.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 2 | Tanh         | mean: +0.00 | std: 3.3701e-03
layer 5 | Tanh         | mean: +0.00 | std: 2.8153e-03
layer 8 | Tanh         | mean: +0.00 | std: 2.6370e-03
layer 11 | Tanh         | mean: -0.00 | std: 2.4682e-03
layer 14 | Tanh         | mean: +0.00 | std: 3.0869e-03
</pre></div>
</div>
<img alt="../../_images/5376f327be346b607097188317a81e6f39ed539d11176f190621d6065d7a276c.png" src="../../_images/5376f327be346b607097188317a81e6f39ed539d11176f190621d6065d7a276c.png" />
</div>
</div>
</section>
<section id="id3">
<h3>Too high initialisation gain (3)<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">use_bn</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">gain</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># 5 / 3</span>

<span class="n">layers</span> <span class="o">=</span> <span class="n">create_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_emb</span><span class="o">=</span><span class="n">n_emb</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">use_bn</span><span class="o">=</span><span class="n">use_bn</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="n">init_weights</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="n">n_emb</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Nr. parameters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">losses</span><span class="p">,</span> <span class="n">uwr</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nr. parameters: 47497
      0/1000: 3.3349
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_activations_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">)</span>
<span class="n">plt_gradients_hist</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">Tanh</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 2 | Tanh         | mean: +0.00 | std: 0.63 | saturated:  2.72%
layer 5 | Tanh         | mean: +0.01 | std: 0.64 | saturated:  2.69%
layer 8 | Tanh         | mean: +0.00 | std: 0.64 | saturated:  2.44%
layer 11 | Tanh         | mean: +0.00 | std: 0.64 | saturated:  1.84%
layer 14 | Tanh         | mean: +0.00 | std: 0.64 | saturated:  2.34%
</pre></div>
</div>
<img alt="../../_images/3f8c6e1c19469ac93895647eab346be9d2a638afaeac45a7721927519ede9336.png" src="../../_images/3f8c6e1c19469ac93895647eab346be9d2a638afaeac45a7721927519ede9336.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 2 | Tanh         | mean: +0.00 | std: 5.0882e-03
layer 5 | Tanh         | mean: -0.00 | std: 4.3340e-03
layer 8 | Tanh         | mean: +0.00 | std: 3.8639e-03
layer 11 | Tanh         | mean: -0.00 | std: 3.5354e-03
layer 14 | Tanh         | mean: +0.00 | std: 3.4085e-03
</pre></div>
</div>
<img alt="../../_images/adde0da00f1b47f69720cfdcac76f1c6ebb700c576871948700cb90e28eb5e93.png" src="../../_images/adde0da00f1b47f69720cfdcac76f1c6ebb700c576871948700cb90e28eb5e93.png" />
</div>
</div>
</section>
<section id="conclusions">
<h3>Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this heading">#</a></h3>
<p>We can observe that adding batch normalization layers to a neural network results in well-behaved activations and gradients, even when the initialization gain is not optimal. The use of batch normalization makes the neural network less sensitive to initial conditions, which can simplify the process of finding a good initialization.</p>
</section>
<section id="weight">
<h3>Weight<a class="headerlink" href="#weight" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_weights_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">plt_update_weights_ratio</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">uwr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    0 | weight:   (27, 10) | mean: -0.00 | std: 1.3109e-02 | grad:data ratio: 1.3089e-02
    1 | weight:  (30, 100) | mean: -0.00 | std: 6.4590e-03 | grad:data ratio: 1.1654e-02
    5 | weight: (100, 100) | mean: +0.00 | std: 6.0543e-03 | grad:data ratio: 2.0028e-02
    9 | weight: (100, 100) | mean: +0.00 | std: 5.1148e-03 | grad:data ratio: 1.6930e-02
   13 | weight: (100, 100) | mean: +0.00 | std: 4.3957e-03 | grad:data ratio: 1.4554e-02
   17 | weight: (100, 100) | mean: +0.00 | std: 3.7799e-03 | grad:data ratio: 1.2517e-02
   21 | weight:  (100, 27) | mean: +0.00 | std: 2.1064e-02 | grad:data ratio: 1.9037e-01
</pre></div>
</div>
<img alt="../../_images/40e7746a8a1cfeda14b6390797344660f44e6d7b8a25e02403ab095a1ce21262.png" src="../../_images/40e7746a8a1cfeda14b6390797344660f44e6d7b8a25e02403ab095a1ce21262.png" />
<img alt="../../_images/18c7e4766d16240871e2c5e7b705ef9ba808fa8da5f057d693e2ddaa3e2bf844.png" src="../../_images/18c7e4766d16240871e2c5e7b705ef9ba808fa8da5f057d693e2ddaa3e2bf844.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/build_language_model"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="3.1_activations_gradients_batchnorm.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Activations, Gradients and Batch Normalisation - Part 1</p>
      </div>
    </a>
    <a class="right-next"
       href="4_backprop.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Backpropagation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modules">Modules</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear">Linear</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batchnorm1d">BatchNorm1d</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh">Tanh</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp">MLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostic-tools">Diagnostic tools</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-distribution">Activation distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-gradients">Activation gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-gradients">Weight gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#update-weights-ratio">Update weights ratio</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#initialisation-gain">Initialisation gain</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correct-initialisation-gain-5-3">Correct initialisation gain (5/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#too-low-initialisation-gain-1">Too low initialisation gain (1)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#too-high-initialisation-gain-3">Too high initialisation gain (3)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalisation">Batch normalisation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Correct initialisation gain (5/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Too low initialisation gain (1)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Too high initialisation gain (3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions">Conclusions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight">Weight</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Marc Dumon
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>