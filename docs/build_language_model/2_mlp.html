
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Multilayer Perceptron &#8212; Machine Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Activations, Gradients and Batch Normalisation" href="3_activations_gradients_batchnorm.html" />
    <link rel="prev" title="Bigram Language Model" href="1_bigrams.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  BUILD A LANGUAGE MODEL
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="0_intro.html">
   Intro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1_bigrams.html">
   Bigram Language Model
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Multilayer Perceptron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_activations_gradients_batchnorm.html">
   Activations, Gradients and Batch Normalisation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MY FILES
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../my_files/jupyter_book_notes.html">
   Jupyter Book Notes
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/marcdumon/marcdumon.github.io/blob/master/docs/docs/build_language_model/2_mlp.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/marcdumon/marcdumon.github.io"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/marcdumon/marcdumon.github.io/issues/new?title=Issue%20on%20page%20%2Fdocs/build_language_model/2_mlp.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/docs/build_language_model/2_mlp.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#character-embedding">
   Character embedding
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-mlp-step-by-step">
   Building MLP step by step
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loss-function">
   Loss function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mlp-model">
   MLP model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-1-batch">
     Training 1 batch
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-the-full-dataset">
     Training the full dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-with-mini-batches">
     Training with mini-batches
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-valid-test-split">
   Train | valid | test split
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tuning-hyperparameters">
   Tuning hyperparameters
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-rate">
     Learning rate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hidden-dim">
     Hidden dim
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#embedding-dim">
     Embedding dim
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visialize-embeddings">
   Visialize embeddings
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-names">
   Sampling names
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Multilayer Perceptron</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#character-embedding">
   Character embedding
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-mlp-step-by-step">
   Building MLP step by step
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loss-function">
   Loss function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mlp-model">
   MLP model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-1-batch">
     Training 1 batch
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-the-full-dataset">
     Training the full dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-with-mini-batches">
     Training with mini-batches
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-valid-test-split">
   Train | valid | test split
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tuning-hyperparameters">
   Tuning hyperparameters
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-rate">
     Learning rate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hidden-dim">
     Hidden dim
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#embedding-dim">
     Embedding dim
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visialize-embeddings">
   Visialize embeddings
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-names">
   Sampling names
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>

<span class="n">torch</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<hr>
<section class="tex2jax_ignore mathjax_ignore" id="multilayer-perceptron">
<h1>Multilayer Perceptron<a class="headerlink" href="#multilayer-perceptron" title="Permalink to this headline">#</a></h1>
<hr><p>In this notebook, we will develop a <a class="reference external" href="https://www.wikiwand.com/en/Multilayer_perceptron">multilayer perceptron</a> (MLP) model to predict the next character in a sequence. Our approach is based on the paper <a class="reference external" href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">“A Neural Probabilistic Language Model” by Bengio et al. (2003)</a>,which presents a word-level language model. However, we will work with characters instead of words and follow a similar modeling approach. The goal is to train an MLP to learn the patterns and structure of a character sequence and use this knowledge to accurately predict the next character in the sequence.</p>
<figure class="align-default" id="mlp">
<a class="reference internal image-reference" href="../../_images/2_mlp_2022-10-26-19-49-45.png"><img alt="../../_images/2_mlp_2022-10-26-19-49-45.png" src="../../_images/2_mlp_2022-10-26-19-49-45.png" style="width: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Multilayer Perceptron with embeddings (Bengio et al. 2003)</span><a class="headerlink" href="#mlp" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">#</a></h2>
<p><strong>Load the data</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./data/names.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">names</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Total # names: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">)</span><span class="si">:</span><span class="s1">.&gt;25,</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total # characters in all names: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">names</span><span class="p">))</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">names</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total # names: ...................32,033
Total # characters in all names: 196,113
[&#39;emma&#39;, &#39;olivia&#39;, &#39;ava&#39;, &#39;isabella&#39;, &#39;sophia&#39;]
</pre></div>
</div>
</div>
</div>
<p><strong>Encoding and decoding characters</strong><br />
We define an encoder <code class="docutils literal notranslate"><span class="pre">ch2ix</span></code> and decoder <code class="docutils literal notranslate"><span class="pre">ix2ch</span></code> function that maps a character with a numerical representation (i.e. a unique integer) and vice-versa.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chars</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span> <span class="o">+</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_lowercase</span>
<span class="n">ch2ix</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="n">ix2ch</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ch2ix</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Encoder ch2ix:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ch2ix</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Decoder ix2ch:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ix2ch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;E.g. emma:&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">ch2ix</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="s1">&#39;.emma.&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encoder ch2ix:
 {&#39;.&#39;: 0, &#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 4, &#39;e&#39;: 5, &#39;f&#39;: 6, &#39;g&#39;: 7, &#39;h&#39;: 8, &#39;i&#39;: 9, &#39;j&#39;: 10, &#39;k&#39;: 11, &#39;l&#39;: 12, &#39;m&#39;: 13, &#39;n&#39;: 14, &#39;o&#39;: 15, &#39;p&#39;: 16, &#39;q&#39;: 17, &#39;r&#39;: 18, &#39;s&#39;: 19, &#39;t&#39;: 20, &#39;u&#39;: 21, &#39;v&#39;: 22, &#39;w&#39;: 23, &#39;x&#39;: 24, &#39;y&#39;: 25, &#39;z&#39;: 26}
Decoder ix2ch:
 {0: &#39;.&#39;, 1: &#39;a&#39;, 2: &#39;b&#39;, 3: &#39;c&#39;, 4: &#39;d&#39;, 5: &#39;e&#39;, 6: &#39;f&#39;, 7: &#39;g&#39;, 8: &#39;h&#39;, 9: &#39;i&#39;, 10: &#39;j&#39;, 11: &#39;k&#39;, 12: &#39;l&#39;, 13: &#39;m&#39;, 14: &#39;n&#39;, 15: &#39;o&#39;, 16: &#39;p&#39;, 17: &#39;q&#39;, 18: &#39;r&#39;, 19: &#39;s&#39;, 20: &#39;t&#39;, 21: &#39;u&#39;, 22: &#39;v&#39;, 23: &#39;w&#39;, 24: &#39;x&#39;, 25: &#39;y&#39;, 26: &#39;z&#39;}
E.g. emma: [0, 5, 13, 13, 1, 0]
</pre></div>
</div>
</div>
</div>
<p><strong>Dataset</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Dataset</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attributes:</span>
<span class="sd">    names: list of names.</span>
<span class="sd">    block_size: context length: how many characters do we take to predict the next one?</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">Y</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_make_dataset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_make_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">names</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>
            <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">w</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">:</span>
                <span class="n">ix</span> <span class="o">=</span> <span class="n">ch2ix</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span>
                <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
                <span class="n">Y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ix2ch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">context</span><span class="p">),</span> <span class="s1">&#39;---&gt;&#39;</span><span class="p">,</span> <span class="n">ix2ch</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
                <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>  <span class="c1"># crop and append</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">names</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">ds</span><span class="o">.</span><span class="n">Y</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>emma
... ---&gt; e
..e ---&gt; m
.em ---&gt; m
emm ---&gt; a
mma ---&gt; .
</pre></div>
</div>
</div>
</div>
</section>
<section id="character-embedding">
<h2>Character embedding<a class="headerlink" href="#character-embedding" title="Permalink to this headline">#</a></h2>
<p>To train the MLP to predict the next character in a sequence, we will represent each input character as a learnable m-dimensional vector. These vectors will be initialized randomly and will be adjusted during the training process. Characters that are similar to each other, such as vowels, will tend to be located closer to each other in this vector space.</p>
<p>To embed our 27 characters into an m-dimensional space, we can create a 27xm lookup table (C) and randomly initialize it. To obtain the embedding vector for the i-th character, we can either encode the character as a one-hot vector and multiply it by C, or simply use the index to retrieve the i-th row of C. This will give us the m-dimensional vector that represents the i-th character, which can be used as input to the MLP model.</p>
<p><strong>one-hot vs index</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="c1"># One-hot method</span>
<span class="n">onehot</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">onehot</span> <span class="o">@</span> <span class="n">C</span><span class="p">)</span>
<span class="c1"># Index method</span>
<span class="nb">print</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-2.1791, -1.0158])
tensor([-2.1791, -1.0158])
</pre></div>
</div>
</div>
</div>
<p>Using the index is much faster than using a one-hot vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> onehot = F.one_hot(torch.tensor(i), num_classes=27).float()
<span class="o">%</span><span class="k">timeit</span> index = C[i]
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12.6 µs ± 275 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)
930 ns ± 15.5 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
</pre></div>
</div>
</div>
</div>
<p>We can also index by a multi-dimentional tensor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 3])
tensor([[[-2.1791, -1.0158],
         [-2.1791, -1.0158],
         [-0.5073, -1.2585]],

        [[-0.1092, -0.6073],
         [ 0.1240,  0.9430],
         [-1.2564,  1.0715]]])
torch.Size([2, 3, 2])
</pre></div>
</div>
</div>
</div>
<p>So to embed simultaniously all the integers of X, we simply do <code class="docutils literal notranslate"><span class="pre">C[X]</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">X</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 3])
torch.Size([5, 3, 2])
</pre></div>
</div>
</div>
</div>
<p><strong>Plot untrained embedding</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">C</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">C</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightblue&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">27</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">chars</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Untrained embeddings&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2_mlp_22_0.png" src="../../_images/2_mlp_22_0.png" />
</div>
</div>
</section>
<section id="building-mlp-step-by-step">
<h2>Building MLP step by step<a class="headerlink" href="#building-mlp-step-by-step" title="Permalink to this headline">#</a></h2>
<p>We will build a MLP model has three components:</p>
<ol class="simple">
<li><p>An <strong>embedding matrix</strong> <code class="docutils literal notranslate"><span class="pre">C</span></code> that maps each input character to an m-dimensional embedding vector</p></li>
<li><p><strong>A hidden layer</strong> that applies the tanh non-linearity to the output of the concatenated embedding vectors</p></li>
<li><p>An <strong>output layer</strong> that produces logits from the output of the hidden layer and applies the softmax function to generate probabilities</p></li>
</ol>
<p>The hidden layer has:</p>
<ul class="simple">
<li><p>Weights <code class="docutils literal notranslate"><span class="pre">W1</span></code> with shape (<code class="docutils literal notranslate"><span class="pre">block_size</span></code> * <code class="docutils literal notranslate"><span class="pre">emb_dim</span></code>, <code class="docutils literal notranslate"><span class="pre">hid_dim</span></code>)</p></li>
<li><p>Bias b1 with shape (<code class="docutils literal notranslate"><span class="pre">hid_dim</span></code>)</p></li>
</ul>
<p>The output layer has:</p>
<ul class="simple">
<li><p>Weights <code class="docutils literal notranslate"><span class="pre">W2</span></code> with shape (<code class="docutils literal notranslate"><span class="pre">hid_dim</span></code>, 27)</p></li>
<li><p>Bias <code class="docutils literal notranslate"><span class="pre">b2</span></code> with shape (27)</p></li>
</ul>
<p><strong>Hyperparameters</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">block_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 3 (# input characters)</span>
<span class="n">emb_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hid_dim</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Embedding</strong></p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">))</span>  <span class="c1"># (27, emb_dim)</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">X</span><span class="p">]</span>  <span class="c1"># (bs, block_size, emb_dim)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 3, 2])
tensor([[[-0.7129,  0.0984],
         [-0.7129,  0.0984],
         [-0.7129,  0.0984]],

        [[-0.7129,  0.0984],
         [-0.7129,  0.0984],
         [ 0.0692,  0.9171]],

        [[-0.7129,  0.0984],
         [ 0.0692,  0.9171],
         [ 0.9794, -1.0215]],

        [[ 0.0692,  0.9171],
         [ 0.9794, -1.0215],
         [ 0.9794, -1.0215]],

        [[ 0.9794, -1.0215],
         [ 0.9794, -1.0215],
         [ 1.0069,  0.5345]]])
</pre></div>
</div>
</div>
</div>
<p>To perform the matrix multiplication between the embedding tensor  (<code class="docutils literal notranslate"><span class="pre">bs</span></code>, <code class="docutils literal notranslate"><span class="pre">block_size</span></code>, <code class="docutils literal notranslate"><span class="pre">emb_dim</span></code>) and the weights <code class="docutils literal notranslate"><span class="pre">W1</span></code> (<code class="docutils literal notranslate"><span class="pre">block_size</span></code> * <code class="docutils literal notranslate"><span class="pre">emb_dim</span></code>, <code class="docutils literal notranslate"><span class="pre">hid_dim</span></code>) we must concatenate the blocks of the embedding tensor. This can be achieved using the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cat.html">torch.cat()</a> or with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html">torch.Tensor.view()</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.view()</span></code> is much more efficient, because unlike <code class="docutils literal notranslate"><span class="pre">torch.cat()</span></code>, no memory is being changed, created, copied or moved  by the view() operation. The storage is unchanged.
A good blogpost on how pytorch internaly works to stores tensors:<a class="reference external" href="http://blog.ezyang.com/2019/05/pytorch-internals/">PyTorch internals</a>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> emb.view(-1, block_size*emb_dim)
<span class="o">%</span><span class="k">timeit</span> torch.cat(torch.unbind(emb, dim=1), dim=1)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.44 µs ± 29.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
6.65 µs ± 145 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)
</pre></div>
</div>
</div>
</div>
<p>Concatenate embeddings with view</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emb_concat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">emb_dim</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">emb_concat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">emb_concat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 6])
tensor([[-0.7129,  0.0984, -0.7129,  0.0984, -0.7129,  0.0984],
        [-0.7129,  0.0984, -0.7129,  0.0984,  0.0692,  0.9171],
        [-0.7129,  0.0984,  0.0692,  0.9171,  0.9794, -1.0215],
        [ 0.0692,  0.9171,  0.9794, -1.0215,  0.9794, -1.0215],
        [ 0.9794, -1.0215,  0.9794, -1.0215,  1.0069,  0.5345]])
</pre></div>
</div>
</div>
</div>
<p><strong>Hidden layer</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">block_size</span> <span class="o">*</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can now calculate the hidden states h.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The shape of <code class="docutils literal notranslate"><span class="pre">b1</span></code> is (hid_dim). We will add it to a tensor with shape (<code class="docutils literal notranslate"><span class="pre">block_size</span></code> * <code class="docutils literal notranslate"><span class="pre">emb_dim</span></code>, <code class="docutils literal notranslate"><span class="pre">hid_dim</span></code>). In order to do the addition <code class="docutils literal notranslate"><span class="pre">b1</span></code> will be broadcasted into a tensor of the same shape as <code class="docutils literal notranslate"><span class="pre">W1</span></code>. In other words, the bias will be added to all the rows of the tensor <code class="docutils literal notranslate"><span class="pre">emb</span></code> x<code class="docutils literal notranslate"> <span class="pre">W1</span></code>.</p>
</div>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="n">emb_concat</span> <span class="o">@</span> <span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 100])
tensor([[ 2.9250e-01, -1.7270e-01, -4.4131e-02,  3.1039e+00, -1.8438e+00, -7.3514e-01, -1.5009e-01, -8.0428e-01,
          4.3553e-01,  2.0493e+00,  1.2477e+00, -2.3337e-01, -3.6995e+00,  1.9541e-01,  3.9950e-01, -6.5415e-01,
         -1.0628e+00, -1.1908e+00, -6.7111e-01, -2.2623e+00,  2.8200e+00, -8.0388e-01,  2.9225e-01,  9.5519e-01,
          2.4211e+00,  6.1082e-01,  2.2438e+00, -2.4678e+00, -3.2454e+00, -5.8777e-02, -1.0052e+00, -1.4054e+00,
          1.3817e+00,  1.0508e+00,  1.8156e+00,  8.8641e-01, -3.1273e+00,  4.1666e-01, -4.3277e-01,  1.4938e+00,
         -7.4963e-01, -5.1770e-02, -1.4402e+00,  9.0927e-01,  3.0679e-01,  3.4361e+00,  9.5012e-01,  8.5520e-01,
         -5.2127e-01,  1.1680e+00,  9.1671e-01,  5.0057e-01,  2.1331e+00, -2.9728e-01, -4.1146e+00,  1.7657e+00,
         -1.1038e+00,  1.4275e+00,  1.1980e+00,  1.7752e+00, -7.0758e-01,  6.2899e-01, -8.9479e-01, -3.9028e+00,
          2.8801e+00,  1.7202e+00,  1.4665e+00, -1.5325e+00, -1.5144e-01,  8.9945e-01,  2.7294e+00, -2.1494e+00,
         -4.1232e+00, -1.5113e+00,  2.5436e-01,  1.7384e+00,  2.3128e+00,  1.7922e+00,  1.1174e+00, -5.4205e-01,
          2.0315e+00, -1.9150e+00,  2.5076e+00, -1.6775e+00,  9.6231e-01,  1.0703e-01,  1.4047e+00, -1.4666e+00,
         -1.2177e+00, -5.5958e-01, -1.0041e+00,  9.1008e-02, -2.1895e-01,  6.7582e-01,  1.4779e+00,  8.7873e-01,
          8.1940e-01,  1.2633e+00, -1.8571e+00, -2.8245e+00],
        [-9.0703e-01,  7.0154e-01,  1.6926e-01,  4.9397e+00, -2.4158e+00, -1.1020e+00, -1.3460e+00, -7.5420e-01,
         -1.0445e+00,  1.5800e+00,  1.3351e+00, -1.2030e-01, -6.1089e+00, -7.2760e-02,  2.4962e+00, -2.0417e+00,
         -1.1253e+00, -1.9978e+00,  5.1350e-01, -2.4888e+00,  1.0774e+00,  1.7363e-01,  4.6540e-01,  5.9306e-01,
          2.5841e+00,  1.1177e+00,  1.3488e+00, -1.0100e+00, -2.1893e+00, -1.0317e+00, -1.3662e+00, -7.1338e-01,
          1.2357e+00, -2.0682e+00,  1.6838e+00,  7.8776e-01, -2.4135e+00,  5.6140e-01, -1.2869e+00,  2.4437e+00,
         -6.7639e-01,  1.3546e+00, -1.3218e+00,  1.2447e+00,  1.7738e+00,  3.8265e+00,  2.2643e+00,  1.5271e+00,
          2.9792e+00,  2.3317e+00,  1.0531e-01,  6.4023e-01,  2.8214e+00, -6.0743e-01, -5.3896e+00,  2.0777e+00,
         -1.6329e+00,  7.1813e-01,  2.3725e+00,  2.8809e+00,  7.5530e-01,  1.2874e+00, -1.4813e+00, -5.4237e+00,
          3.4632e+00,  1.1969e+00, -9.9573e-01, -1.0648e+00,  1.0828e+00, -5.9321e-01,  2.5335e+00, -1.8245e+00,
         -2.5263e+00, -2.0419e+00, -8.2575e-01,  9.5994e-02, -9.6782e-01,  2.2801e+00,  2.9311e+00, -8.4152e-03,
          2.6650e+00, -3.2974e+00,  1.8644e+00, -1.8588e+00, -9.5813e-01,  6.9635e-01,  6.6436e-01, -2.7793e+00,
         -3.1929e-01,  4.5319e-01, -8.1113e-02,  2.2046e+00, -3.0607e-01,  6.0256e-01, -8.7937e-01, -1.7907e-01,
          4.5590e-01,  9.1642e-01, -8.2732e-01, -2.6444e+00],
        [ 3.6614e+00, -1.2409e-01,  4.5566e-01,  3.4013e+00, -3.7145e+00,  1.3983e+00, -5.0671e-01, -2.3571e+00,
         -3.0858e-01,  1.2141e+00,  1.1480e+00, -1.0411e+00,  4.5755e+00, -7.3959e-02, -6.9769e-02, -1.8258e-01,
         -1.0931e+00, -2.5104e+00,  1.5308e+00, -5.9887e-01, -1.9982e+00, -1.0683e+00,  4.0635e+00, -1.6472e+00,
          1.6508e+00,  1.7244e+00,  1.0170e-01,  1.1893e+00, -4.1306e+00,  2.6169e+00,  1.3450e+00, -2.6039e+00,
         -1.1694e+00,  1.1749e+00,  3.3538e+00,  2.4194e+00, -2.3059e+00,  7.2270e-01, -1.2179e+00, -1.3191e+00,
         -4.6719e-02, -1.3296e-01,  4.3495e-01, -1.6547e+00, -2.1455e+00,  3.4777e+00, -9.9722e-02, -2.9069e+00,
         -1.8446e+00, -1.6236e+00, -2.9678e+00, -4.5314e-01, -2.1559e-01, -1.7033e+00, -7.4768e-01,  4.3143e-01,
          1.3242e+00, -1.4507e+00,  2.5931e+00, -1.4685e+00,  2.0933e-01,  1.8788e+00, -1.0034e-01,  3.2294e-01,
          5.4494e+00, -4.6913e+00,  1.5480e+00, -6.1081e-01, -1.4464e+00,  2.0119e+00,  2.0016e+00, -3.2165e+00,
          2.3131e+00, -1.4544e-01,  1.4801e+00,  3.2548e+00,  1.5708e+00,  3.0084e+00,  1.3467e+00, -2.0270e+00,
          3.3298e-01, -5.6024e-01, -4.2578e+00, -1.9908e+00,  7.7484e-01,  5.3982e+00, -2.9845e-01, -2.0608e+00,
         -4.7497e-01,  1.8782e-01, -2.1048e+00, -2.0081e+00,  9.6142e-01, -3.4746e+00,  9.6999e-01, -1.9231e+00,
          2.7574e+00,  3.9624e+00, -7.8782e-01, -1.9511e+00],
        [ 3.4736e+00,  1.1855e+00, -1.4833e+00,  1.5078e+00, -1.2575e+00,  2.8863e+00, -2.3176e-01, -4.3764e+00,
         -1.2717e+00,  1.8765e+00,  2.6937e-01, -1.4307e+00,  2.6177e+00,  1.0943e+00,  2.8128e+00, -3.2518e+00,
          4.5315e-01, -1.5509e+00,  2.6382e+00, -4.8007e+00, -1.2201e-01, -2.4409e-01,  5.0683e+00, -2.0991e+00,
          1.2841e+00,  6.9009e-02, -3.1315e+00,  3.2608e+00,  1.0372e+00, -4.4583e+00,  1.3645e+00,  1.5726e+00,
          7.7725e-01, -7.4231e-01, -2.8061e+00, -8.4173e-01, -2.7740e+00,  2.6119e+00, -1.8394e+00, -2.8220e+00,
          1.3664e+00,  2.9911e-01,  3.4177e+00,  8.9574e-01, -4.3324e+00,  2.9679e-01, -2.7395e+00, -1.7279e+00,
         -9.3413e-01, -2.9893e+00, -1.0100e+00, -1.1578e+00, -5.2150e+00, -3.9106e+00,  1.9415e+00, -1.9061e+00,
         -2.7839e-01, -1.9666e+00,  4.0422e+00,  3.0610e+00, -1.2351e+00,  4.9301e-01, -1.2161e+00,  3.2437e+00,
         -2.2334e+00, -8.7594e+00,  1.2497e+00,  2.9968e+00, -4.7637e+00,  5.9123e+00,  1.1575e-01, -6.8660e-01,
          1.2529e-01, -1.8128e+00, -3.0175e+00,  8.5108e-01,  4.7443e+00, -1.8950e+00, -2.1496e+00, -7.8302e-01,
         -3.4272e+00,  1.1905e+00,  1.2483e+00, -3.4340e+00, -7.5840e-01,  3.7364e-01, -7.8976e-03, -7.1096e-01,
          9.1797e-01,  6.4332e-01, -2.7249e+00,  9.8480e-01,  5.0090e-01, -1.5204e+00, -2.2722e+00, -3.9586e+00,
         -9.8730e-01,  3.4614e+00,  1.5226e+00,  3.4411e+00],
        [ 2.6608e-04,  2.7036e+00, -5.3023e-01, -5.8721e-01, -2.1924e-01, -5.7310e-01,  7.0671e-01,  2.4174e+00,
         -5.9397e+00, -4.6389e+00, -1.2587e+00, -2.2927e+00, -4.5021e+00,  1.3523e+00,  6.9757e+00, -6.0287e+00,
         -1.7911e+00,  2.9398e+00,  6.3317e+00, -2.5807e+00, -2.5828e+00,  2.3804e+00,  3.4067e+00, -1.5840e+00,
          2.5869e-01, -4.5164e+00, -3.3046e+00,  3.4102e+00,  4.6367e+00, -2.5926e+00,  1.9041e+00,  2.0304e+00,
         -2.4017e-01, -5.5740e+00, -4.4225e+00, -2.5708e-01,  1.2174e+00, -4.0636e-01, -3.4685e+00, -2.6845e+00,
         -1.1474e+00,  1.7493e+00,  6.0148e+00,  1.2682e+00,  2.2959e+00, -9.4295e-01,  1.8923e-01, -6.6633e-01,
          2.4994e+00, -1.8540e-02, -3.2480e+00, -4.4274e+00,  1.0588e+00, -4.8288e+00,  3.9986e+00,  9.1609e-02,
          9.1976e-01, -3.0977e+00, -1.0059e-01,  5.4388e+00,  1.5424e+00,  4.5753e+00,  1.2693e+00, -5.6213e-01,
         -5.2925e+00, -2.9569e+00, -7.1637e-01,  1.9850e+00, -3.9727e+00,  2.2016e-01, -6.5920e-02,  4.2772e-04,
          6.9596e-01, -2.4615e+00, -3.9940e+00,  1.5820e+00, -2.8854e+00, -5.8673e+00,  2.5278e+00,  2.2036e+00,
         -3.4701e-01, -1.2632e+00, -9.4465e-01,  1.0935e+00, -1.7595e+00, -1.7333e+00, -5.0444e-01, -2.7111e+00,
          4.9358e-02, -2.8211e-01, -1.2218e-01,  3.7104e+00,  1.2514e+00,  3.8791e-01, -2.1216e+00,  2.4033e+00,
         -8.6246e-03,  3.8284e+00,  4.4343e+00,  6.3600e+00]])
</pre></div>
</div>
</div>
</div>
<p>Add tanh non-linear activation</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;tanh&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2_mlp_38_0.png" src="../../_images/2_mlp_38_0.png" />
</div>
</div>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>  <span class="c1"># (bs, hid_dim)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 100])
tensor([[ 2.8444e-01, -1.7100e-01, -4.4103e-02,  9.9598e-01, -9.5116e-01, -6.2620e-01, -1.4897e-01, -6.6642e-01,
          4.0993e-01,  9.6735e-01,  8.4764e-01, -2.2922e-01, -9.9878e-01,  1.9296e-01,  3.7952e-01, -5.7446e-01,
         -7.8674e-01, -8.3082e-01, -5.8571e-01, -9.7855e-01,  9.9292e-01, -6.6620e-01,  2.8421e-01,  7.4212e-01,
          9.8434e-01,  5.4470e-01,  9.7775e-01, -9.8573e-01, -9.9697e-01, -5.8709e-02, -7.6379e-01, -8.8651e-01,
          8.8134e-01,  7.8213e-01,  9.4840e-01,  7.0962e-01, -9.9616e-01,  3.9411e-01, -4.0763e-01,  9.0403e-01,
         -6.3493e-01, -5.1724e-02, -8.9373e-01,  7.2078e-01,  2.9752e-01,  9.9793e-01,  7.3984e-01,  6.9378e-01,
         -4.7868e-01,  8.2363e-01,  7.2434e-01,  4.6257e-01,  9.7232e-01, -2.8882e-01, -9.9947e-01,  9.4313e-01,
         -8.0185e-01,  8.9115e-01,  8.3305e-01,  9.4418e-01, -6.0916e-01,  5.5736e-01, -7.1375e-01, -9.9919e-01,
          9.9372e-01,  9.3789e-01,  8.9890e-01, -9.1084e-01, -1.5029e-01,  7.1603e-01,  9.9152e-01, -9.7320e-01,
         -9.9948e-01, -9.0716e-01,  2.4901e-01,  9.4004e-01,  9.8059e-01,  9.4600e-01,  8.0665e-01, -4.9454e-01,
          9.6619e-01, -9.5750e-01,  9.8681e-01, -9.3254e-01,  7.4530e-01,  1.0662e-01,  8.8637e-01, -8.9893e-01,
         -8.3897e-01, -5.0767e-01, -7.6333e-01,  9.0758e-02, -2.1552e-01,  5.8879e-01,  9.0107e-01,  7.0578e-01,
          6.7474e-01,  8.5198e-01, -9.5241e-01, -9.9298e-01],
        [-7.1971e-01,  6.0535e-01,  1.6766e-01,  9.9990e-01, -9.8418e-01, -8.0121e-01, -8.7310e-01, -6.3765e-01,
         -7.7966e-01,  9.1861e-01,  8.7048e-01, -1.1972e-01, -9.9999e-01, -7.2632e-02,  9.8651e-01, -9.6686e-01,
         -8.0942e-01, -9.6387e-01,  4.7267e-01, -9.8631e-01,  7.9222e-01,  1.7190e-01,  4.3447e-01,  5.3209e-01,
          9.8867e-01,  8.0676e-01,  8.7377e-01, -7.6574e-01, -9.7522e-01, -7.7460e-01, -8.7782e-01, -6.1279e-01,
          8.4422e-01, -9.6854e-01,  9.3335e-01,  6.5714e-01, -9.8411e-01,  5.0901e-01, -8.5831e-01,  9.8503e-01,
         -5.8917e-01,  8.7513e-01, -8.6723e-01,  8.4680e-01,  9.4403e-01,  9.9905e-01,  9.7864e-01,  9.0993e-01,
          9.9484e-01,  9.8131e-01,  1.0492e-01,  5.6506e-01,  9.9294e-01, -5.4232e-01, -9.9996e-01,  9.6912e-01,
         -9.2648e-01,  6.1575e-01,  9.8276e-01,  9.9373e-01,  6.3830e-01,  8.5845e-01, -9.0172e-01, -9.9996e-01,
          9.9804e-01,  8.3270e-01, -7.5979e-01, -7.8750e-01,  7.9424e-01, -5.3220e-01,  9.8748e-01, -9.4928e-01,
         -9.8730e-01, -9.6687e-01, -6.7819e-01,  9.5700e-02, -7.4774e-01,  9.7930e-01,  9.9433e-01, -8.4150e-03,
          9.9036e-01, -9.9727e-01,  9.5308e-01, -9.5256e-01, -7.4344e-01,  6.0205e-01,  5.8126e-01, -9.9232e-01,
         -3.0886e-01,  4.2451e-01, -8.0936e-02,  9.7596e-01, -2.9686e-01,  5.3887e-01, -7.0610e-01, -1.7718e-01,
          4.2673e-01,  7.2420e-01, -6.7904e-01, -9.8996e-01],
        [ 9.9868e-01, -1.2346e-01,  4.2654e-01,  9.9778e-01, -9.9881e-01,  8.8497e-01, -4.6738e-01, -9.8222e-01,
         -2.9914e-01,  8.3789e-01,  8.1708e-01, -7.7830e-01,  9.9979e-01, -7.3824e-02, -6.9656e-02, -1.8058e-01,
         -7.9800e-01, -9.8689e-01,  9.1057e-01, -5.3624e-01, -9.6390e-01, -7.8882e-01,  9.9941e-01, -9.2847e-01,
          9.2897e-01,  9.3839e-01,  1.0135e-01,  8.3035e-01, -9.9948e-01,  9.8939e-01,  8.7288e-01, -9.8911e-01,
         -8.2407e-01,  8.2583e-01,  9.9756e-01,  9.8429e-01, -9.8033e-01,  6.1858e-01, -8.3905e-01, -8.6655e-01,
         -4.6685e-02, -1.3218e-01,  4.0945e-01, -9.2950e-01, -9.7299e-01,  9.9809e-01, -9.9393e-02, -9.9405e-01,
         -9.5124e-01, -9.2514e-01, -9.9473e-01, -4.2448e-01, -2.1231e-01, -9.3582e-01, -6.3376e-01,  4.0651e-01,
          8.6783e-01, -8.9582e-01,  9.8888e-01, -8.9929e-01,  2.0633e-01,  9.5439e-01, -1.0001e-01,  3.1216e-01,
          9.9996e-01, -9.9983e-01,  9.1346e-01, -5.4470e-01, -8.9498e-01,  9.6486e-01,  9.6414e-01, -9.9679e-01,
          9.8061e-01, -1.4443e-01,  9.0149e-01,  9.9703e-01,  9.1715e-01,  9.9514e-01,  8.7328e-01, -9.6589e-01,
          3.2120e-01, -5.0815e-01, -9.9960e-01, -9.6337e-01,  6.4973e-01,  9.9996e-01, -2.8990e-01, -9.6808e-01,
         -4.4220e-01,  1.8564e-01, -9.7073e-01, -9.6459e-01,  7.4491e-01, -9.9808e-01,  7.4870e-01, -9.5817e-01,
          9.9198e-01,  9.9928e-01, -6.5717e-01, -9.6041e-01],
        [ 9.9808e-01,  8.2918e-01, -9.0209e-01,  9.0655e-01, -8.5038e-01,  9.9380e-01, -2.2769e-01, -9.9968e-01,
         -8.5427e-01,  9.5418e-01,  2.6304e-01, -8.9181e-01,  9.8941e-01,  7.9845e-01,  9.9282e-01, -9.9701e-01,
          4.2449e-01, -9.1394e-01,  9.8983e-01, -9.9986e-01, -1.2141e-01, -2.3935e-01,  9.9992e-01, -9.7040e-01,
          8.5758e-01,  6.8900e-02, -9.9620e-01,  9.9706e-01,  7.7680e-01, -9.9973e-01,  8.7744e-01,  9.1743e-01,
          6.5113e-01, -6.3054e-01, -9.9272e-01, -6.8673e-01, -9.9224e-01,  9.8928e-01, -9.5074e-01, -9.9295e-01,
          8.7788e-01,  2.9050e-01,  9.9785e-01,  7.1422e-01, -9.9965e-01,  2.8837e-01, -9.9169e-01, -9.3881e-01,
         -7.3252e-01, -9.9495e-01, -7.6575e-01, -8.2031e-01, -9.9994e-01, -9.9920e-01,  9.5965e-01, -9.5676e-01,
         -2.7142e-01, -9.6159e-01,  9.9938e-01,  9.9562e-01, -8.4405e-01,  4.5660e-01, -8.3849e-01,  9.9696e-01,
         -9.7729e-01, -1.0000e+00,  8.4820e-01,  9.9502e-01, -9.9985e-01,  9.9999e-01,  1.1524e-01, -5.9579e-01,
          1.2464e-01, -9.4812e-01, -9.9522e-01,  6.9163e-01,  9.9985e-01, -9.5581e-01, -9.7321e-01, -6.5444e-01,
         -9.9789e-01,  8.3075e-01,  8.4780e-01, -9.9792e-01, -6.4013e-01,  3.5717e-01, -7.8974e-03, -6.1128e-01,
          7.2494e-01,  5.6716e-01, -9.9144e-01,  7.5513e-01,  4.6282e-01, -9.0877e-01, -9.7897e-01, -9.9927e-01,
         -7.5621e-01,  9.9803e-01,  9.0914e-01,  9.9795e-01],
        [ 2.6608e-04,  9.9107e-01, -4.8556e-01, -5.2789e-01, -2.1579e-01, -5.1763e-01,  6.0861e-01,  9.8423e-01,
         -9.9999e-01, -9.9981e-01, -8.5070e-01, -9.7981e-01, -9.9975e-01,  8.7459e-01,  1.0000e+00, -9.9999e-01,
         -9.4587e-01,  9.9442e-01,  9.9999e-01, -9.8860e-01, -9.8865e-01,  9.8303e-01,  9.9780e-01, -9.1923e-01,
          2.5307e-01, -9.9976e-01, -9.9731e-01,  9.9782e-01,  9.9981e-01, -9.8886e-01,  9.5658e-01,  9.6612e-01,
         -2.3566e-01, -9.9997e-01, -9.9971e-01, -2.5156e-01,  8.3887e-01, -3.8538e-01, -9.9806e-01, -9.9073e-01,
         -8.1691e-01,  9.4130e-01,  9.9999e-01,  8.5330e-01,  9.7994e-01, -7.3657e-01,  1.8700e-01, -5.8256e-01,
          9.8660e-01, -1.8538e-02, -9.9699e-01, -9.9971e-01,  7.8520e-01, -9.9987e-01,  9.9933e-01,  9.1354e-02,
          7.2579e-01, -9.9593e-01, -1.0025e-01,  9.9996e-01,  9.1252e-01,  9.9979e-01,  8.5361e-01, -5.0956e-01,
         -9.9995e-01, -9.9461e-01, -6.1466e-01,  9.6295e-01, -9.9929e-01,  2.1667e-01, -6.5825e-02,  4.2772e-04,
          6.0180e-01, -9.8555e-01, -9.9932e-01,  9.1891e-01, -9.9379e-01, -9.9998e-01,  9.8733e-01,  9.7592e-01,
         -3.3372e-01, -8.5195e-01, -7.3735e-01,  7.9816e-01, -9.4245e-01, -9.3944e-01, -4.6560e-01, -9.9120e-01,
          4.9318e-02, -2.7486e-01, -1.2158e-01,  9.9880e-01,  8.4868e-01,  3.6956e-01, -9.7168e-01,  9.8378e-01,
         -8.6243e-03,  9.9905e-01,  9.9972e-01,  9.9999e-01]])
</pre></div>
</div>
</div>
</div>
<p><strong>Output layer</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="mi">27</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">27</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">W2</span> <span class="o">+</span> <span class="n">b2</span>  <span class="c1"># (bs, 27)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 27])
tensor([[  3.5164,   7.9188, -14.8647,   0.8755,   1.6126,   7.5288,   2.6149,  16.4876,   2.9750,  -1.8132,  -1.3996,
          -4.4212, -11.2526,   3.1436, -15.8442,  -8.6950,   8.4528,  16.0921,   3.0693,   7.1922,   1.5154,   8.1665,
           2.7921,   3.4124,  10.3626,  -9.2752,  -1.7361],
        [ 12.9916,  10.1320, -10.9615,  -2.1059,   1.6970,   9.6038,  -0.3085,  18.8717,   2.9242,   5.9700,   1.9080,
           0.7925,  -0.4738,   6.4673,  -3.4823,  -4.2014,  10.1522,  14.5589,   1.6249,   5.8882,  -5.1297,   4.4828,
          -7.3323,   5.6532,  12.2617,  -6.5820,   2.5483],
        [ -6.0280,  -1.1630,  14.2044,  -1.3971,  -8.9427,  -0.7807,   5.4797, -12.5450,   5.1574,  -0.8181,  12.1904,
          -7.6255,  -4.9768,  -2.2345,  -2.6586,  -3.6031,  -5.8067,   7.4556,   2.7115,   3.8857,  10.9229,   6.9937,
          -9.3695,  -3.1185, -10.0213,  -7.4274,   2.3042],
        [ -5.7312, -14.1166,  19.7070,  -0.5077,  -4.8559,   6.1687, -11.1373,  -1.5845,   5.6457,   7.2706,   7.8800,
         -14.1940,  -0.2375,  13.2387,  -3.3675,   5.2660,  -5.5096, -14.0373,  -1.7789,  -9.0936,  17.6510,  -9.6496,
          -3.9086,   7.9329,  -3.0212,  -8.9337,  -0.5434],
        [  2.2441, -21.5520,   9.7879,   1.3651, -13.9590,   8.7249,  -3.9102,  -2.7913,  -2.1874,   4.8023,  13.2302,
          -4.6565,  10.5169,  10.7108,   7.0819,  16.2046,   4.4507, -10.0559,  -2.9908, -10.9940,  10.5577, -17.6467,
         -19.3634,  -1.2516,   1.7977,   5.1121,   2.9888]])
</pre></div>
</div>
</div>
</div>
<p>Transform logits into probabilities with softmax.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (bs, 27)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 27])
tensor([[1.3876e-06, 1.1329e-04, 1.4436e-14, 9.8930e-08, 2.0676e-07, 7.6703e-05, 5.6332e-07, 5.9646e-01, 8.0750e-07,
         6.7244e-09, 1.0169e-08, 4.9546e-10, 5.3478e-13, 9.5581e-07, 5.4206e-15, 6.9014e-12, 1.9326e-04, 4.0164e-01,
         8.8735e-07, 5.4785e-05, 1.8761e-07, 1.4514e-04, 6.7254e-07, 1.2505e-06, 1.3047e-03, 3.8632e-12, 7.2630e-09],
        [2.7454e-03, 1.5728e-04, 1.0862e-13, 7.6177e-10, 3.4152e-08, 9.2749e-05, 4.5965e-09, 9.8235e-01, 1.1651e-07,
         2.4499e-06, 4.2173e-08, 1.3822e-08, 3.8963e-09, 4.0285e-06, 1.9235e-10, 9.3704e-11, 1.6049e-04, 1.3160e-02,
         3.1776e-08, 2.2575e-06, 3.7035e-11, 5.5369e-07, 4.0931e-12, 1.7847e-06, 1.3231e-03, 8.6669e-12, 8.0006e-08],
        [1.3926e-09, 1.8057e-07, 8.5231e-01, 1.4288e-07, 7.5501e-11, 2.6465e-07, 1.3853e-04, 2.0582e-12, 1.0036e-04,
         2.5493e-07, 1.1375e-01, 2.8186e-10, 3.9841e-09, 6.1845e-08, 4.0469e-08, 1.5737e-08, 1.7374e-09, 9.9920e-04,
         8.6960e-06, 2.8135e-05, 3.2023e-02, 6.2959e-04, 4.9273e-11, 2.5548e-08, 2.5677e-11, 3.4359e-10, 5.7865e-06],
        [7.9328e-12, 1.8101e-15, 8.8531e-01, 1.4722e-09, 1.9036e-11, 1.1682e-06, 3.5613e-14, 5.0155e-10, 6.9245e-07,
         3.5162e-06, 6.4674e-06, 1.6753e-15, 1.9290e-09, 1.3739e-03, 8.4327e-11, 4.7365e-07, 9.9015e-12, 1.9595e-15,
         4.1294e-10, 2.7491e-13, 1.1330e-01, 1.5765e-13, 4.9090e-11, 6.8186e-06, 1.1923e-10, 3.2256e-13, 1.4206e-09],
        [8.1267e-07, 3.7619e-17, 1.5351e-03, 3.3740e-07, 7.4645e-14, 5.3025e-04, 1.7263e-09, 5.2851e-09, 9.6682e-09,
         1.0494e-05, 4.7985e-02, 8.1849e-10, 3.1823e-03, 3.8634e-03, 1.0255e-04, 9.3945e-01, 7.3827e-06, 3.6989e-12,
         4.3293e-09, 1.4477e-12, 3.3147e-03, 1.8682e-15, 3.3565e-16, 2.4646e-08, 5.2007e-07, 1.4304e-05, 1.7113e-06]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="loss-function">
<h2>Loss function<a class="headerlink" href="#loss-function" title="Permalink to this headline">#</a></h2>
<p>To calculate the loss of the model, we extract the predicted probability of the true label and compute the negative log-likelihood. The objective is to minimize this loss in order to improve the model’s ability to predict the correct character in the sequence.<br />
Btw: Minimizing the negative log-likelihood is equivalent to maximizing the likelihood.</p>
<p><strong>Negative log-likelihood loss (NLL)</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">Y</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(17.2929)
</pre></div>
</div>
</div>
</div>
<p><strong>Cross entropy loss</strong>: The negative log-likelihood and cross entropy are two interpretations of the same formula.</p>
<p><span class="math notranslate nohighlight">\(\text{cross entropy}= H(p,q)= -\sum_x{p(x)log(q(x))}\)</span><br />
with<br />
<span class="math notranslate nohighlight">\(x\)</span>: <span class="math notranslate nohighlight">\(\in\)</span> classes<br />
<span class="math notranslate nohighlight">\(p(x)\)</span>: true probability distribution<br />
<span class="math notranslate nohighlight">\(q(x)\)</span>: predicted probability distribution</p>
<p>There are many good reasons to use <code class="docutils literal notranslate"><span class="pre">F.cross_entropy()</span></code> instead of manually calculating the loss. It’s much more efficient and it’s more numerically well behaved. The manual calculation uses <code class="docutils literal notranslate"><span class="pre">logits.exp()</span></code> and that can lead to overflow when an element of logits is very high. <code class="docutils literal notranslate"><span class="pre">F.cross_entropy()</span></code> avoids these kinds of problems.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(17.2929)
</pre></div>
</div>
</div>
</div>
</section>
<section id="mlp-model">
<h2>MLP model<a class="headerlink" href="#mlp-model" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> <span class="k">if</span> <span class="n">seed</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_dim</span> <span class="o">=</span> <span class="n">emb_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hid_dim</span> <span class="o">=</span> <span class="n">hid_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">block_size</span> <span class="o">*</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">hid_dim</span><span class="p">,</span> <span class="mi">27</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">27</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_parameters</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Does a forward pass and returns the logits&quot;&quot;&quot;</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">[</span><span class="n">xs</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_dim</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the gradients to None&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">optimise_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Updat the parameters&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<section id="training-1-batch">
<h3>Training 1 batch<a class="headerlink" href="#training-1-batch" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">ds</span><span class="o">.</span><span class="n">Y</span>
<span class="n">Xsamp</span><span class="p">,</span> <span class="n">Ysamp</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">32</span><span class="p">],</span> <span class="n">Y</span><span class="p">[:</span><span class="mi">32</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;# parameters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># parameters: 3481
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emb_dim</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">hid_dim</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;# parameters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># parameters: 3808
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20000</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xsamp</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Ysamp</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">loss</span><span class="si">=}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss=tensor(0.2516, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>The low loss value of 0.2515 when training a single batch suggests that the model is overfitting, as it has a large number of parameters (3,481) relative to the small number of samples (32). This allows the model to memorize the true labels, rather than learning generalizable patterns.</p>
<p>It is also important to note that the loss will not be able to decrease to zero because there are multiple different y values for the same x values in the batch. For example, the 5 samples with all [0, 0, 0] x values have different y values. In this case, the minimum loss that can be achieved when the network has memorized all samples can be calculated as follows:</p>
<p><span class="math notranslate nohighlight">\(H(p,q) = -\sum_x{p(x)log(q(x))}\)</span><br />
where:<br />
<span class="math notranslate nohighlight">\(p(x)=1\)</span> and <span class="math notranslate nohighlight">\(q(x)=1\)</span> for all “unique” samples (not [0, 0, 0])<br />
<span class="math notranslate nohighlight">\(p(x)=1\)</span> and <span class="math notranslate nohighlight">\(q(x)=1/5\)</span> for the 5 “non-unique” samples ([0, 0, 0])<br />
therefore:<br />
<span class="math notranslate nohighlight">\(H(p,q) = -\frac{5\log(1/5)}{32}=0.2515\)</span></p>
</section>
<section id="training-the-full-dataset">
<h3>Training the full dataset<a class="headerlink" href="#training-the-full-dataset" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">ds</span><span class="o">.</span><span class="n">Y</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;# parameters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># parameters: 3481
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">loss</span><span class="si">=}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss=tensor(2.5585, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;log-loss full dataset&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;steps&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$log_</span><span class="si">{10}</span><span class="s1">(loss)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2_mlp_61_0.png" src="../../_images/2_mlp_61_0.png" />
</div>
</div>
<p>Training for 1000 iterations on the full dataset is very slow. It takes more than 2 min to obtain a loss of 2.5610.</p>
</section>
<section id="training-with-mini-batches">
<h3>Training with mini-batches<a class="headerlink" href="#training-with-mini-batches" title="Permalink to this headline">#</a></h3>
<p>Instead of training on the full dataset, we can randomly select at each iteration a portion of the dataset, a mini-batch,  and do a forward and backward pass and update on each mini-batch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">ds</span><span class="o">.</span><span class="n">Y</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;# parameters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># parameters: 3481
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_minibatch</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">bs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>


<span class="n">bs</span> <span class="o">=</span> <span class="mi">320</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">get_minibatch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Minibatch loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Minibatch loss: 2.4103
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span>
<span class="n">ma</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">losses_ma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ma</span><span class="p">)</span> <span class="o">/</span> <span class="n">ma</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">ma</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses_ma</span><span class="p">)</span> <span class="o">+</span> <span class="n">ma</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">losses_ma</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;log-loss mini-batches&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;steps&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$log_</span><span class="si">{10}</span><span class="s1">(loss)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2_mlp_67_0.png" src="../../_images/2_mlp_67_0.png" />
</div>
</div>
<p>The use of minibatches in training can result in a decrease in the precision of the gradient calculation. While the gradient calculated from minibatches may not be as accurate as the gradient obtained from training on the entire dataset, it is still sufficient for practical use. It is generally more efficient to make progress with an approximate gradient by taking more steps, rather than obtaining an exact gradient but taking fewer steps. To accurately gauge the performance of the network, it is necessary to evaluate the loss on the entire dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">loss</span><span class="si">=}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss=tensor(2.5235, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Training on mini-batches of size 320 is much faster than training on the full dataset. Training for 1000 iterations takes less than 1 sec and has a similar loss of 2.5756 compared to the loss of 2.5610 from training on the full dataset.</p>
</section>
</section>
<section id="train-valid-test-split">
<h2>Train | valid | test split<a class="headerlink" href="#train-valid-test-split" title="Permalink to this headline">#</a></h2>
<p>If the number of parameters of a model is large enough then the model has enough capacity to memorize the training set. The loss will be very low, but if you sample from it then it will only produce samples it already saw in the trainingset. It won’t produce new data.<br />
If you evaluate the loss on withheld names, i.e. samples that are not in the traininset, than that loss on those samples will be very high. The model is <strong>overfitted</strong>.</p>
<p>The solution is to split the dataset into 3 sets: <strong>training set, validation set and test set</strong>. The training set (~80%) is for optimizing the parameters of the model. the validation set (~10%) is used to tune the hyperparameters of the model and the test set (~10%) is used to test the quality of the model at the very end.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_valid_test_split</span><span class="p">(</span>
    <span class="n">names</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">train_frac</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">valid_frac</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Function that split the dataset.</span>
<span class="sd">    It returns a tuple of (Xtrn, Ytrn, Xval, Yval, Xtst, Ytst)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">seed</span><span class="p">:</span>
        <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">names</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="n">n1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">train_frac</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">n2</span> <span class="o">=</span> <span class="n">n1</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">valid_frac</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="n">n1</span><span class="p">])</span>
    <span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">ds</span><span class="o">.</span><span class="n">Y</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">])</span>
    <span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">ds</span><span class="o">.</span><span class="n">Y</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">n2</span><span class="p">:])</span>
    <span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">ds</span><span class="o">.</span><span class="n">Y</span>
    <span class="k">return</span> <span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">,</span> <span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">,</span> <span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">ds</span><span class="o">.</span><span class="n">Y</span>
<span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">,</span> <span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">,</span> <span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span> <span class="o">=</span> <span class="n">train_valid_test_split</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="tuning-hyperparameters">
<h2>Tuning hyperparameters<a class="headerlink" href="#tuning-hyperparameters" title="Permalink to this headline">#</a></h2>
<section id="learning-rate">
<h3>Learning rate<a class="headerlink" href="#learning-rate" title="Permalink to this headline">#</a></h3>
<p>One way to determin a reasonable learning rate is as follows:</p>
<ul class="simple">
<li><p>Determine lower and upper bounds for the learning rate through trial and error. A learning rate that is too low may result in slow convergence, while a learning rate that is too high may result in unstable or oscillating loss.</p></li>
<li><p>Train the network for one step using the lower bound learning rate. At each subsequent step, exponentially increase the learning rate up to the upper bound.</p></li>
<li><p>Plot the learning rate (lr) against the loss. A reasonable learning rate is likely to be at the plateau of the plot.</p></li>
</ul>
<p><strong>Upper and lower bound</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fix</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">bs</span> <span class="o">=</span> <span class="mi">320</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e0</span><span class="p">,</span> <span class="mf">1e1</span><span class="p">]):</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">get_minibatch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2_mlp_79_0.png" src="../../_images/2_mlp_79_0.png" />
</div>
</div>
<p><strong>Traing with exponentially increasing lr</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lb</span><span class="p">,</span> <span class="n">ub</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">lr_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">lb</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">ub</span><span class="p">),</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># [-3 ... 0]</span>
<span class="n">lrs</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="n">lr_exp</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lrs</span><span class="p">):</span>
    <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">get_minibatch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ma</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">losses_ma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ma</span><span class="p">)</span> <span class="o">/</span> <span class="n">ma</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lr_exp</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lr_exp</span><span class="p">[</span><span class="n">ma</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">:</span> <span class="o">-</span><span class="n">ma</span> <span class="o">//</span> <span class="mi">2</span><span class="p">],</span> <span class="n">losses_ma</span><span class="p">)</span>
<span class="n">min_ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">losses_ma</span><span class="p">)</span>
<span class="n">min_lr</span> <span class="o">=</span> <span class="n">lrs</span><span class="p">[</span><span class="n">min_ix</span> <span class="o">+</span> <span class="n">ma</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">min_lr</span><span class="p">),</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">min_lr</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;lr: </span><span class="si">{</span><span class="n">min_lr</span><span class="si">:</span><span class="s1">.2</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\log_</span><span class="si">{10}</span><span class="s1">(lr)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2_mlp_82_0.png" src="../../_images/2_mlp_82_0.png" />
</div>
</div>
<p>A reasonably good learning rate is somewhere between <span class="math notranslate nohighlight">\(10^{-1}\)</span> and <span class="math notranslate nohighlight">\(10^{-0.5}\)</span>.</p>
<p><strong>Weight decay</strong>: We train our model with the decent learning rate till loss doesn’t decrease anymore. Then we decay the learning rate with factor 10 and train a little longer.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">MLP</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>


<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.15</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">320</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">500_000</span>
<span class="n">decay_step</span> <span class="o">=</span> <span class="mi">495_000</span>
<span class="n">batch_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">valid_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">lrd</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="c1"># Lr decay</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">decay_step</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">lrd</span><span class="p">:</span>
        <span class="n">lr</span> <span class="o">/=</span> <span class="mi">10</span>
        <span class="n">lrd</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Lr decay: </span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">get_minibatch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>
    <span class="n">batch_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">)</span>
        <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
        <span class="n">valid_loss</span> <span class="o">=</span> <span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">)</span>
        <span class="n">valid_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">valid_loss</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">)</span>
        <span class="n">valid_loss</span> <span class="o">=</span> <span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Loss (train | valid) : </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">&gt;7</span><span class="si">}</span><span class="s1"> | </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> |  </span><span class="si">{</span><span class="n">valid_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Loss (total | test) : </span><span class="si">{</span><span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> | </span><span class="si">{</span><span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss (train | valid) :       0 | 16.3661 |  16.3210
Loss (train | valid) :    1000 | 2.5557 |  2.5623
Loss (train | valid) :    2000 | 2.4914 |  2.4944
Loss (train | valid) :    3000 | 2.4595 |  2.4617
Loss (train | valid) :    4000 | 2.4517 |  2.4549
Loss (train | valid) :    5000 | 2.4258 |  2.4264
Loss (train | valid) :    6000 | 2.4310 |  2.4323
Loss (train | valid) :    7000 | 2.4114 |  2.4124
Loss (train | valid) :    8000 | 2.4117 |  2.4152
Loss (train | valid) :    9000 | 2.4018 |  2.4057
Loss (train | valid) :   10000 | 2.3978 |  2.4002
Loss (train | valid) :   11000 | 2.3933 |  2.3987
Loss (train | valid) :   12000 | 2.3879 |  2.3888
Loss (train | valid) :   13000 | 2.3834 |  2.3859
Loss (train | valid) :   14000 | 2.3875 |  2.3913
Loss (train | valid) :   15000 | 2.3744 |  2.3775
Loss (train | valid) :   16000 | 2.3689 |  2.3739
Loss (train | valid) :   17000 | 2.3729 |  2.3774
Loss (train | valid) :   18000 | 2.3636 |  2.3654
Loss (train | valid) :   19000 | 2.3600 |  2.3649
Loss (train | valid) :   20000 | 2.3596 |  2.3644
Loss (train | valid) :   21000 | 2.3602 |  2.3655
Loss (train | valid) :   22000 | 2.3544 |  2.3588
Loss (train | valid) :   23000 | 2.3539 |  2.3594
Loss (train | valid) :   24000 | 2.3508 |  2.3572
Loss (train | valid) :   25000 | 2.3428 |  2.3487
Loss (train | valid) :   26000 | 2.3398 |  2.3453
Loss (train | valid) :   27000 | 2.3345 |  2.3377
Loss (train | valid) :   28000 | 2.3509 |  2.3561
Loss (train | valid) :   29000 | 2.3347 |  2.3400
Loss (train | valid) :   30000 | 2.3355 |  2.3401
Loss (train | valid) :   31000 | 2.3285 |  2.3338
Loss (train | valid) :   32000 | 2.3282 |  2.3331
Loss (train | valid) :   33000 | 2.3351 |  2.3398
Loss (train | valid) :   34000 | 2.3268 |  2.3318
Loss (train | valid) :   35000 | 2.3218 |  2.3271
Loss (train | valid) :   36000 | 2.3223 |  2.3275
Loss (train | valid) :   37000 | 2.3245 |  2.3283
Loss (train | valid) :   38000 | 2.3221 |  2.3291
Loss (train | valid) :   39000 | 2.3169 |  2.3223
Loss (train | valid) :   40000 | 2.3196 |  2.3242
Loss (train | valid) :   41000 | 2.3123 |  2.3172
Loss (train | valid) :   42000 | 2.3175 |  2.3226
Loss (train | valid) :   43000 | 2.3109 |  2.3148
Loss (train | valid) :   44000 | 2.3154 |  2.3198
Loss (train | valid) :   45000 | 2.3069 |  2.3139
Loss (train | valid) :   46000 | 2.3190 |  2.3247
Loss (train | valid) :   47000 | 2.3137 |  2.3196
Loss (train | valid) :   48000 | 2.3096 |  2.3161
Loss (train | valid) :   49000 | 2.3062 |  2.3133
Loss (train | valid) :   50000 | 2.3082 |  2.3134
Loss (train | valid) :   51000 | 2.3027 |  2.3068
Loss (train | valid) :   52000 | 2.3035 |  2.3088
Loss (train | valid) :   53000 | 2.3034 |  2.3093
Loss (train | valid) :   54000 | 2.3034 |  2.3075
Loss (train | valid) :   55000 | 2.2985 |  2.3063
Loss (train | valid) :   56000 | 2.2989 |  2.3071
Loss (train | valid) :   57000 | 2.2980 |  2.3022
Loss (train | valid) :   58000 | 2.2935 |  2.3003
Loss (train | valid) :   59000 | 2.2984 |  2.3023
Loss (train | valid) :   60000 | 2.2952 |  2.3002
Loss (train | valid) :   61000 | 2.2947 |  2.3022
Loss (train | valid) :   62000 | 2.3086 |  2.3159
Loss (train | valid) :   63000 | 2.2894 |  2.2932
Loss (train | valid) :   64000 | 2.2946 |  2.2996
Loss (train | valid) :   65000 | 2.2868 |  2.2917
Loss (train | valid) :   66000 | 2.3011 |  2.3055
Loss (train | valid) :   67000 | 2.2947 |  2.3004
Loss (train | valid) :   68000 | 2.2884 |  2.2911
Loss (train | valid) :   69000 | 2.2947 |  2.3000
Loss (train | valid) :   70000 | 2.2887 |  2.2951
Loss (train | valid) :   71000 | 2.2831 |  2.2866
Loss (train | valid) :   72000 | 2.2884 |  2.2949
Loss (train | valid) :   73000 | 2.2817 |  2.2871
Loss (train | valid) :   74000 | 2.2835 |  2.2894
Loss (train | valid) :   75000 | 2.2798 |  2.2834
Loss (train | valid) :   76000 | 2.2843 |  2.2902
Loss (train | valid) :   77000 | 2.2837 |  2.2880
Loss (train | valid) :   78000 | 2.2740 |  2.2816
Loss (train | valid) :   79000 | 2.2816 |  2.2890
Loss (train | valid) :   80000 | 2.2739 |  2.2798
Loss (train | valid) :   81000 | 2.2859 |  2.2920
Loss (train | valid) :   82000 | 2.2749 |  2.2800
Loss (train | valid) :   83000 | 2.2730 |  2.2799
Loss (train | valid) :   84000 | 2.2787 |  2.2851
Loss (train | valid) :   85000 | 2.2714 |  2.2789
Loss (train | valid) :   86000 | 2.2738 |  2.2786
Loss (train | valid) :   87000 | 2.2835 |  2.2886
Loss (train | valid) :   88000 | 2.2709 |  2.2760
Loss (train | valid) :   89000 | 2.2688 |  2.2739
Loss (train | valid) :   90000 | 2.2744 |  2.2810
Loss (train | valid) :   91000 | 2.2690 |  2.2761
Loss (train | valid) :   92000 | 2.2657 |  2.2695
Loss (train | valid) :   93000 | 2.2704 |  2.2755
Loss (train | valid) :   94000 | 2.2703 |  2.2751
Loss (train | valid) :   95000 | 2.2636 |  2.2687
Loss (train | valid) :   96000 | 2.2669 |  2.2697
Loss (train | valid) :   97000 | 2.2663 |  2.2710
Loss (train | valid) :   98000 | 2.2717 |  2.2761
Loss (train | valid) :   99000 | 2.2746 |  2.2774
Loss (train | valid) :  100000 | 2.2618 |  2.2677
Loss (train | valid) :  101000 | 2.2659 |  2.2691
Loss (train | valid) :  102000 | 2.2656 |  2.2696
Loss (train | valid) :  103000 | 2.2643 |  2.2703
Loss (train | valid) :  104000 | 2.2659 |  2.2716
Loss (train | valid) :  105000 | 2.2614 |  2.2667
Loss (train | valid) :  106000 | 2.2584 |  2.2646
Loss (train | valid) :  107000 | 2.2634 |  2.2679
Loss (train | valid) :  108000 | 2.2618 |  2.2659
Loss (train | valid) :  109000 | 2.2557 |  2.2618
Loss (train | valid) :  110000 | 2.2574 |  2.2625
Loss (train | valid) :  111000 | 2.2555 |  2.2615
Loss (train | valid) :  112000 | 2.2617 |  2.2696
Loss (train | valid) :  113000 | 2.2565 |  2.2631
Loss (train | valid) :  114000 | 2.2585 |  2.2643
Loss (train | valid) :  115000 | 2.2524 |  2.2568
Loss (train | valid) :  116000 | 2.2636 |  2.2676
Loss (train | valid) :  117000 | 2.2627 |  2.2710
Loss (train | valid) :  118000 | 2.2508 |  2.2569
Loss (train | valid) :  119000 | 2.2571 |  2.2630
Loss (train | valid) :  120000 | 2.2542 |  2.2600
Loss (train | valid) :  121000 | 2.2567 |  2.2625
Loss (train | valid) :  122000 | 2.2535 |  2.2609
Loss (train | valid) :  123000 | 2.2560 |  2.2626
Loss (train | valid) :  124000 | 2.2576 |  2.2634
Loss (train | valid) :  125000 | 2.2536 |  2.2594
Loss (train | valid) :  126000 | 2.2525 |  2.2575
Loss (train | valid) :  127000 | 2.2487 |  2.2545
Loss (train | valid) :  128000 | 2.2609 |  2.2670
Loss (train | valid) :  129000 | 2.2541 |  2.2579
Loss (train | valid) :  130000 | 2.2516 |  2.2577
Loss (train | valid) :  131000 | 2.2514 |  2.2556
Loss (train | valid) :  132000 | 2.2468 |  2.2532
Loss (train | valid) :  133000 | 2.2477 |  2.2541
Loss (train | valid) :  134000 | 2.2555 |  2.2624
Loss (train | valid) :  135000 | 2.2496 |  2.2564
Loss (train | valid) :  136000 | 2.2455 |  2.2517
Loss (train | valid) :  137000 | 2.2510 |  2.2566
Loss (train | valid) :  138000 | 2.2485 |  2.2539
Loss (train | valid) :  139000 | 2.2459 |  2.2514
Loss (train | valid) :  140000 | 2.2482 |  2.2545
Loss (train | valid) :  141000 | 2.2469 |  2.2519
Loss (train | valid) :  142000 | 2.2421 |  2.2487
Loss (train | valid) :  143000 | 2.2435 |  2.2493
Loss (train | valid) :  144000 | 2.2430 |  2.2492
Loss (train | valid) :  145000 | 2.2485 |  2.2559
Loss (train | valid) :  146000 | 2.2409 |  2.2473
Loss (train | valid) :  147000 | 2.2395 |  2.2478
Loss (train | valid) :  148000 | 2.2401 |  2.2474
Loss (train | valid) :  149000 | 2.2466 |  2.2527
Loss (train | valid) :  150000 | 2.2382 |  2.2440
Loss (train | valid) :  151000 | 2.2422 |  2.2492
Loss (train | valid) :  152000 | 2.2453 |  2.2488
Loss (train | valid) :  153000 | 2.2457 |  2.2528
Loss (train | valid) :  154000 | 2.2388 |  2.2446
Loss (train | valid) :  155000 | 2.2386 |  2.2439
Loss (train | valid) :  156000 | 2.2460 |  2.2510
Loss (train | valid) :  157000 | 2.2422 |  2.2476
Loss (train | valid) :  158000 | 2.2429 |  2.2508
Loss (train | valid) :  159000 | 2.2380 |  2.2440
Loss (train | valid) :  160000 | 2.2360 |  2.2433
Loss (train | valid) :  161000 | 2.2389 |  2.2470
Loss (train | valid) :  162000 | 2.2511 |  2.2572
Loss (train | valid) :  163000 | 2.2358 |  2.2394
Loss (train | valid) :  164000 | 2.2371 |  2.2445
Loss (train | valid) :  165000 | 2.2402 |  2.2462
Loss (train | valid) :  166000 | 2.2399 |  2.2452
Loss (train | valid) :  167000 | 2.2437 |  2.2484
Loss (train | valid) :  168000 | 2.2403 |  2.2464
Loss (train | valid) :  169000 | 2.2343 |  2.2400
Loss (train | valid) :  170000 | 2.2379 |  2.2410
Loss (train | valid) :  171000 | 2.2361 |  2.2413
Loss (train | valid) :  172000 | 2.2334 |  2.2395
Loss (train | valid) :  173000 | 2.2353 |  2.2387
Loss (train | valid) :  174000 | 2.2358 |  2.2421
Loss (train | valid) :  175000 | 2.2338 |  2.2414
Loss (train | valid) :  176000 | 2.2369 |  2.2421
Loss (train | valid) :  177000 | 2.2384 |  2.2440
Loss (train | valid) :  178000 | 2.2348 |  2.2405
Loss (train | valid) :  179000 | 2.2383 |  2.2448
Loss (train | valid) :  180000 | 2.2323 |  2.2395
Loss (train | valid) :  181000 | 2.2368 |  2.2418
Loss (train | valid) :  182000 | 2.2370 |  2.2439
Loss (train | valid) :  183000 | 2.2349 |  2.2398
Loss (train | valid) :  184000 | 2.2342 |  2.2406
Loss (train | valid) :  185000 | 2.2355 |  2.2426
Loss (train | valid) :  186000 | 2.2328 |  2.2391
Loss (train | valid) :  187000 | 2.2344 |  2.2393
Loss (train | valid) :  188000 | 2.2334 |  2.2388
Loss (train | valid) :  189000 | 2.2309 |  2.2374
Loss (train | valid) :  190000 | 2.2298 |  2.2349
Loss (train | valid) :  191000 | 2.2398 |  2.2459
Loss (train | valid) :  192000 | 2.2286 |  2.2348
Loss (train | valid) :  193000 | 2.2335 |  2.2391
Loss (train | valid) :  194000 | 2.2317 |  2.2356
Loss (train | valid) :  195000 | 2.2404 |  2.2468
Loss (train | valid) :  196000 | 2.2282 |  2.2334
Loss (train | valid) :  197000 | 2.2280 |  2.2369
Loss (train | valid) :  198000 | 2.2299 |  2.2363
Loss (train | valid) :  199000 | 2.2293 |  2.2348
Loss (train | valid) :  200000 | 2.2259 |  2.2304
Loss (train | valid) :  201000 | 2.2286 |  2.2352
Loss (train | valid) :  202000 | 2.2311 |  2.2378
Loss (train | valid) :  203000 | 2.2268 |  2.2338
Loss (train | valid) :  204000 | 2.2305 |  2.2359
Loss (train | valid) :  205000 | 2.2278 |  2.2353
Loss (train | valid) :  206000 | 2.2288 |  2.2348
Loss (train | valid) :  207000 | 2.2343 |  2.2414
Loss (train | valid) :  208000 | 2.2251 |  2.2300
Loss (train | valid) :  209000 | 2.2279 |  2.2339
Loss (train | valid) :  210000 | 2.2322 |  2.2392
Loss (train | valid) :  211000 | 2.2317 |  2.2377
Loss (train | valid) :  212000 | 2.2362 |  2.2419
Loss (train | valid) :  213000 | 2.2298 |  2.2348
Loss (train | valid) :  214000 | 2.2259 |  2.2306
Loss (train | valid) :  215000 | 2.2267 |  2.2326
Loss (train | valid) :  216000 | 2.2265 |  2.2296
Loss (train | valid) :  217000 | 2.2307 |  2.2372
Loss (train | valid) :  218000 | 2.2257 |  2.2302
Loss (train | valid) :  219000 | 2.2277 |  2.2353
Loss (train | valid) :  220000 | 2.2301 |  2.2347
Loss (train | valid) :  221000 | 2.2256 |  2.2326
Loss (train | valid) :  222000 | 2.2298 |  2.2331
Loss (train | valid) :  223000 | 2.2314 |  2.2380
Loss (train | valid) :  224000 | 2.2255 |  2.2307
Loss (train | valid) :  225000 | 2.2231 |  2.2284
Loss (train | valid) :  226000 | 2.2276 |  2.2324
Loss (train | valid) :  227000 | 2.2277 |  2.2353
Loss (train | valid) :  228000 | 2.2307 |  2.2356
Loss (train | valid) :  229000 | 2.2268 |  2.2323
Loss (train | valid) :  230000 | 2.2228 |  2.2271
Loss (train | valid) :  231000 | 2.2271 |  2.2292
Loss (train | valid) :  232000 | 2.2260 |  2.2308
Loss (train | valid) :  233000 | 2.2249 |  2.2312
Loss (train | valid) :  234000 | 2.2275 |  2.2332
Loss (train | valid) :  235000 | 2.2266 |  2.2318
Loss (train | valid) :  236000 | 2.2279 |  2.2319
Loss (train | valid) :  237000 | 2.2241 |  2.2299
Loss (train | valid) :  238000 | 2.2232 |  2.2295
Loss (train | valid) :  239000 | 2.2203 |  2.2233
Loss (train | valid) :  240000 | 2.2224 |  2.2275
Loss (train | valid) :  241000 | 2.2247 |  2.2291
Loss (train | valid) :  242000 | 2.2197 |  2.2240
Loss (train | valid) :  243000 | 2.2216 |  2.2236
Loss (train | valid) :  244000 | 2.2228 |  2.2295
Loss (train | valid) :  245000 | 2.2193 |  2.2239
Loss (train | valid) :  246000 | 2.2182 |  2.2226
Loss (train | valid) :  247000 | 2.2239 |  2.2282
Loss (train | valid) :  248000 | 2.2197 |  2.2251
Loss (train | valid) :  249000 | 2.2233 |  2.2267
Loss (train | valid) :  250000 | 2.2191 |  2.2234
Loss (train | valid) :  251000 | 2.2205 |  2.2262
Loss (train | valid) :  252000 | 2.2250 |  2.2300
Loss (train | valid) :  253000 | 2.2208 |  2.2261
Loss (train | valid) :  254000 | 2.2225 |  2.2273
Loss (train | valid) :  255000 | 2.2245 |  2.2287
Loss (train | valid) :  256000 | 2.2246 |  2.2295
Loss (train | valid) :  257000 | 2.2221 |  2.2269
Loss (train | valid) :  258000 | 2.2218 |  2.2291
Loss (train | valid) :  259000 | 2.2245 |  2.2306
Loss (train | valid) :  260000 | 2.2193 |  2.2225
Loss (train | valid) :  261000 | 2.2214 |  2.2262
Loss (train | valid) :  262000 | 2.2209 |  2.2250
Loss (train | valid) :  263000 | 2.2162 |  2.2223
Loss (train | valid) :  264000 | 2.2192 |  2.2249
Loss (train | valid) :  265000 | 2.2190 |  2.2230
Loss (train | valid) :  266000 | 2.2239 |  2.2278
Loss (train | valid) :  267000 | 2.2213 |  2.2276
Loss (train | valid) :  268000 | 2.2185 |  2.2230
Loss (train | valid) :  269000 | 2.2196 |  2.2233
Loss (train | valid) :  270000 | 2.2179 |  2.2221
Loss (train | valid) :  271000 | 2.2197 |  2.2242
Loss (train | valid) :  272000 | 2.2137 |  2.2174
Loss (train | valid) :  273000 | 2.2192 |  2.2238
Loss (train | valid) :  274000 | 2.2169 |  2.2224
Loss (train | valid) :  275000 | 2.2190 |  2.2221
Loss (train | valid) :  276000 | 2.2309 |  2.2354
Loss (train | valid) :  277000 | 2.2230 |  2.2318
Loss (train | valid) :  278000 | 2.2176 |  2.2217
Loss (train | valid) :  279000 | 2.2174 |  2.2229
Loss (train | valid) :  280000 | 2.2224 |  2.2283
Loss (train | valid) :  281000 | 2.2166 |  2.2221
Loss (train | valid) :  282000 | 2.2196 |  2.2243
Loss (train | valid) :  283000 | 2.2162 |  2.2201
Loss (train | valid) :  284000 | 2.2201 |  2.2240
Loss (train | valid) :  285000 | 2.2143 |  2.2194
Loss (train | valid) :  286000 | 2.2170 |  2.2209
Loss (train | valid) :  287000 | 2.2163 |  2.2251
Loss (train | valid) :  288000 | 2.2161 |  2.2213
Loss (train | valid) :  289000 | 2.2215 |  2.2250
Loss (train | valid) :  290000 | 2.2145 |  2.2180
Loss (train | valid) :  291000 | 2.2211 |  2.2265
Loss (train | valid) :  292000 | 2.2212 |  2.2250
Loss (train | valid) :  293000 | 2.2189 |  2.2250
Loss (train | valid) :  294000 | 2.2136 |  2.2195
Loss (train | valid) :  295000 | 2.2141 |  2.2184
Loss (train | valid) :  296000 | 2.2139 |  2.2188
Loss (train | valid) :  297000 | 2.2157 |  2.2209
Loss (train | valid) :  298000 | 2.2278 |  2.2309
Loss (train | valid) :  299000 | 2.2177 |  2.2225
Loss (train | valid) :  300000 | 2.2135 |  2.2181
Loss (train | valid) :  301000 | 2.2163 |  2.2217
Loss (train | valid) :  302000 | 2.2170 |  2.2215
Loss (train | valid) :  303000 | 2.2137 |  2.2197
Loss (train | valid) :  304000 | 2.2177 |  2.2219
Loss (train | valid) :  305000 | 2.2176 |  2.2212
Loss (train | valid) :  306000 | 2.2181 |  2.2229
Loss (train | valid) :  307000 | 2.2163 |  2.2219
Loss (train | valid) :  308000 | 2.2177 |  2.2239
Loss (train | valid) :  309000 | 2.2134 |  2.2185
Loss (train | valid) :  310000 | 2.2139 |  2.2205
Loss (train | valid) :  311000 | 2.2104 |  2.2138
Loss (train | valid) :  312000 | 2.2132 |  2.2199
Loss (train | valid) :  313000 | 2.2099 |  2.2132
Loss (train | valid) :  314000 | 2.2112 |  2.2167
Loss (train | valid) :  315000 | 2.2116 |  2.2157
Loss (train | valid) :  316000 | 2.2135 |  2.2191
Loss (train | valid) :  317000 | 2.2137 |  2.2158
Loss (train | valid) :  318000 | 2.2130 |  2.2188
Loss (train | valid) :  319000 | 2.2114 |  2.2157
Loss (train | valid) :  320000 | 2.2136 |  2.2164
Loss (train | valid) :  321000 | 2.2146 |  2.2206
Loss (train | valid) :  322000 | 2.2112 |  2.2166
Loss (train | valid) :  323000 | 2.2122 |  2.2184
Loss (train | valid) :  324000 | 2.2150 |  2.2172
Loss (train | valid) :  325000 | 2.2130 |  2.2165
Loss (train | valid) :  326000 | 2.2130 |  2.2166
Loss (train | valid) :  327000 | 2.2147 |  2.2173
Loss (train | valid) :  328000 | 2.2147 |  2.2186
Loss (train | valid) :  329000 | 2.2106 |  2.2149
Loss (train | valid) :  330000 | 2.2174 |  2.2216
Loss (train | valid) :  331000 | 2.2117 |  2.2161
Loss (train | valid) :  332000 | 2.2118 |  2.2153
Loss (train | valid) :  333000 | 2.2109 |  2.2170
Loss (train | valid) :  334000 | 2.2107 |  2.2163
Loss (train | valid) :  335000 | 2.2141 |  2.2206
Loss (train | valid) :  336000 | 2.2164 |  2.2209
Loss (train | valid) :  337000 | 2.2089 |  2.2128
Loss (train | valid) :  338000 | 2.2123 |  2.2180
Loss (train | valid) :  339000 | 2.2095 |  2.2130
Loss (train | valid) :  340000 | 2.2109 |  2.2162
Loss (train | valid) :  341000 | 2.2116 |  2.2169
Loss (train | valid) :  342000 | 2.2127 |  2.2190
Loss (train | valid) :  343000 | 2.2093 |  2.2129
Loss (train | valid) :  344000 | 2.2130 |  2.2182
Loss (train | valid) :  345000 | 2.2126 |  2.2179
Loss (train | valid) :  346000 | 2.2093 |  2.2129
Loss (train | valid) :  347000 | 2.2117 |  2.2183
Loss (train | valid) :  348000 | 2.2088 |  2.2146
Loss (train | valid) :  349000 | 2.2068 |  2.2141
Loss (train | valid) :  350000 | 2.2143 |  2.2191
Loss (train | valid) :  351000 | 2.2090 |  2.2135
Loss (train | valid) :  352000 | 2.2089 |  2.2148
Loss (train | valid) :  353000 | 2.2102 |  2.2162
Loss (train | valid) :  354000 | 2.2095 |  2.2148
Loss (train | valid) :  355000 | 2.2118 |  2.2184
Loss (train | valid) :  356000 | 2.2089 |  2.2147
Loss (train | valid) :  357000 | 2.2104 |  2.2147
Loss (train | valid) :  358000 | 2.2187 |  2.2227
Loss (train | valid) :  359000 | 2.2077 |  2.2137
Loss (train | valid) :  360000 | 2.2215 |  2.2289
Loss (train | valid) :  361000 | 2.2121 |  2.2192
Loss (train | valid) :  362000 | 2.2147 |  2.2174
Loss (train | valid) :  363000 | 2.2079 |  2.2128
Loss (train | valid) :  364000 | 2.2092 |  2.2154
Loss (train | valid) :  365000 | 2.2082 |  2.2129
Loss (train | valid) :  366000 | 2.2090 |  2.2140
Loss (train | valid) :  367000 | 2.2081 |  2.2126
Loss (train | valid) :  368000 | 2.2063 |  2.2137
Loss (train | valid) :  369000 | 2.2087 |  2.2138
Loss (train | valid) :  370000 | 2.2090 |  2.2149
Loss (train | valid) :  371000 | 2.2090 |  2.2157
Loss (train | valid) :  372000 | 2.2082 |  2.2139
Loss (train | valid) :  373000 | 2.2140 |  2.2193
Loss (train | valid) :  374000 | 2.2109 |  2.2135
Loss (train | valid) :  375000 | 2.2067 |  2.2131
Loss (train | valid) :  376000 | 2.2051 |  2.2085
Loss (train | valid) :  377000 | 2.2059 |  2.2108
Loss (train | valid) :  378000 | 2.2047 |  2.2096
Loss (train | valid) :  379000 | 2.2121 |  2.2173
Loss (train | valid) :  380000 | 2.2048 |  2.2099
Loss (train | valid) :  381000 | 2.2030 |  2.2095
Loss (train | valid) :  382000 | 2.2057 |  2.2134
Loss (train | valid) :  383000 | 2.2107 |  2.2185
Loss (train | valid) :  384000 | 2.2089 |  2.2161
Loss (train | valid) :  385000 | 2.2075 |  2.2141
Loss (train | valid) :  386000 | 2.2042 |  2.2102
Loss (train | valid) :  387000 | 2.2037 |  2.2115
Loss (train | valid) :  388000 | 2.2052 |  2.2108
Loss (train | valid) :  389000 | 2.2069 |  2.2121
Loss (train | valid) :  390000 | 2.2076 |  2.2142
Loss (train | valid) :  391000 | 2.2076 |  2.2142
Loss (train | valid) :  392000 | 2.2062 |  2.2126
Loss (train | valid) :  393000 | 2.2087 |  2.2174
Loss (train | valid) :  394000 | 2.2066 |  2.2102
Loss (train | valid) :  395000 | 2.2038 |  2.2079
Loss (train | valid) :  396000 | 2.2081 |  2.2122
Loss (train | valid) :  397000 | 2.2061 |  2.2098
Loss (train | valid) :  398000 | 2.2144 |  2.2185
Loss (train | valid) :  399000 | 2.2089 |  2.2174
Loss (train | valid) :  400000 | 2.2068 |  2.2132
Loss (train | valid) :  401000 | 2.2082 |  2.2134
Loss (train | valid) :  402000 | 2.2065 |  2.2140
Loss (train | valid) :  403000 | 2.2056 |  2.2106
Loss (train | valid) :  404000 | 2.2094 |  2.2144
Loss (train | valid) :  405000 | 2.2027 |  2.2089
Loss (train | valid) :  406000 | 2.2022 |  2.2087
Loss (train | valid) :  407000 | 2.2040 |  2.2119
Loss (train | valid) :  408000 | 2.2077 |  2.2128
Loss (train | valid) :  409000 | 2.2033 |  2.2108
Loss (train | valid) :  410000 | 2.2045 |  2.2110
Loss (train | valid) :  411000 | 2.2058 |  2.2103
Loss (train | valid) :  412000 | 2.2047 |  2.2081
Loss (train | valid) :  413000 | 2.2032 |  2.2085
Loss (train | valid) :  414000 | 2.2055 |  2.2089
Loss (train | valid) :  415000 | 2.2066 |  2.2106
Loss (train | valid) :  416000 | 2.2051 |  2.2106
Loss (train | valid) :  417000 | 2.2051 |  2.2123
Loss (train | valid) :  418000 | 2.2052 |  2.2098
Loss (train | valid) :  419000 | 2.2055 |  2.2110
Loss (train | valid) :  420000 | 2.2064 |  2.2134
Loss (train | valid) :  421000 | 2.2063 |  2.2124
Loss (train | valid) :  422000 | 2.2022 |  2.2050
Loss (train | valid) :  423000 | 2.2073 |  2.2114
Loss (train | valid) :  424000 | 2.2019 |  2.2069
Loss (train | valid) :  425000 | 2.2103 |  2.2165
Loss (train | valid) :  426000 | 2.2013 |  2.2060
Loss (train | valid) :  427000 | 2.2031 |  2.2086
Loss (train | valid) :  428000 | 2.2064 |  2.2125
Loss (train | valid) :  429000 | 2.2033 |  2.2073
Loss (train | valid) :  430000 | 2.2029 |  2.2078
Loss (train | valid) :  431000 | 2.2053 |  2.2123
Loss (train | valid) :  432000 | 2.2048 |  2.2123
Loss (train | valid) :  433000 | 2.2075 |  2.2126
Loss (train | valid) :  434000 | 2.2057 |  2.2087
Loss (train | valid) :  435000 | 2.2044 |  2.2089
Loss (train | valid) :  436000 | 2.2033 |  2.2085
Loss (train | valid) :  437000 | 2.2028 |  2.2091
Loss (train | valid) :  438000 | 2.2061 |  2.2119
Loss (train | valid) :  439000 | 2.2055 |  2.2105
Loss (train | valid) :  440000 | 2.2045 |  2.2083
Loss (train | valid) :  441000 | 2.2044 |  2.2098
Loss (train | valid) :  442000 | 2.2032 |  2.2070
Loss (train | valid) :  443000 | 2.2015 |  2.2094
Loss (train | valid) :  444000 | 2.2042 |  2.2080
Loss (train | valid) :  445000 | 2.2084 |  2.2157
Loss (train | valid) :  446000 | 2.2073 |  2.2137
Loss (train | valid) :  447000 | 2.2039 |  2.2086
Loss (train | valid) :  448000 | 2.2063 |  2.2126
Loss (train | valid) :  449000 | 2.2040 |  2.2107
Loss (train | valid) :  450000 | 2.1983 |  2.2055
Loss (train | valid) :  451000 | 2.2030 |  2.2102
Loss (train | valid) :  452000 | 2.2020 |  2.2091
Loss (train | valid) :  453000 | 2.2027 |  2.2101
Loss (train | valid) :  454000 | 2.2025 |  2.2075
Loss (train | valid) :  455000 | 2.2059 |  2.2114
Loss (train | valid) :  456000 | 2.2046 |  2.2099
Loss (train | valid) :  457000 | 2.2014 |  2.2065
Loss (train | valid) :  458000 | 2.1976 |  2.2042
Loss (train | valid) :  459000 | 2.2010 |  2.2078
Loss (train | valid) :  460000 | 2.2058 |  2.2104
Loss (train | valid) :  461000 | 2.2079 |  2.2121
Loss (train | valid) :  462000 | 2.2043 |  2.2106
Loss (train | valid) :  463000 | 2.2007 |  2.2062
Loss (train | valid) :  464000 | 2.2006 |  2.2061
Loss (train | valid) :  465000 | 2.2034 |  2.2089
Loss (train | valid) :  466000 | 2.2001 |  2.2053
Loss (train | valid) :  467000 | 2.1993 |  2.2045
Loss (train | valid) :  468000 | 2.2002 |  2.2054
Loss (train | valid) :  469000 | 2.2006 |  2.2071
Loss (train | valid) :  470000 | 2.2027 |  2.2098
Loss (train | valid) :  471000 | 2.1999 |  2.2082
Loss (train | valid) :  472000 | 2.2017 |  2.2066
Loss (train | valid) :  473000 | 2.2013 |  2.2076
Loss (train | valid) :  474000 | 2.2041 |  2.2121
Loss (train | valid) :  475000 | 2.2018 |  2.2057
Loss (train | valid) :  476000 | 2.2021 |  2.2076
Loss (train | valid) :  477000 | 2.2036 |  2.2089
Loss (train | valid) :  478000 | 2.1986 |  2.2057
Loss (train | valid) :  479000 | 2.2009 |  2.2096
Loss (train | valid) :  480000 | 2.2019 |  2.2062
Loss (train | valid) :  481000 | 2.2022 |  2.2082
Loss (train | valid) :  482000 | 2.2013 |  2.2074
Loss (train | valid) :  483000 | 2.2061 |  2.2133
Loss (train | valid) :  484000 | 2.2019 |  2.2084
Loss (train | valid) :  485000 | 2.2051 |  2.2088
Loss (train | valid) :  486000 | 2.2006 |  2.2062
Loss (train | valid) :  487000 | 2.2019 |  2.2086
Loss (train | valid) :  488000 | 2.1987 |  2.2025
Loss (train | valid) :  489000 | 2.2026 |  2.2076
Loss (train | valid) :  490000 | 2.2038 |  2.2091
Loss (train | valid) :  491000 | 2.2050 |  2.2074
Loss (train | valid) :  492000 | 2.2003 |  2.2053
Loss (train | valid) :  493000 | 2.1992 |  2.2049
Loss (train | valid) :  494000 | 2.2015 |  2.2083
Loss (train | valid) :  495000 | 2.1976 |  2.2038
Lr decay: 0.015
Loss (train | valid) :  496000 | 2.1878 |  2.1944
Loss (train | valid) :  497000 | 2.1878 |  2.1935
Loss (train | valid) :  498000 | 2.1873 |  2.1935
Loss (train | valid) :  499000 | 2.1874 |  2.1930
Loss (total | test) : 2.1868 | 2.1765
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">[</span><span class="mi">100</span><span class="p">:],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">valid_losses</span><span class="p">[</span><span class="mi">100</span><span class="p">:],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train loss&#39;</span><span class="p">,</span> <span class="s1">&#39;valid loss&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;train / valid loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2_mlp_85_0.png" src="../../_images/2_mlp_85_0.png" />
</div>
</div>
<p>We obtain a total loss of 2.17, which is a significant improvement from the loss of 2.45 of the bigram model!</p>
</section>
<section id="hidden-dim">
<h3>Hidden dim<a class="headerlink" href="#hidden-dim" title="Permalink to this headline">#</a></h3>
<p>During previous training, the training set loss was 2.1874 and the validation set loss was 2.1930. This suggests that overfitting is not occurring, and therefore it may be beneficial to increase the model size (i.e., number of parameters).</p>
<p>To identify the optimal hidden layer size, we will train the network using different configurations and monitor the total and validation loss. It is expected that the network will overfit with high values of hidden dimension.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.15</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">320</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100_000</span>
<span class="n">decay_step</span> <span class="o">=</span> <span class="mi">90_000</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hd</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">]):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="n">hd</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">lrd</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="c1"># Lr decay</span>
        <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;</span> <span class="n">decay_step</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">lrd</span><span class="p">:</span>
            <span class="n">lr</span> <span class="o">/=</span> <span class="mi">10</span>
            <span class="n">lrd</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">get_minibatch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;hdim: </span><span class="si">{</span><span class="n">hd</span><span class="si">}</span><span class="s1">, #paramters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Loss hdim=</span><span class="si">{</span><span class="n">hd</span><span class="si">}</span><span class="s1"> (total | valid) : </span><span class="si">{</span><span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> | </span><span class="si">{</span><span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hdim: 50, #paramters: 1781
Loss hdim=50 (total | valid) : 2.2764 | 2.2813
hdim: 100, #paramters: 3481
Loss hdim=100 (total | valid) : 2.3847 | 2.3878
hdim: 200, #paramters: 6881
Loss hdim=200 (total | valid) : 2.6033 | 2.6118
hdim: 300, #paramters: 10281
Loss hdim=300 (total | valid) : 3.1716 | 3.1790
hdim: 400, #paramters: 13681
Loss hdim=400 (total | valid) : 13.6597 | 13.6473
hdim: 800, #paramters: 27281
Loss hdim=800 (total | valid) : 39.4532 | 39.5919
</pre></div>
</div>
</div>
</div>
<p>Increasing the width of the hidden layer results in worse validation loss and does not improve performance. This may suggest that the true bottleneck is not the hidden layer, but rather the size of the embeddings, or that the larger network requires more training steps to optimize its increased number of parameters.</p>
</section>
<section id="embedding-dim">
<h3>Embedding dim<a class="headerlink" href="#embedding-dim" title="Permalink to this headline">#</a></h3>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.15</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">320</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200_000</span>
<span class="n">decay_step</span> <span class="o">=</span> <span class="mi">190_000</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ed</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="n">ed</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">lrd</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="c1"># Lr decay</span>
        <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;</span> <span class="n">decay_step</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">lrd</span><span class="p">:</span>
            <span class="n">lr</span> <span class="o">/=</span> <span class="mi">10</span>
            <span class="n">lrd</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">get_minibatch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;ed: </span><span class="si">{</span><span class="n">ed</span><span class="si">}</span><span class="s1">, #paramters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Loss (total | valid) : </span><span class="si">{</span><span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> | </span><span class="si">{</span><span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ed: 2, #paramters: 3481
Loss (total | valid) : 2.2176 | 2.2249
ed: 3, #paramters: 3808
Loss (total | valid) : 2.3092 | 2.3099
ed: 4, #paramters: 4135
Loss (total | valid) : 2.4541 | 2.4564
ed: 5, #paramters: 4462
Loss (total | valid) : 3.3572 | 3.3692
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="visialize-embeddings">
<h2>Visialize embeddings<a class="headerlink" href="#visialize-embeddings" title="Permalink to this headline">#</a></h2>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emb_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hid_dim</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">60000</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">25000</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.3</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">ds</span><span class="o">.</span><span class="n">Y</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;# parameters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">Cs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">Cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">C</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">ixs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="p">(</span><span class="n">bs</span><span class="p">,))</span>
    <span class="n">Xb</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">ixs</span><span class="p">]</span>
    <span class="n">Yb</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">ixs</span><span class="p">]</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Y</span><span class="p">[</span><span class="n">ixs</span><span class="p">])</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">&gt;10</span><span class="si">}</span><span class="s1"> | </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># parameters: 1781
         0 | 11.7508
      1000 | 2.4502
      2000 | 2.4229
      3000 | 2.4022
      4000 | 2.3838
      5000 | 2.3740
      6000 | 2.3653
      7000 | 2.3560
      8000 | 2.3659
      9000 | 2.3516
     10000 | 2.3460
     11000 | 2.3334
     12000 | 2.3352
     13000 | 2.3294
     14000 | 2.3296
     15000 | 2.3258
     16000 | 2.3295
     17000 | 2.3153
     18000 | 2.3220
     19000 | 2.3158
     20000 | 2.3105
     21000 | 2.3101
     22000 | 2.3131
     23000 | 2.3080
     24000 | 2.3112
     25000 | 2.3015
     26000 | 2.3076
     27000 | 2.2866
     28000 | 2.3027
     29000 | 2.3081
     30000 | 2.2870
     31000 | 2.2971
     32000 | 2.2932
     33000 | 2.3012
     34000 | 2.2834
     35000 | 2.2952
     36000 | 2.2946
     37000 | 2.2808
     38000 | 2.2843
     39000 | 2.2731
     40000 | 2.2868
     41000 | 2.2803
     42000 | 2.2765
     43000 | 2.2833
     44000 | 2.2643
     45000 | 2.2739
     46000 | 2.2669
     47000 | 2.2761
     48000 | 2.2725
     49000 | 2.2778
     50000 | 2.2811
     51000 | 2.2598
     52000 | 2.2816
     53000 | 2.2694
     54000 | 2.2633
     55000 | 2.2611
     56000 | 2.2669
     57000 | 2.2536
     58000 | 2.2718
     59000 | 2.2793
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;x</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">)]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">emb</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">Cs</span><span class="p">[:</span><span class="mi">1000000</span><span class="p">]):</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">cols</span><span class="p">)</span>
    <span class="n">emb</span><span class="p">[</span><span class="s1">&#39;ep&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
    <span class="n">emb</span><span class="p">[</span><span class="s1">&#39;chr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="s1">&#39;.&#39;</span> <span class="o">+</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_lowercase</span><span class="p">)</span>
    <span class="n">emb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">emb</span><span class="p">[</span><span class="s1">&#39;chr&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="s1">&#39;aeiou&#39;</span><span class="p">)),</span> <span class="s1">&#39;col&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span>
    <span class="n">emb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="o">~</span><span class="n">emb</span><span class="p">[</span><span class="s1">&#39;chr&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="s1">&#39;aeiou&#39;</span><span class="p">)),</span> <span class="s1">&#39;col&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">emb</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x0&#39;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span>
    <span class="n">text</span><span class="o">=</span><span class="s1">&#39;chr&#39;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;col&#39;</span><span class="p">,</span>
    <span class="n">animation_frame</span><span class="o">=</span><span class="s1">&#39;ep&#39;</span><span class="p">,</span>
    <span class="n">range_x</span><span class="o">=</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x0&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">-</span><span class="mf">.2</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x0&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="mf">.2</span><span class="p">],</span>
    <span class="n">range_y</span><span class="o">=</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">-</span><span class="mf">.2</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="mf">.2</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">updatemenus</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">buttons</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;frame&#39;</span><span class="p">][</span><span class="s1">&#39;duration&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># type: ignore</span>
<span class="n">fig</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">updatemenus</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">buttons</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;transition&#39;</span><span class="p">][</span><span class="s1">&#39;duration&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># type: ignore</span>

<span class="n">fig</span><span class="o">.</span><span class="n">update_traces</span><span class="p">(</span><span class="n">marker</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span> <span class="s1">&#39;opacity&#39;</span><span class="p">:</span> <span class="mf">0.35</span><span class="p">})</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">)</span>
<span class="n">fig</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<p>The visualization of the embeddings shows that the representations for vowels are grouped together, with the special character “.” being distinct from the other characters.</p>
</section>
<section id="sampling-names">
<h2>Sampling names<a class="headerlink" href="#sampling-names" title="Permalink to this headline">#</a></h2>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200_000</span>
<span class="n">decay_step</span> <span class="o">=</span> <span class="mi">100_000</span>
<span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">hid_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">lrd</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="c1"># Lr decay</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">decay_step</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">lrd</span><span class="p">:</span>
        <span class="n">lr</span> <span class="o">/=</span> <span class="mi">10</span>
        <span class="n">lrd</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">get_minibatch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>
    <span class="n">batch_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">)</span>
        <span class="n">valid_loss</span> <span class="o">=</span> <span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;lr: </span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1"> | Loss (train | valid) : </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">&gt;7</span><span class="si">}</span><span class="s1"> | </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> |  </span><span class="si">{</span><span class="n">valid_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Loss (total | test) : </span><span class="si">{</span><span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1"> | </span><span class="si">{</span><span class="n">calc_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>lr: 0.1 | Loss (train | valid) :       0 | 16.4574 |  16.3855
lr: 0.1 | Loss (train | valid) :   10000 | 2.4252 |  2.4359
lr: 0.1 | Loss (train | valid) :   20000 | 2.3617 |  2.3645
lr: 0.1 | Loss (train | valid) :   30000 | 2.3248 |  2.3297
lr: 0.1 | Loss (train | valid) :   40000 | 2.3166 |  2.3252
lr: 0.1 | Loss (train | valid) :   50000 | 2.2781 |  2.2795
lr: 0.1 | Loss (train | valid) :   60000 | 2.2642 |  2.2660
lr: 0.1 | Loss (train | valid) :   70000 | 2.2945 |  2.2968
lr: 0.1 | Loss (train | valid) :   80000 | 2.2417 |  2.2405
lr: 0.1 | Loss (train | valid) :   90000 | 2.2510 |  2.2537
lr: 0.1 | Loss (train | valid) :  100000 | 2.2317 |  2.2333
lr: 0.01 | Loss (train | valid) :  110000 | 2.1682 |  2.1722
lr: 0.01 | Loss (train | valid) :  120000 | 2.1683 |  2.1723
lr: 0.01 | Loss (train | valid) :  130000 | 2.1659 |  2.1715
lr: 0.01 | Loss (train | valid) :  140000 | 2.1640 |  2.1689
lr: 0.01 | Loss (train | valid) :  150000 | 2.1648 |  2.1697
lr: 0.01 | Loss (train | valid) :  160000 | 2.1634 |  2.1681
lr: 0.01 | Loss (train | valid) :  170000 | 2.1612 |  2.1652
lr: 0.01 | Loss (train | valid) :  180000 | 2.1635 |  2.1682
lr: 0.01 | Loss (train | valid) :  190000 | 2.1611 |  2.1664
Loss (total | test) : 2.1627 | 2.1569
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span> <span class="c1"># &#39;...&#39;</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">context</span><span class="p">))</span>  <span class="c1"># predict log-counts</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
        <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix2ch</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>  <span class="c1"># type: ignore</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dex.
marialynn.
roczitah.
mellistyna.
nolla.
kama.
da.
samiyah.
javer.
gotzi.
moriella.
ovie.
teda.
kaley.
maside.
enkaviyn.
rosba.
millie.
bron.
lasu.
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/build_language_model"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="1_bigrams.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Bigram Language Model</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3_activations_gradients_batchnorm.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Activations, Gradients and Batch Normalisation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Marc Dumon<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>