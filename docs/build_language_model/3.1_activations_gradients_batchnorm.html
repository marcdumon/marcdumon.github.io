

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Activations, Gradients and Batch Normalisation - Part 1 &#8212; Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MSZJYDCNQ4"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MSZJYDCNQ4');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/build_language_model/3.1_activations_gradients_batchnorm';</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Activations, Gradients and Batch Normalisation - Part 2" href="3.2_activations_gradients_batchnorm.html" />
    <link rel="prev" title="Multilayer Perceptron" href="2_mlp.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">BUILD A LANGUAGE MODEL</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_intro.html">Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_bigrams.html">Bigram Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_mlp.html">Multilayer Perceptron</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Activations, Gradients and Batch Normalisation - Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="3.2_activations_gradients_batchnorm.html">Activations, Gradients and Batch Normalisation - Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_backprop.html">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_wavenet.html">Wavenet</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">MY FILES</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../my_files/jupyter_book_notes.html">Jupyter Book Notes</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/marcdumon/marcdumon.github.io/blob/master/docs/docs/build_language_model/3.1_activations_gradients_batchnorm.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/marcdumon/marcdumon.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/marcdumon/marcdumon.github.io/issues/new?title=Issue%20on%20page%20%2Fdocs/build_language_model/3.1_activations_gradients_batchnorm.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/docs/build_language_model/3.1_activations_gradients_batchnorm.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Activations, Gradients and Batch Normalisation - Part 1</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducing-results-from-previous-notebook">Reproducing results from previous notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#initialisation">Initialisation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fixing-the-logits">Fixing the logits</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#initial-model">Initial model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#improved-initialisation">Improved initialisation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fixing-the-saturated-tanh">Fixing the saturated tanh</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#improved-model">Improved model</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kaiming-initialisation">Kaiming initialisation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#apply-kaiming-and-train-the-network">Apply Kaiming and train the network</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalisation">Batch Normalisation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-batch-normalisation">Pytorch Batch Normalisation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Literal</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>

<span class="n">torch</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</details>
</div>
<hr>
<section class="tex2jax_ignore mathjax_ignore" id="activations-gradients-and-batch-normalisation-part-1">
<h1>Activations, Gradients and Batch Normalisation - Part 1<a class="headerlink" href="#activations-gradients-and-batch-normalisation-part-1" title="Permalink to this heading">#</a></h1>
<hr><p>In this notebook, we will delve into the inner workings of multi-layer perceptrons (MLPs) and examine the statistics of forward pass activations and backward pass gradients. We will also discuss potential issues that can arise from improper scaling and explore diagnostic tools and visualizations for understanding the performance of a deep network. Additionally, we will explore why training deep neural networks can be challenging and introduce one of the key innovations that made this process easier: Batch Normalization.</p>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this heading">#</a></h2>
<p><strong>Load the data</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;./data/names.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">names</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total # names: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">)</span><span class="si">:</span><span class="s2">.&gt;25,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total # characters in all names: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">names</span><span class="p">))</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">names</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total # names: ...................32,033
Total # characters in all names: 196,113
[&#39;emma&#39;, &#39;olivia&#39;, &#39;ava&#39;, &#39;isabella&#39;, &#39;sophia&#39;]
</pre></div>
</div>
</div>
</div>
<p><strong>Encoding and decoding characters</strong><br />
We define an encoder <code class="docutils literal notranslate"><span class="pre">ch2ix</span></code> and decoder <code class="docutils literal notranslate"><span class="pre">ix2ch</span></code> function that maps a character with a numerical representation (i.e. a unique integer) and vice-versa.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chars</span> <span class="o">=</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_lowercase</span>
<span class="n">ch2ix</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="n">ix2ch</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ch2ix</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoder ch2ix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ch2ix</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decoder ix2ch:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ix2ch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;E.g. emma:&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">ch2ix</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="s2">&quot;.emma.&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encoder ch2ix:
 {&#39;.&#39;: 0, &#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 4, &#39;e&#39;: 5, &#39;f&#39;: 6, &#39;g&#39;: 7, &#39;h&#39;: 8, &#39;i&#39;: 9, &#39;j&#39;: 10, &#39;k&#39;: 11, &#39;l&#39;: 12, &#39;m&#39;: 13, &#39;n&#39;: 14, &#39;o&#39;: 15, &#39;p&#39;: 16, &#39;q&#39;: 17, &#39;r&#39;: 18, &#39;s&#39;: 19, &#39;t&#39;: 20, &#39;u&#39;: 21, &#39;v&#39;: 22, &#39;w&#39;: 23, &#39;x&#39;: 24, &#39;y&#39;: 25, &#39;z&#39;: 26}
Decoder ix2ch:
 {0: &#39;.&#39;, 1: &#39;a&#39;, 2: &#39;b&#39;, 3: &#39;c&#39;, 4: &#39;d&#39;, 5: &#39;e&#39;, 6: &#39;f&#39;, 7: &#39;g&#39;, 8: &#39;h&#39;, 9: &#39;i&#39;, 10: &#39;j&#39;, 11: &#39;k&#39;, 12: &#39;l&#39;, 13: &#39;m&#39;, 14: &#39;n&#39;, 15: &#39;o&#39;, 16: &#39;p&#39;, 17: &#39;q&#39;, 18: &#39;r&#39;, 19: &#39;s&#39;, 20: &#39;t&#39;, 21: &#39;u&#39;, 22: &#39;v&#39;, 23: &#39;w&#39;, 24: &#39;x&#39;, 25: &#39;y&#39;, 26: &#39;z&#39;}
E.g. emma: [0, 5, 13, 13, 1, 0]
</pre></div>
</div>
</div>
</div>
<p><strong>Build the datasets</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
        <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">w</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">:</span>
            <span class="n">ix</span> <span class="o">=</span> <span class="n">ch2ix</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span>
            <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
            <span class="n">Y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>  <span class="c1"># crop and append</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape (X, Y): </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="n">names</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="n">n1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
<span class="n">n2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
<span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span> <span class="o">=</span> <span class="n">make_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="n">n1</span><span class="p">])</span>
<span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span> <span class="o">=</span> <span class="n">make_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">])</span>
<span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span> <span class="o">=</span> <span class="n">make_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">n2</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape (X, Y): torch.Size([182625, 3]), torch.Size([182625])
Shape (X, Y): torch.Size([22655, 3]), torch.Size([22655])
Shape (X, Y): torch.Size([22866, 3]), torch.Size([22866])
</pre></div>
</div>
</div>
</div>
<p><strong>MLP</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">27</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">emb_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">hid_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> <span class="k">if</span> <span class="n">seed</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_dim</span> <span class="o">=</span> <span class="n">emb_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hid_dim</span> <span class="o">=</span> <span class="n">hid_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">block_size</span> <span class="o">*</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_parameters</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Does a forward pass and returns the logits&quot;&quot;&quot;</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">[</span><span class="n">xs</span><span class="p">]</span>  <span class="c1"># (bs, 3, 10) Embed characters into vectors</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_dim</span>
        <span class="p">)</span>  <span class="c1"># (bs, 30) Concatenate the vectors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span>  <span class="c1"># Hidden layer pre-activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span><span class="p">)</span>  <span class="c1"># Hidden layer</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span>  <span class="c1"># Output layer</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets the gradients to None&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">optimise_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Updat the parameters&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="reproducing-results-from-previous-notebook">
<h2>Reproducing results from previous notebook<a class="headerlink" href="#reproducing-results-from-previous-notebook" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ix2ch</span><span class="p">)</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">emb_dim</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># The dimentionality of the character embedding vectors</span>
<span class="n">hid_dim</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># the number of neurons in the hidden layer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span>
    <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Nr of parameters: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">n_parameters</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nr of parameters: 11897
</pre></div>
</div>
</div>
</div>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_steps</span> <span class="o">=</span> <span class="mi">200001</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="c1"># Minibatch construct</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtrn</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtrn</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytrn</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>

    <span class="c1"># Forward pass</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>

    <span class="c1"># Backward pass</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Update</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100000</span> <span class="k">else</span> <span class="mf">0.01</span>  <span class="c1"># Step learning rate decay</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># Tack stats</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/200001: 27.1911
  10000/200001: 2.1946
  20000/200001: 2.4400
  30000/200001: 2.4734
  40000/200001: 2.1567
  50000/200001: 2.4929
  60000/200001: 2.1454
  70000/200001: 2.2762
  80000/200001: 2.7076
  90000/200001: 1.9478
 100000/200001: 2.4221
 110000/200001: 2.1470
 120000/200001: 2.1992
 130000/200001: 1.8029
 140000/200001: 2.2573
 150000/200001: 2.3827
 160000/200001: 2.0819
 170000/200001: 1.6986
 180000/200001: 2.5956
 190000/200001: 2.2642
 200000/200001: 2.0490
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># Decorator to disable gradient tracking</span>
<span class="k">def</span> <span class="nf">split_loss</span><span class="p">(</span><span class="n">split</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">),</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">),</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span><span class="p">)}</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">split</span><span class="p">]</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (N, vocab_size)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.1243
valid loss: 2.1680
</pre></div>
</div>
</div>
</div>
<p><strong>Sample from the model</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>  <span class="c1"># Initialize as [&#39;.&#39;, &#39;.&#39;, &#39;.&#39;]</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="c1"># Forward pass</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">context</span><span class="p">))</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Sample from the distribution</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="c1"># Shift the context window and track the samples</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>
        <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
        <span class="c1"># Break when we sample &#39;.&#39;</span>
        <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ix2ch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dex.
mariah.
makilah.
tyha.
malissana.
nella.
kaman.
arre.
zyau.
jaxsiniotas.
moza.
danvo.
kentedo.
kameria.
sadeu.
niaviyah.
foluw.
huna.
gavtah.
ash.
</pre></div>
</div>
</div>
</div>
<p>The names sampled from the MLP model are of higher-quality compared to the bigrams model, but there is still room for improvement.</p>
</section>
<section id="initialisation">
<h2>Initialisation<a class="headerlink" href="#initialisation" title="Permalink to this heading">#</a></h2>
<p>In this section, we will examine the impact of weight initialization on training.</p>
<p>The starting loss (0/200001: 27.1911) is quite high but decreases quickly. This indicates that the initialisation is sub-optimal. Typically, we have an approximate understanding of what the loss should be at the start based on the architecture and the loss function.</p>
<p>We can calculate the expected loss at initialization using the following method: We have 27 characters, and there is no indication that any one of these 27 characters is more likely than the others. So we expect that the probability distribution that comes out initially is the uniform distribution, assigning equal probability to all the 27 characters and therefor We want the probability for any character to be <span class="math notranslate nohighlight">\(\frac{1}{27}\)</span>.<br />
The expected loss is therefore <span class="math notranslate nohighlight">\(-\log \frac{1}{27} = \log 27 \approx 3.2\)</span>.</p>
<p>This is much lower than the 27.1911 loss indicating that the initialisation is off. Some characters have very high probabilities and other ones have very low probabilities. The network is very confidently wrong leading to very high loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 4-dimensional example of this issue</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="p">)</span>  <span class="c1"># Uniform distribution; all labels equaly likely</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>  <span class="c1"># Correct label is 2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>  <span class="c1"># Loss is what we would expect</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>  <span class="c1"># Label is 2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>  <span class="c1"># Loss is very low because the probability of correct label is high</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>  <span class="c1"># Label is 2</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">loss</span>
<span class="p">)</span>  <span class="c1"># Loss is high because the probability of wrong label is high</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>  <span class="c1"># Normally distributed logits with mu=0 and sigma=10</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>  <span class="c1"># Label is 2</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">loss</span>
<span class="p">)</span>  <span class="c1"># Loss is very high because the probability of wrong label is high</span>

<span class="c1"># Even higher variance in logits:</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>  <span class="c1"># Normally distributed logits with mu=0 and sigma=100</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>  <span class="c1"># Label is 2</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">loss</span>
<span class="p">)</span>  <span class="c1"># Loss is very high because the probability of wrong label is high</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 0., 0., 0.]) tensor([0.2500, 0.2500, 0.2500, 0.2500]) tensor(1.3863)
tensor([0.0066, 0.0066, 0.9802, 0.0066]) tensor(0.0200)
tensor([0., 5., 0., 0.]) tensor([0.0066, 0.9802, 0.0066, 0.0066]) tensor(5.0200)
tensor([-5.3989, -0.2382,  4.4352,  9.1075]) tensor([4.9645e-07, 8.6522e-05, 9.2632e-03, 9.9065e-01]) tensor(4.6817)
tensor([ 44.1407,  65.8503, -56.8471,  70.6941]) tensor([2.9148e-12, 7.8156e-03, 0.0000e+00, 9.9218e-01]) tensor(inf)
</pre></div>
</div>
</div>
</div>
<p><strong>Conclusion:</strong>
The 4-dimensional example shows us that the higher the variants of the logits, the higher the loss. We want the initial logits to be roughly uniformly distributed. By symmetry we dont want these logits to be any positive or negative number but we just want them to be all 0s.</p>
<p>Lets now modify our model to have better initialisation and check the results.</p>
<section id="fixing-the-logits">
<h3>Fixing the logits<a class="headerlink" href="#fixing-the-logits" title="Permalink to this heading">#</a></h3>
<section id="initial-model">
<h4>Initial model<a class="headerlink" href="#initial-model" title="Permalink to this heading">#</a></h4>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span>
    <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">200001</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">W1_std</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">W2_std</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="c1"># Minibatch construct</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtrn</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtrn</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytrn</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>

    <span class="c1"># Forward pass</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>

    <span class="c1"># Backward pass</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># retain_graph = True because we changed the</span>

    <span class="c1"># Update</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100000</span> <span class="k">else</span> <span class="mf">0.01</span>  <span class="c1"># Step learning rate decay</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># Tack stats</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">W1_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">W2_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/200001: 27.1911
  10000/200001: 2.1946
  20000/200001: 2.4400
  30000/200001: 2.4734
  40000/200001: 2.1567
  50000/200001: 2.4929
  60000/200001: 2.1454
  70000/200001: 2.2762
  80000/200001: 2.7076
  90000/200001: 1.9478
 100000/200001: 2.4221
 110000/200001: 2.1470
 120000/200001: 2.1992
 130000/200001: 1.8029
 140000/200001: 2.2573
 150000/200001: 2.3827
 160000/200001: 2.0819
 170000/200001: 1.6986
 180000/200001: 2.5956
 190000/200001: 2.2642
 200000/200001: 2.0490
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">500</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">))])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;log-loss&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$log_</span><span class="si">{10}</span><span class="s2">(loss)$&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">W1_std</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;W1&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sigma$&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">W2_std</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;W2&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sigma$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9ff39d630754e2eb5c31441eec262adcd1e297fde60e41d88054d943c8238cb5.png" src="../../_images/9ff39d630754e2eb5c31441eec262adcd1e297fde60e41d88054d943c8238cb5.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># Decorator to disable gradient tracking</span>
<span class="k">def</span> <span class="nf">split_loss</span><span class="p">(</span><span class="n">split</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">),</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">),</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span><span class="p">)}</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">split</span><span class="p">]</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (N, vocab_size)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.1243
valid loss: 2.1680
</pre></div>
</div>
</div>
</div>
<p><strong>Single forward pass</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span>
    <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Minibatch construct</span>
<span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtrn</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtrn</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytrn</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>

<span class="c1"># Forward pass</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial loss:   </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial logits: </span><span class="si">{</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\u03BC</span><span class="s2">: </span><span class="si">{</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">, </span><span class="se">\u03C3</span><span class="s2">: </span><span class="si">{</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initial loss:   26.5520
Initial logits: [12.411925315856934, -7.6996283531188965, 2.3776426315307617, 6.556832790374756, -6.789996147155762, -15.600006103515625, -21.210203170776367, -0.6893330216407776, 13.282832145690918, -12.542020797729492, -4.280671119689941, 25.916994094848633, 1.7244635820388794, -19.724594116210938, 2.658778190612793, 7.776032447814941, -15.661372184753418, 14.814678192138672, 16.635112762451172, -9.397875785827637, -6.041196823120117, -2.7173681259155273, -1.9347691535949707, -4.294508457183838, -9.465439796447754, -5.1644392013549805, 0.7409148216247559]
: -1.4191560745239258, : 11.508225440979004
</pre></div>
</div>
</div>
</div>
<p>The logits have quite extreme values and that create the fake confidence and high initial loss. These logits need to be much closer to 0.</p>
<p>How can we do that?</p>
<p><code class="docutils literal notranslate"><span class="pre">logits</span> <span class="pre">=</span> <span class="pre">h</span> <span class="pre">&#64;</span> <span class="pre">W2</span> <span class="pre">+</span> <span class="pre">b2</span></code></p>
<ul class="simple">
<li><p>The bias <code class="docutils literal notranslate"><span class="pre">b2</span></code> is initialized with random values with mean 0 and std 1. But because we want the logits to be roughly 0s we shouldnt add a bias of random values.</p></li>
<li><p>The weight <code class="docutils literal notranslate"><span class="pre">W2</span></code> is also initialized with random values. If we want the logits to be very small we also need to make the weights small.</p></li>
</ul>
<p>If we would set the weight <code class="docutils literal notranslate"><span class="pre">W2</span></code> to 0s then the logits would be 0s and the loss would be exactly equal to the ideal loss of 0.29. However its not a good idea to do that. The reason why its better to have a bit of entropy will be explained later in this notebook.</p>
</section>
<section id="improved-initialisation">
<h4>Improved initialisation<a class="headerlink" href="#improved-initialisation" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span>
    <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Don&#39;t add b2</span>
<span class="n">model</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">b2</span> <span class="o">*</span> <span class="mi">0</span>
<span class="c1"># Make W2 very small</span>
<span class="n">model</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">W2</span> <span class="o">*</span> <span class="mf">0.01</span>

<span class="c1"># Minibatch construct</span>
<span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtrn</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtrn</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytrn</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>

<span class="c1"># Forward pass</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial loss:   </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial logits: </span><span class="si">{</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\u03BC</span><span class="s2">: </span><span class="si">{</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">, </span><span class="se">\u03C3</span><span class="s2">: </span><span class="si">{</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initial loss:   3.2939
Initial logits: [-0.03930702432990074, -0.10857903212308884, 0.08649525046348572, -0.06766287982463837, -0.0863657295703888, -0.015910087153315544, -0.09526076167821884, -0.026780564337968826, -0.06762383878231049, 0.12035798281431198, 0.019621366634964943, 0.15579168498516083, 0.04114056006073952, 0.05029834806919098, -0.08983135223388672, 0.08724547177553177, -0.22866690158843994, -0.12500609457492828, 0.10545863211154938, 0.21103426814079285, -0.08953241258859634, -0.026477359235286713, -0.19447888433933258, 0.2250606268644333, 0.04665874317288399, 0.1288319081068039, -0.06684305518865585]
: -0.0018641165224835277, : 0.11609502881765366
</pre></div>
</div>
</div>
</div>
<p>The logits are now much closer to 0 and the initial loss of 3.32 is very close to the ideal loss of 3.29.</p>
<p>Lets now see the effect of better initialisation.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Improved initialisation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span>
    <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">b2</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="mi">0</span>
<span class="n">model</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="mf">0.01</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">200001</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">W1_std</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">W2_std</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="c1"># Minibatch construct</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtrn</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtrn</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytrn</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>

    <span class="c1"># Forward pass</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>

    <span class="c1"># Backward pass</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># retain_graph = True because we changed the</span>

    <span class="c1"># Update</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100000</span> <span class="k">else</span> <span class="mf">0.01</span>  <span class="c1"># Step learning rate decay</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># Tack stats</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">W1_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">W2_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/200001: 3.3179
  10000/200001: 2.1848
  20000/200001: 2.3320
  30000/200001: 2.4731
  40000/200001: 1.9858
  50000/200001: 2.4827
  60000/200001: 2.1361
  70000/200001: 2.1642
  80000/200001: 2.3967
  90000/200001: 2.0793
 100000/200001: 2.4436
 110000/200001: 2.1291
 120000/200001: 2.2657
 130000/200001: 1.7314
 140000/200001: 2.1808
 150000/200001: 2.2475
 160000/200001: 2.0536
 170000/200001: 1.6976
 180000/200001: 2.4663
 190000/200001: 2.2927
 200000/200001: 1.9672
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">500</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">))])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;log-loss&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$log_</span><span class="si">{10}</span><span class="s2">(loss)$&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">W1_std</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;W1&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sigma$&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">W2_std</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;W2&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sigma$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/76ebb63bc676e44a242bc3c054ee1b97faec835ba5474762f6316362f2d0e70f.png" src="../../_images/76ebb63bc676e44a242bc3c054ee1b97faec835ba5474762f6316362f2d0e70f.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># Decorator to disable gradient tracking</span>
<span class="k">def</span> <span class="nf">split_loss</span><span class="p">(</span><span class="n">split</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">),</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">),</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span><span class="p">)}</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">split</span><span class="p">]</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (N, vocab_size)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.0686
valid loss: 2.1294
</pre></div>
</div>
</div>
</div>
<p>The plot of the loss doesnt have the hockey shape appearance because we start at a much lower loss. In the initial model the first few thousand iterations are used to bring the loss down closer to the ideal loss by shrinking the weights. With the optimised initialisation we dont spend time on this easy part but immediately start training the network.</p>
<p>The training loss also slightly improved from 2.12 to 2.07 and the valid loss from 2.17 to 2.13. The reason for this is that we spend more time optimizing the network instead of spending the first several thousand iterations shrinking the weights.</p>
</section>
</section>
<section id="fixing-the-saturated-tanh">
<h3>Fixing the saturated tanh<a class="headerlink" href="#fixing-the-saturated-tanh" title="Permalink to this heading">#</a></h3>
<p>Despite the promising start with an initial loss of 3.32, which is close to the ideal initial loss, there is an underlying issue within the network related to its initialization.</p>
<p>The logits appear to be functioning properly, however, the problem now lies with the values of h, the activations of the hidden states.</p>
<p><strong>Visualisation of the tensor h</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span>
    <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">h</span>
<span class="n">h</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-1.0000,  0.9604, -0.1418,  ..., -0.1266,  1.0000,  1.0000],
        [-1.0000, -0.9978, -0.9997,  ...,  0.7127,  0.9988, -0.1015],
        [-1.0000, -0.9942, -1.0000,  ..., -1.0000, -0.9995,  1.0000],
        ...,
        [-0.9716, -0.9999, -0.9919,  ..., -0.9976, -0.9999, -0.3318],
        [ 0.9923, -0.9907, -1.0000,  ..., -0.9999, -0.9476,  0.9999],
        [-0.9716, -0.9999, -0.9919,  ..., -0.9976, -0.9999, -0.3318]], grad_fn=&lt;TanhBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Many elements of <code class="docutils literal notranslate"><span class="pre">h</span></code> are 1 or -1.</p>
<p><code class="docutils literal notranslate"><span class="pre">h</span> <span class="pre">=</span> <span class="pre">tanh(emb</span> <span class="pre">&#64;</span> <span class="pre">W1</span> <span class="pre">+</span> <span class="pre">b1)</span></code></p>
<p>The activation function tanh, short for hyperbolic tangent, is a type of squashing function that maps real numbers to a range of [-1,1]. It transforms large negative numbers to values near -1 and large positive numbers to values near 1.</p>
<p>The majority of values produced by the tanh activation function tend to be either -1 or 1. This can be seen by examining the distribution of <code class="docutils literal notranslate"><span class="pre">h_preact</span></code>, where the numbers range from -20 to 20. The tanh activation squashes these large numbers to -1 and 1, thus resulting in the majority of values being either -1 or 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">hist</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>  <span class="c1"># (bs, hid_dim) -&gt; (bs*hid_dim)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">hist</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Histogram h&quot;</span><span class="p">)</span>

<span class="n">h_preact</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">h_preact</span>
<span class="n">hist</span> <span class="o">=</span> <span class="n">h_preact</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>  <span class="c1"># (bs, hid_dim) -&gt; (bs*hid_dim)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">hist</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Histogram h_preact&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/3a682685d8f55f3cef56274c5d9b27d4961b91812968d9b380227cb54706f5e3.png" src="../../_images/3a682685d8f55f3cef56274c5d9b27d4961b91812968d9b380227cb54706f5e3.png" />
</div>
</div>
<p>Why is this a problem?<br />
During the backward pass, we begin with the loss and move backwards through the network. This includes backpropagating through the tanh function. The hidden layer of 200 neurons that use the elementwise tanh function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The derivative of tanh is <span class="math notranslate nohighlight">\(\Large\frac{tanh(x)}{d(x)}\large=1-tanh^2\)</span>.<br />
The chain rule is <span class="math notranslate nohighlight">\(\Large\frac{dy}{dx}=\frac{dy}{du} \cdot \frac{du}{dx}\)</span> where <span class="math notranslate nohighlight">\(y = f(g(x))\)</span> and <span class="math notranslate nohighlight">\(\frac{dy}{du} = f'(u) and \frac{du}{dx} = g'(x) \)</span>.</p>
</div>
<p>When the output of the tanh function is close to -1 or 1, the derivative of tanh is 0. This is due to the fact that we are in the flat region of the tanh function. As a result, small changes in the weights of the neuron do not affect the loss. This in turn stops the backpropagation process because we multiply by 0 in the chain rule. The neuron</p>
<p>When the output of the tanh function is close to 0, the derivative is close to 1. The gradient just passes through because we multiply by 1 in the chain rule.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>

<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>

<span class="n">ys</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\frac{tanh(x)}</span><span class="si">{dx}</span><span class="s2"> = 1-tanh(x)^2$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e2d9918d4c3f177e341f8c785bc7ba982397b64a52e2646075f4201ad8a78d49.png" src="../../_images/e2d9918d4c3f177e341f8c785bc7ba982397b64a52e2646075f4201ad8a78d49.png" />
</div>
</div>
<p><strong>Visualisation of neurons in flat region of tanh</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.99</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;abs(h) &gt; 0.99&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;neuron&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;sample&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/3060ba476d18f26b1343efb7793cf9a1d97066892e66cd27cb16b59fd5e781eb.png" src="../../_images/3060ba476d18f26b1343efb7793cf9a1d97066892e66cd27cb16b59fd5e781eb.png" />
</div>
</div>
<p>The above image shows that there are a lot of neurons that are in the flat tails of tanh (white). The gradients during the backward pass for these neurons would be severely reduced. If an entire column is white (tanh &gt; 0.99) for all samples, this represents a death neuron, where no single sample ever activates the tanh in the active part of tanh. As a result, the neuron will not be able to learn during training.</p>
<p>Death neurons can happen if the weights of the network are initialized with very large values. It can also happen if the learning rate is set too high, which can cause the weights to become large during training.</p>
<section id="improved-model">
<h4>Improved model<a class="headerlink" href="#improved-model" title="Permalink to this heading">#</a></h4>
<p>We want less outputs of tanh to be -1 or 1. In other words, we want the pre-activation <code class="docutils literal notranslate"><span class="pre">h_preact</span></code> to be closer to 0.</p>
<p>How can we do that?</p>
<p><code class="docutils literal notranslate"><span class="pre">h</span> <span class="pre">=</span> <span class="pre">tanh(emb</span> <span class="pre">&#64;</span> <span class="pre">W1</span> <span class="pre">+</span> <span class="pre">b1)</span></code></p>
<ul class="simple">
<li><p>We could set the bias <code class="docutils literal notranslate"><span class="pre">b1</span></code> to 0, but its often better to have a little entropy, so well set it to <code class="docutils literal notranslate"><span class="pre">b1</span> <span class="pre">*</span> <span class="pre">0.01</span></code>.</p></li>
<li><p>We set <code class="docutils literal notranslate"><span class="pre">W1</span> <span class="pre">=</span> <span class="pre">W1</span> <span class="pre">*</span> <span class="pre">0.2</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span>
    <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Fixing the logits</span>
<span class="n">model</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">b2</span> <span class="o">*</span> <span class="mi">0</span>
<span class="n">model</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">W2</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="c1"># Fixing the tanh</span>
<span class="n">model</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">b1</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">model</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">W1</span> <span class="o">*</span> <span class="mf">0.2</span>

<span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">h</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">hist</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>  <span class="c1"># (bs, hid_dim) -&gt; (bs*hid_dim)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">hist</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Histogram h&quot;</span><span class="p">)</span>

<span class="n">h_preact</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">h_preact</span>
<span class="n">hist</span> <span class="o">=</span> <span class="n">h_preact</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>  <span class="c1"># (bs, hid_dim) -&gt; (bs*hid_dim)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">hist</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Histogram h_preact&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/464153f8419b2730f77986eb9637d2ba85758a8b00e897eb25d61750003e10ed.png" src="../../_images/464153f8419b2730f77986eb9637d2ba85758a8b00e897eb25d61750003e10ed.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.99</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;abs(h) &gt; 0.99&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;neuron&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;sample&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/67ed2b58b84a5bea50e9624087072e8da3c2db27be7d8a579d6df85d5b5f042d.png" src="../../_images/67ed2b58b84a5bea50e9624087072e8da3c2db27be7d8a579d6df85d5b5f042d.png" />
</div>
</div>
<p>The histogram show that <code class="docutils literal notranslate"><span class="pre">h_preact</span></code> is much closer to 0 and <code class="docutils literal notranslate"><span class="pre">h</span></code> has far less -1s and 1s.</p>
<p>Lets now see the effect of better initialisation.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Improved initialisation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span>
    <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Fixing the logits</span>
<span class="n">model</span><span class="o">.</span><span class="n">b2</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="mi">0</span>
<span class="n">model</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="mf">0.01</span>
<span class="c1"># Fixing the tanh</span>
<span class="n">model</span><span class="o">.</span><span class="n">b1</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="mf">0.01</span>
<span class="n">model</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="mf">0.2</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">200001</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">W1_std</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">W2_std</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="c1"># Minibatch construct</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtrn</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtrn</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytrn</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>

    <span class="c1"># Forward pass</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>

    <span class="c1"># Backward pass</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># retain_graph = True because we changed the</span>

    <span class="c1"># Update</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100000</span> <span class="k">else</span> <span class="mf">0.01</span>  <span class="c1"># Step learning rate decay</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># Tack stats</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">W1_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">W2_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/200001: 3.3070
  10000/200001: 2.0390
  20000/200001: 2.3337
  30000/200001: 2.1899
  40000/200001: 2.0281
  50000/200001: 2.3006
  60000/200001: 1.9184
  70000/200001: 2.0165
  80000/200001: 2.1462
  90000/200001: 2.0186
 100000/200001: 2.1868
 110000/200001: 2.0560
 120000/200001: 2.0876
 130000/200001: 1.6509
 140000/200001: 2.2062
 150000/200001: 2.2495
 160000/200001: 2.0697
 170000/200001: 1.6954
 180000/200001: 2.5340
 190000/200001: 2.2555
 200000/200001: 1.8769
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">500</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">))])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;log-loss&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$log_</span><span class="si">{10}</span><span class="s2">(loss)$&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">W1_std</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;W1&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sigma$&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">W2_std</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;W2&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sigma$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/aa1e4c065bcd767041d10ce51f47d0bbafd8e8353c54b09be81391762010eb50.png" src="../../_images/aa1e4c065bcd767041d10ce51f47d0bbafd8e8353c54b09be81391762010eb50.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># Decorator to disable gradient tracking</span>
<span class="k">def</span> <span class="nf">split_loss</span><span class="p">(</span><span class="n">split</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">),</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">),</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span><span class="p">)}</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">split</span><span class="p">]</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (N, vocab_size)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.0365
valid loss: 2.1045
</pre></div>
</div>
</div>
</div>
<p>The training loss slightly improved from 2.07 to 2.04 and the valid loss from 2.13 to 2.10.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Log of losses</p>
<p>Original loss:
train: 2.1243
valid: 2.1680</p>
<p>Fix softmax confidently wrong:
train: 2.0686
valid: 2.1294</p>
<p>Fix tanh layer too saturated at init:
train: 2.0365
valid: 2.1045</p>
</div>
<p>Our MLP has a simple architecture with only one layer, which makes it relatively insensitive to poor initialization. Even if our initialisation was terrible, the network was still able to learn eventually. However, this is not the case for deeper networks (e.g. 50 layers) where the impact of poor initialization can compound. The deeper and more complex the network is, the more critical the initialization becomes, and it may not be able to learn at all.</p>
</section>
</section>
<section id="kaiming-initialisation">
<h3>Kaiming initialisation<a class="headerlink" href="#kaiming-initialisation" title="Permalink to this heading">#</a></h3>
<p>The paper <a class="reference external" href="https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers by Kaiming He et al. (2015)</a> is widely referenced as a guide for initializing weights in deep neural networks. This paper specifically examines the ReLU and PReLU activations in convolutional neural networks, but the analysis is also applicable to tanh activations in MLPs.</p>
<p>When dealing with a large neural network with many layers, it is not feasible to manually set the scales (standard deviation) of all the weights and biases, as was done above. Instead, there are several established methods for determining these scales in a principled way, which will be introduced in the following section.</p>
<p>Given an input <code class="docutils literal notranslate"><span class="pre">x</span></code> randomly drawn from a Gaussian distribution (mean=0, std=1) with 1000 samples and 10 dimensions, and a layer <code class="docutils literal notranslate"><span class="pre">w</span></code> initialized with a Gaussian distribution (mean=0, std=1) with 200 neurons and 10 inputs each, we can calculate the pre-activations <code class="docutils literal notranslate"><span class="pre">y</span></code> by performing the dot product of <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">w</span></code>.<br />
The question is, what are the mean and standard deviation of <code class="docutils literal notranslate"><span class="pre">y</span></code>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Histograms for x and y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x mean / std: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y mean / std: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/0f0368b3298c1906d2deb3c7b8659d4dce25ef7f61b2252182f0f9806074ed7b.png" src="../../_images/0f0368b3298c1906d2deb3c7b8659d4dce25ef7f61b2252182f0f9806074ed7b.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x mean / std: 0.0215 / 0.9926
y mean / std: -0.0085 / 3.1987
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x mean / std: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">10</span><span class="o">**</span><span class="mf">0.5</span><span class="p">]:</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span> <span class="o">*</span> <span class="n">s</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scale: </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2"> -&gt; y mean / std: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x mean / std: -0.0090 / 0.9811
Scale: 1 -&gt; y mean / std: 0.0004 / 3.1526
Scale: 5 -&gt; y mean / std: 0.0136 / 15.9764
Scale: 0.2 -&gt; y mean / std: 0.0010 / 0.6153
Scale: 0.31622776601683794 -&gt; y mean / std: -0.0000 / 0.9751
</pre></div>
</div>
</div>
</div>
<p>The mean of <code class="docutils literal notranslate"><span class="pre">y</span></code> remains around 0 (due to the symmetric nature of the operation), but the standard deviation increases to around 3. If we scale with a factor bigger than 1 the std of <code class="docutils literal notranslate"><span class="pre">y</span></code> increases, if we scale with a factor less than 1, the std decreases.<br />
If we scale <code class="docutils literal notranslate"><span class="pre">w</span></code> with <span class="math notranslate nohighlight">\(\sqrt{1/\text{fan}_{in}}= \sqrt{1/10}\)</span> then we obtain a std of 1.</p>
<p>The ReLU function is a squashing function that squashes the negative values to 0, which throws away half of the distribution. It is necessary to compensate the scaling <span class="math notranslate nohighlight">\(\sqrt{1/\text{fan}_{in}}\)</span> with a gain depending on the non-linearity, so that the variance of the outputs of each layer remains approximately the same. This helps to prevent the vanishing or exploding gradients problem, and improves the overall training performance of the neural network. For ReLU, the gain is <span class="math notranslate nohighlight">\(\sqrt2\)</span>, meaning that the weights should be initialized with a Gaussian distribution with mean 0 and standard deviation of <span class="math notranslate nohighlight">\(\sqrt{(2/\text{fan}_{in})}\)</span>.</p>
<p>The paper also examines the backpropagations, ensuring that the gradients are well-behaved. If the forward pass is properly initialized, the backward pass is also approximately initialized, except for a constant factor that depends on the size of hidden neurons in the previous and next layer. Empirically, it has been shown that this constant factor does not have a significant impact on the overall performance of the neural network.</p>
<div class="highlight-Note notranslate"><div class="highlight"><pre><span></span>**Pytorch Kaiming initialisation**

[torch.nn.init.kaiming_normal_(tensor, a=0, mode=&#39;fan_in&#39;, nonlinearity=&#39;leaky_relu)`](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_)

The resulting tensor will have values sampled from $\mathcal{N}(0, \text{std}^2)$ where $\text{std} = \frac{\text{gain}}{\sqrt{\text{fan\_mode}}}$.  

The `mode` parameter can be either fan_in to normalise the forward pass or fan_out to normalise the backwardpass to a gaussian with mean 0 and std 1.
The `nonlinearity` parameter is used to calculate the gain. See [torch.nn.init.calculate_gain(nonlinearity, param=None)](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.calculate_gain)

Gain:
- Linear: 1
- Tanh: 5/3
- ReLU: $\sqrt2$
</pre></div>
</div>
<section id="apply-kaiming-and-train-the-network">
<h4>Apply Kaiming and train the network<a class="headerlink" href="#apply-kaiming-and-train-the-network" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span>
    <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">gain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>  <span class="c1"># 5 / 3</span>
<span class="c1"># Initialise the weights</span>
<span class="n">fan_in</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">model</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">gain</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># 0.304</span>
<span class="n">model</span><span class="o">.</span><span class="n">b1</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="mf">0.01</span>
<span class="n">fan_in</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">model</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">gain</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># 0.118</span>
<span class="n">model</span><span class="o">.</span><span class="n">b2</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="mi">0</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">200001</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="c1"># Minibatch construct</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtrn</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtrn</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytrn</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>

    <span class="c1"># Forward pass</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>

    <span class="c1"># Backward pass</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># retain_graph = True because we changed the</span>

    <span class="c1"># Update</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100000</span> <span class="k">else</span> <span class="mf">0.01</span>  <span class="c1"># Step learning rate decay</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># Tack stats</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/200001: 4.0933
  10000/200001: 2.0371
  20000/200001: 2.3370
  30000/200001: 2.2522
  40000/200001: 1.9623
  50000/200001: 2.3524
  60000/200001: 2.0237
  70000/200001: 2.0491
  80000/200001: 2.2408
  90000/200001: 2.0058
 100000/200001: 2.2135
 110000/200001: 2.0812
 120000/200001: 2.2047
 130000/200001: 1.5784
 140000/200001: 2.2378
 150000/200001: 2.2673
 160000/200001: 2.0387
 170000/200001: 1.6633
 180000/200001: 2.6312
 190000/200001: 2.1635
 200000/200001: 1.9434
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># Decorator to disable gradient tracking</span>
<span class="k">def</span> <span class="nf">split_loss</span><span class="p">(</span><span class="n">split</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">),</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">),</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span><span class="p">)}</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">split</span><span class="p">]</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (N, vocab_size)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.0392
valid loss: 2.1072
</pre></div>
</div>
</div>
</div>
<p>The training loss remains similar to before, however, with Kaiming initialization, the scaling factors are not determined by inspecting histograms. Instead, a semi-principled approach is used that is able to scale to larger networks.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Log of losses</p>
<p>Original loss:
train: 2.1243
valid: 2.1680</p>
<p>Fix softmax confidently wrong:
train: 2.0686
valid: 2.1294</p>
<p>Fix tanh layer too saturated at init:
train: 2.0365
valid: 2.1045</p>
<p>Kaiming:
train: 2.0392
valid: 2.1072</p>
</div>
</section>
</section>
</section>
<section id="batch-normalisation">
<h2>Batch Normalisation<a class="headerlink" href="#batch-normalisation" title="Permalink to this heading">#</a></h2>
<p><span class="math notranslate nohighlight">\(y_i \leftarrow \gamma\hat{x_i}+\beta\)</span></p>
<p>Recent advancements have made neural networks more stable, reducing the need for precise initialization. The paper <a class="reference external" href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization by Sergey Ioffe et al. (2015)</a> had a significant impact on the field, making it possible to train very deep neural networks with more reliability.<br />
The idea behind the BatchNorm layer is to standardise the pre-activations to be roughly unit Gaussian by subtracting the mean from the batch pre-activation and divide this by the std. The normalisation operation is differentiable.</p>
<p>We dont want the activations to be always unit gaussian. After all, normalizing each input of a layer alters the layers representational capacity. For instance, normalizing the inputs of a sigmoid function would confine them to the linear range of the nonlinearity.<br />
To overcome this issue, we introduce a set of parameters, namely <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>, for every activation <code class="docutils literal notranslate"><span class="pre">x</span></code>, that scale and shift the normalized value as follows: <span class="math notranslate nohighlight">\(y=\gamma x+\beta\)</span>. These parameters are learned together with the original model parameters and restore the networks representational power.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>BatchNorm</strong>
Normalize:
<span class="math notranslate nohighlight">\(\hat{x_i} \leftarrow \frac{x_1 - \mu}{\sigma}\)</span> where<br />
<span class="math notranslate nohighlight">\(\mu \leftarrow \frac{1}{m}\sum_{i=1}^m{x_i}\)</span> and<br />
<span class="math notranslate nohighlight">\(\sigma^2 \leftarrow \frac{1}{m}\sum_{i=1}^m{(x_i - \mu)^2}\)</span>.
Scale and shift:
<span class="math notranslate nohighlight">\(y_i \leftarrow \gamma\hat{x_i}+\beta\)</span></p>
</div>
<p>Lets implement this in <code class="docutils literal notranslate"><span class="pre">MPL()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">27</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">emb_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">hid_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> <span class="k">if</span> <span class="n">seed</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_dim</span> <span class="o">=</span> <span class="n">emb_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hid_dim</span> <span class="o">=</span> <span class="n">hid_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">block_size</span> <span class="o">*</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>  <span class="c1"># gain</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>  <span class="c1"># bias</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b1</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b2</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bn_gamma</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bn_beta</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_parameters</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Does a forward pass and returns the logits&quot;&quot;&quot;</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">[</span><span class="n">xs</span><span class="p">]</span>  <span class="c1"># (bs, 3, 10) Embed characters into vectors</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_dim</span>
        <span class="p">)</span>  <span class="c1"># (bs, 30) Concatenate the vectors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span>  <span class="c1"># Hidden layer pre-activation</span>

        <span class="c1"># BatchNorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span>  <span class="c1"># Normalising</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_beta</span>  <span class="c1"># Shift and scale</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span><span class="p">)</span>  <span class="c1"># Hidden layer</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span>  <span class="c1"># Output layer</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets the gradients to None&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">optimise_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Updat the parameters&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Training</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ix2ch</span><span class="p">)</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">emb_dim</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># The dimentionality of the character embedding vectors</span>
<span class="n">hid_dim</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># the number of neurons in the hidden layer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span>
    <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Initialise the weights</span>
<span class="n">gain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>  <span class="c1"># 5 / 3</span>
<span class="n">fan_in</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">model</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">gain</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># 0.304</span>
<span class="n">model</span><span class="o">.</span><span class="n">b1</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="mf">0.01</span>
<span class="n">fan_in</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">model</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">gain</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># 0.118</span>
<span class="n">model</span><span class="o">.</span><span class="n">b2</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="mi">0</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">200001</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mus</span><span class="p">,</span> <span class="n">sigmas</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="n">gammas</span><span class="p">,</span> <span class="n">betas</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="c1"># Minibatch construct</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtrn</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtrn</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytrn</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>

    <span class="c1"># Forward pass</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>

    <span class="c1"># Backward pass</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># retain_graph = True because we changed the</span>

    <span class="c1"># Update</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100000</span> <span class="k">else</span> <span class="mf">0.01</span>  <span class="c1"># Step learning rate decay</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># Tack stats</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">mus</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">sigmas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sigma</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">betas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">bn_beta</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">gammas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">bn_gamma</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/200001: 3.9589
  10000/200001: 2.0083
  20000/200001: 2.4120
  30000/200001: 2.1590
  40000/200001: 2.0901
  50000/200001: 2.3454
  60000/200001: 2.0026
  70000/200001: 2.1650
  80000/200001: 2.2169
  90000/200001: 2.1430
 100000/200001: 2.2172
 110000/200001: 2.1164
 120000/200001: 2.3085
 130000/200001: 1.6323
 140000/200001: 2.1411
 150000/200001: 2.3615
 160000/200001: 2.0977
 170000/200001: 1.7015
 180000/200001: 2.5745
 190000/200001: 2.3151
 200000/200001: 1.9448
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;log-loss&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$log_</span><span class="si">{10}</span><span class="s2">(loss)$&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mus</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\mu$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\bar{\mu}$&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigmas</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sigma$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\bar{\sigma}$&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">gammas</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\gamma$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\bar{\gamma}$&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">betas</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\bar{\beta}$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/83f9ac0b66944796c24f365209a5a53eed80e60f0b45b856a4caecc370ad7bca.png" src="../../_images/83f9ac0b66944796c24f365209a5a53eed80e60f0b45b856a4caecc370ad7bca.png" />
</div>
</div>
<p><strong>Inference</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># Decorator to disable gradient tracking</span>
<span class="k">def</span> <span class="nf">split_loss</span><span class="p">(</span><span class="n">split</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">),</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">),</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span><span class="p">)}</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">split</span><span class="p">]</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (N, vocab_size)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.1929
valid loss: 2.1979
</pre></div>
</div>
</div>
</div>
<p>The training loss remains comparable to its previous value, which is not surprising since we are working with a very basic neural network with only one hidden layer. In contrast, with more complex networks involving deep layers, convolutional layers, and residual connections, it becomes considerably more challenging to tune the scales of the weights  such that all activations across the network are approximately normally distributed with unit variance. Its much easier to use batch normalisation after a linear or convolutional layer in stead.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Log of losses</p>
<p>Original loss:
train: 2.1243
valid: 2.1680</p>
<p>Fix softmax confidently wrong:
train: 2.0686
valid: 2.1294</p>
<p>Fix tanh layer too saturated at init:
train: 2.0365
valid: 2.1045</p>
<p>Kaiming:
train: 2.0392
valid: 2.1072</p>
<p>Batchnorm:
train: 2.0703
valid: 2.1088</p>
</div>
<p><strong>Batch normalization introduces a tradeoff.</strong><br />
When not using batch normalization, operations are performed on individual inputs. To make training more efficient, we work with batches that process each input individually in parallel but operations are still performed on individual inputs. One input from the batch doesnt influence the operations of another input.<br />
However, when using batch normalisation, the individual inputs are mathematically coupled in the forward and backward pass due to the mean and std for the whole batch. The hidden state activations (<code class="docutils literal notranslate"><span class="pre">h_preact</span></code>) are not only a function of their individual input, but also a function of all other examples in the batch. As a result, the hidden state activation can vary with each batch due to changes in the other examples, leading to jitter.<br />
Remarkably, this jittering phenomenon turns out to be advantageous. The jitter acts like a kind of regularizer or data augmentation.</p>
<p>Another issue with batch normalisation is with inference. We would like to be able to input only a single example in stead of a batch of examples but how do we then calculate the mean and std?</p>
<p>The <a class="reference external" href="https://arxiv.org/pdf/1502.03167.pdf">batch normalisation paper</a> proposes the following: add am additional step after training to calculate the mean and std over the entire training set.</p>
<p><strong>Calculate the mean and std for the whole training set</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">C</span><span class="p">[</span><span class="n">Xtrn</span><span class="p">]</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">Xtrn</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">h_preact</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">@</span> <span class="n">model</span><span class="o">.</span><span class="n">W1</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">b1</span>
    <span class="c1"># Calculate the mean and std for the whole training set</span>
    <span class="n">bn_mean</span> <span class="o">=</span> <span class="n">h_preact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bn_std</span> <span class="o">=</span> <span class="n">h_preact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Inference</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># Decorator to disable gradient tracking</span>
<span class="k">def</span> <span class="nf">split_loss</span><span class="p">(</span><span class="n">split</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">),</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">),</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span><span class="p">)}</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">split</span><span class="p">]</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">h_preact</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">@</span> <span class="n">model</span><span class="o">.</span><span class="n">W1</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">b1</span>
    <span class="c1"># Batch normalisation</span>
    <span class="n">h_preact</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_preact</span> <span class="o">-</span> <span class="n">bn_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">bn_std</span>  <span class="c1"># Normalising</span>
    <span class="n">h_preact</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bn_gamma</span> <span class="o">*</span> <span class="n">h_preact</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">bn_beta</span>  <span class="c1"># Shift and scale</span>

    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">h_preact</span><span class="p">)</span>  <span class="c1"># Hidden layer</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">model</span><span class="o">.</span><span class="n">W2</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">b2</span>  <span class="c1"># Output layer</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.0703
valid loss: 2.1089
</pre></div>
</div>
</div>
</div>
<p>We get identical results but we can now do inference on a single example. The disadvantage of this method is that we have an additional step after training.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Log of losses</p>
<p>Original loss:
train: 2.1243
valid: 2.1680</p>
<p>Fix softmax confidently wrong:
train: 2.0686
valid: 2.1294</p>
<p>Fix tanh layer too saturated at init:
train: 2.0365
valid: 2.1045</p>
<p>Kaiming:
train: 2.0392
valid: 2.1072</p>
<p>Batchnorm:
train: 2.0703
valid: 2.1088</p>
<p>Batchnorm (additional step):
train: 2.0703
valid: 2.1089</p>
</div>
<p>The <a class="reference external" href="https://arxiv.org/pdf/1502.03167.pdf">batch normalisation paper</a> suggest an alternative approach of estimating the running mean and standard deviation. The aim is to ensure that these estimates closely match the mean and standard deviation of the entire training set, thereby eliminating the need for an additional step after training.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Pytorch uses the running mean and std method in their batchnorm implementation</p>
</div>
<p>Another small detail to modify is the bias. Since we add the bias in</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span>  <span class="c1"># Hidden layer pre-activation</span>
</pre></div>
</div>
<p>then calculate the mean in</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>and then subtract that mean in</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span>  <span class="c1"># Normalising</span>
</pre></div>
</div>
<p>we basically add the bias and then subtract it again. Thats why its not necessary to add a bias term in the layer before batch norm. The batch norm introduces its own learnable bias (<code class="docutils literal notranslate"><span class="pre">bn_beta</span></code>).</p>
<p><strong>Calculate running mean and std</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">27</span><span class="p">,</span>
        <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">emb_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">hid_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> <span class="k">if</span> <span class="n">seed</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb_dim</span> <span class="o">=</span> <span class="n">emb_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hid_dim</span> <span class="o">=</span> <span class="n">hid_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">block_size</span> <span class="o">*</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
            <span class="n">hid_dim</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span>
        <span class="p">)</span>  <span class="c1"># Not used, kept otherwise b2 will be different from previous MLP</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu_running</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma_running</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn_gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>  <span class="c1"># gain</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>  <span class="c1"># bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">,</span>
            <span class="c1">#    self.b1,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b2</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bn_gamma</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bn_beta</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_parameters</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Does a forward pass and returns the logits&quot;&quot;&quot;</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">[</span><span class="n">xs</span><span class="p">]</span>  <span class="c1"># (bs, 3, 10) Embed characters into vectors</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb_dim</span>
        <span class="p">)</span>  <span class="c1"># (bs, 30) Concatenate the vectors</span>
        <span class="c1"># self.h_preact = emb @ self.W1 + self.b1  # Hidden layer pre-activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span>  <span class="c1"># Hidden layer pre-activation</span>

        <span class="c1"># BatchNorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span>  <span class="c1"># Normalising</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_gamma</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_beta</span>  <span class="c1"># Shift and scale</span>

        <span class="c1"># Update running mean and std</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mu_running</span> <span class="o">=</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu_running</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sigma_running</span> <span class="o">=</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_running</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_preact</span><span class="p">)</span>  <span class="c1"># Hidden layer</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span>  <span class="c1"># Output layer</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets the gradients to None&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">optimise_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Updat the parameters&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Training</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ix2ch</span><span class="p">)</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">emb_dim</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># The dimentionality of the character embedding vectors</span>
<span class="n">hid_dim</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># the number of neurons in the hidden layer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span>
    <span class="n">hid_dim</span><span class="o">=</span><span class="n">hid_dim</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Initialise the weights</span>
<span class="n">gain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>  <span class="c1"># 5 / 3</span>
<span class="n">fan_in</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">model</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">gain</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># 0.304</span>
<span class="c1"># model.b1.data *= 0.01</span>
<span class="n">fan_in</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">model</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">gain</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># 0.118</span>
<span class="n">model</span><span class="o">.</span><span class="n">b2</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="mi">0</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">200001</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="c1"># Minibatch construct</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xtrn</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">Xb</span><span class="p">,</span> <span class="n">Yb</span> <span class="o">=</span> <span class="n">Xtrn</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">Ytrn</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>

    <span class="c1"># Forward pass</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">Xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>

    <span class="c1"># Backward pass</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># retain_graph = True because we changed the</span>

    <span class="c1"># Update</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100000</span> <span class="k">else</span> <span class="mf">0.01</span>  <span class="c1"># Step learning rate decay</span>
    <span class="n">model</span><span class="o">.</span><span class="n">optimise_step</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># Tack stats</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/200001: 3.9589
  10000/200001: 2.0083
  20000/200001: 2.4120
  30000/200001: 2.1590
  40000/200001: 2.0901
  50000/200001: 2.3454
  60000/200001: 2.0026
  70000/200001: 2.1650
  80000/200001: 2.2169
  90000/200001: 2.1430
 100000/200001: 2.2172
 110000/200001: 2.1164
 120000/200001: 2.3085
 130000/200001: 1.6323
 140000/200001: 2.1411
 150000/200001: 2.3615
 160000/200001: 2.0977
 170000/200001: 1.7015
 180000/200001: 2.5745
 190000/200001: 2.3151
 200000/200001: 1.9448
</pre></div>
</div>
</div>
</div>
<p><strong>Inference</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_running</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">mu_running</span>
<span class="n">std_running</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sigma_running</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># Decorator to disable gradient tracking</span>
<span class="k">def</span> <span class="nf">split_loss</span><span class="p">(</span><span class="n">split</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtrn</span><span class="p">,</span> <span class="n">Ytrn</span><span class="p">),</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xval</span><span class="p">,</span> <span class="n">Yval</span><span class="p">),</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Xtst</span><span class="p">,</span> <span class="n">Ytst</span><span class="p">)}</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="n">split</span><span class="p">]</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">h_preact</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">@</span> <span class="n">model</span><span class="o">.</span><span class="n">W1</span>
    <span class="c1"># Batch normalisation</span>
    <span class="n">h_preact</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_preact</span> <span class="o">-</span> <span class="n">mean_running</span><span class="p">)</span> <span class="o">/</span> <span class="n">std_running</span>  <span class="c1"># Normalising</span>
    <span class="n">h_preact</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bn_gamma</span> <span class="o">*</span> <span class="n">h_preact</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">bn_beta</span>  <span class="c1"># Shift and scale</span>

    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">h_preact</span><span class="p">)</span>  <span class="c1"># Hidden layer</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">model</span><span class="o">.</span><span class="n">W2</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">b2</span>  <span class="c1"># Output layer</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">split_loss</span><span class="p">(</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss: 2.0702
valid loss: 2.1090
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Log of losses</p>
<p>Original loss:
train: 2.1243
valid: 2.1680</p>
<p>Fix softmax confidently wrong:
train: 2.0686
valid: 2.1294</p>
<p>Fix tanh layer too saturated at init:
train: 2.0365
valid: 2.1045</p>
<p>Kaiming:
train: 2.0392
valid: 2.1072</p>
<p>Batchnorm:
train: 2.0703
valid: 2.1088</p>
<p>Batchnorm (additional step):
train: 2.0703
valid: 2.1089</p>
<p>Batchnorm (running mean and std):
train: 2.0702
valid: 2.1090</p>
</div>
</section>
<section id="pytorch-batch-normalisation">
<h2>Pytorch Batch Normalisation<a class="headerlink" href="#pytorch-batch-normalisation" title="Permalink to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> 
                         <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> 
                         <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
                         <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                         <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                         <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                         <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>Pytorch batch normalisation implements following function:<br />
$<span class="math notranslate nohighlight">\(
y=\frac{x-E[x]}{\sqrt{Var[x]+\epsilon}} * \gamma + \beta
\)</span>$</p>
<p><strong>Parameters</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_features</span></code>: the number of features (200 in our case ). Its needed to initialise the gain<span class="math notranslate nohighlight">\(\gamma\)</span> , bias <span class="math notranslate nohighlight">\(\beta\)</span> and the running mean and std for inference.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eps</span></code>: \epsilon is a very small number to avoid division by 0. The default value (1e-5) is usually fine.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">momentum</span></code>: used for calculating the running mean and std (0.001 in our case). Needs to be adapted based on the batch size. When using a large batch size, the mean and standard deviation wont change significantly from batch to batch. Therefore, a relatively high momentum value can be used to compute the running mean and standard deviation. On the other hand, with a small batch size, the mean and standard deviation can fluctuate more between batches. The mean and standard deviation wont wont converge. In this case, a lower momentum value is typically used</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">affine</span></code>: Controls whether the learnable affine parameters gain <span class="math notranslate nohighlight">\(\gamma\)</span> and bias <span class="math notranslate nohighlight">\(\beta\)</span> are utilized. Almost always kept to <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">track_running_stats</span></code>: Controls whether to calculate the running mean and std. Disabling this feature can be useful when you want to estimate the mean and standard deviation at the end of training using an additional step.</p></li>
</ul>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h2>
<p>Understanding the activations and gradients, as well as their statistics, is crucial, particularly as neural networks become bigger, larger, and deeper.</p>
<ul class="simple">
<li><p>During our analysis, we examined the distributions at the output layer and observed that overly confident mispredictions, resulting from activations in the final layer that are too messed up, could lead to a hockeystick loss. By addressing this issue, we can achieve a better loss at the end of training by avoiding wasteful work in the beginning. We saw that we need to control the activations. It is undesirable for them to either compress to zero or expand to infinity, as this would cause significant problems with all of the non-linearities in the neural network. Ideally, the activations should be somewhat Gaussian distributed throughout the neural net. To achieve this, we explored  scaling weights and biases  at the initialization of our neural network.</p></li>
<li><p>We recognized that scaling weights and biases may not be feasible for deeper neural nets due to the challenge of accurately setting them to produce roughly Gaussian activations throughout the network. As a solution, we introduced the concept of a normalization layer. Several normalization layers are used in practice, including batch normalization, layer normalization, instance normalization, layer normalization, and group normalization. Specifically, we delved into batch normalization, which can be incorporated throughout a deep neural net to achieve roughly Gaussian activations. The process involves normalizing the activations by calculating the mean and standard deviation, and it is differentiable. However, we noted that batch normalization can cause issues because it couples examples in a batch. As a better alternative, we recommended using layer or group normalization.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/build_language_model"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="2_mlp.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Multilayer Perceptron</p>
      </div>
    </a>
    <a class="right-next"
       href="3.2_activations_gradients_batchnorm.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Activations, Gradients and Batch Normalisation - Part 2</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducing-results-from-previous-notebook">Reproducing results from previous notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#initialisation">Initialisation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fixing-the-logits">Fixing the logits</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#initial-model">Initial model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#improved-initialisation">Improved initialisation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fixing-the-saturated-tanh">Fixing the saturated tanh</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#improved-model">Improved model</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kaiming-initialisation">Kaiming initialisation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#apply-kaiming-and-train-the-network">Apply Kaiming and train the network</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalisation">Batch Normalisation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-batch-normalisation">Pytorch Batch Normalisation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Marc Dumon
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
       Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>